Trace-announcements

These entries can be used to cite the traces themselves. The first couple are for the May 2011 "full" trace.

 @Misc{clusterdata:Wilkes2011,
   author = 	 {John Wilkes},
   title = 	 {More {Google} cluster data},
   howpublished = {Google research blog},
   month = 	 Nov,
   year = 	 2011,
   note = {Posted at \url{http://googleresearch.blogspot.com/2011/11/more-google-cluster-data.html}.},
 }
 

@techreport{reiss2011gct, 
   author = {Charles Reiss and John Wilkes and Joseph L. Hellerstein}, 
   title = {{Google} cluster-usage traces: format + schema}, institution = {Google Inc.}, year = 2011, month = Nov, type = {Technical Report}, address = {Mountain View, CA, USA}, note = {Revised 2012.03.20. Posted at URL \url{http://code.google.com/p/googleclusterdata/wiki/TraceVersion2}}, }

The second one is for the "small" 7-hour trace that was released first.

 @Misc{clusterdata:Hellersetein2010,
   author = 	 {Joseph L. Hellerstein},
   title = 	 {{Google} cluster data},
   howpublished = {Google research blog},
   month = 	 Jan,
   year = 	 2010,
   note = {Posted at \url{http://googleresearch.blogspot.com/2010/01/google-cluster-data.html}.},
 }
 

The next paper describes the policy choices and technologies used to make the traces safe to release.

 @InProceedings{clusterdata:Reiss2012,
   author = {Charles Reiss and John Wilkes and Joseph L. Hellerstein},
   title = {Obfuscatory obscanturism: making workload traces of
 	commercially-sensitive systems safe to release},
   year = 2012,
   booktitle = {3rd International Workshop on Cloud Management (CLOUDMAN'12)},
   month = Apr,
   publisher = {IEEE},
   pages = {1279--1286},
   url =	{http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6212064},
   location = {Maui, HI, USA},
   abstract = {Cloud providers such as Google are interested in
 	fostering research on the daunting technical challenges they
 	face in supporting planetary-scale distributed systems, but no
 	academic organizations have similar scale systems on which to
 	experiment. Fortunately, good research can still be done using
 	traces of real-life production workloads, but there are risks
 	in releasing such data, including inadvertently disclosing
 	confidential or proprietary information, as happened with the
 	Netflix Prize data. This paper discusses these risks, and our
 	approach to them, which we call systematic obfuscation. It
 	protects proprietary and personal data while leaving it
 	possible to answer interesting research questions. We explain
 	and motivate some of the risks and concerns and propose how
 	they can best be mitigated, using as an example our recent
 	publication of a month-long trace of a production system
 	workload on a 11k-machine cluster.},
   url = {\url{http://www.socc2012.org/s7-reiss.pdf}},
 }
 

Trace-analysis papers

These papers are primarily about analyzing the traces.

 @INPROCEEDINGS{clusterdata:Reiss2012b,
   title = {Heterogeneity and Dynamicity of Clouds at Scale: {Google} Trace Analysis},
   author = {Charles Reiss and Alexey Tumanov and Gregory R. Ganger and Randy H. Katz and Michael A. Kozuch},
   booktitle = {ACM Symposium on Cloud Computing (SoCC)},
   year = 2012,
   mon = Oct,
   address = {San Jose, CA, USA},
   abstract = {To better understand the challenges in developing
     effective cloud-based resource schedulers, we analyze the ﬁrst 
     publicly available trace data from a sizable multi-purpose cluster. 
     The most notable workload characteristic is heterogeneity: in 
     resource types (e.g., cores:RAM per machine) and their usage
     (e.g., duration and resources needed). Such heterogeneity reduces
     the effectiveness of traditional slot- and core-based scheduling.
     Furthermore, some tasks are constrained as to the kind of machine
     types they can use, increasing the complexity of resource assignment
     and complicating task migration. The workload is also highly
     dynamic, varying over time and most workload features, and is
     driven by many short jobs that demand quick scheduling decisions.
     While few simplifying assumptions apply, we ﬁnd that many
     longer-running jobs have relatively stable resource utilizations,
     which can help adaptive resource schedulers.}, 
 }
 

@INPROCEEDINGS{clusterdata:Liu2012, author = {Zitao Liu and Sangyeun Cho}, title = {Characterizing Machines and Workloads on a {Google} Cluster}, booktitle = {8th International Workshop on Scheduling and Resource Management for Parallel and Distributed Systems (SRMPDS'12)}, year = 2012, month = Sep, location = {Pittsburgh, PA, USA}, abstract = {Cloud computing offers high scalability, flexibility and cost-effectiveness to meet emerging computing requirements. Understanding the characteristics of real workloads on a large production cloud cluster benefits not only cloud service providers but also researchers and daily users. This paper studies a large-scale Google cluster usage trace dataset and characterizes how the machines in the cluster are managed and the workloads submitted during a 29-day period behave. We focus on the frequency and pattern of machine maintenance events, job- and task-level workload behavior, and how the overall cluster resources are utilized.}, note = {Posted at \url{http://www.cs.pitt.edu/cast/abstract/liu-srmpds12.html}.}, }

@INPROCEEDINGS{clusterdata:Ali-Eldin2012 author = {Ahmed Ali-Eldin and Maria Kihl and Johan Tordsson and Erik Elmroth}, title = {Efficient Provisioning of Bursty Scientific Workloads on the Cloud Using Adaptive Elasticity Control}, booktitle = {3rd Workshop on Scientific Cloud Computing (ScienceCloud'12)}, year = 2012, month = Jun, location = {Delft, The Nederlands}, isbn = {978-1-4503-1340-7}, pages = {31--40}, URL = {http://dl.acm.org/citation.cfm?id=2287044}, doi = {10.1145/2287036.2287044}, publisher = {ACM}, abstract = {Elasticity is the ability of a cloud infrastructure to dynamically change the amount of resources allocated to a running service as load changes. We build an autonomous elasticity controller that changes the number of virtual machines allocated to a service based on both monitored load changes and predictions of future load. The cloud infrastructure is modeled as a G/G/N queue. This model is used to construct a hybrid reactive-adaptive controller that quickly reacts to sudden load changes, prevents premature release of resources, takes into account the heterogeneity of the workload, and avoids oscillations. Using simulations with Web and cluster workload traces, we show that our proposed controller lowers the number of delayed requests by a factor of 70 for the Web traces and 3 for the cluster traces when compared to a reactive controller. Our controller also decreases the average number of queued requests by a factor of 3 for both traces, and reduces oscillations by a factor of 7 for the Web traces and 3 for the cluster traces. This comes at the expense of between 20\% and 30\% over-provisioning, as compared to a few percent for the reactive controller.}, }

@TechReport{clusterdata:Di2012, author = {Sheng Di and Derrick Kondo and Walfredo Cirne}, title = {Characterization and Comparison of {Google} Cloud Load versus {Grids}}, institution = {MESCAL (INRIA Grenoble Rh\^one-Alpes / LIG laboratoire d'Informatique de Grenoble)}, year = 2012, month = Jun, number = {hal-00705858 version 1}, address = {Grenoble, France}, abstract = {A new era of Cloud Computing has emerged, but the characteristics of Cloud load in data centers is not perfectly clear. Yet this characterization is critical for the design of novel Cloud job and resource management systems. In this paper, we comprehensively characterize the job/task load and host load in a real-world production data center at Google Inc. We use a detailed trace of over 25 million tasks across over 12,500 hosts. We study the differences between a Google data center and other Grid/HPC systems, from the perspective of both work load (w.r.t. jobs and tasks) and host load (w.r.t. machines). In particular, we study the job length, job submission frequency, and the resource utilization of jobs in the different systems, and also investigate valuable statistics of machine's maximum load, queue state and relative usage levels, with different job priorities and resource attributes. We find that the Google data center exhibits finer resource allocation with respect to CPU and memory than that of Grid/HPC systems. Google jobs are always submitted with much higher frequency and they are much shorter than Grid jobs. As such, Google host load exhibits higher variance and noise.}, note = {Posted at \url{http://hal.archives-ouvertes.fr/hal-00705858}.}, }

@TechReport{clusterdata:Reiss2012a, author = {Charles Reiss and Alexey Tumanov and Gregory R. Ganger and Randy H. Katz and Michael A. Kozuch}, title = {Towards understanding heterogeneous clouds at scale: {Google} trace analysis}, institution = {Intel science and technology center for cloud computing, Carnegie Mellon University}, year = 2012, month = Apr, number = {ISTC--CC--TR--12--101}, address = {Pittsburgh, PA, USA}, abstract = {With the emergence of large, heterogeneous, shared computing clusters, their efficient use by mixed distributed workloads and tenants remains an important challenge. Unfortunately, little data has been available about such workloads and clusters. This paper analyzes a recent Google release of scheduler request and utilization data across a large (12500+) general-purpose compute cluster over 29 days. We characterize cluster resource requests, their distribution, and the actual resource utilization. Unlike previous scheduler traces we are aware of, this one includes diverse workloads -- from large web services to large CPU-intensive batch programs -- and permits comparison of actual resource utilization with the user-supplied resource estimates available to the cluster resource scheduler. We observe some under-utilization despite over-commitment of resources, difficulty of scheduling high-priority tasks that specify constraints, and lack of dynamic adjustments to user allocation requests despite the apparent availability of this feature in the scheduler.}, note = {Posted at \url{http://www.istc-cc.cmu.edu/publications/papers/2012/ISTC-CC-TR-12-101.pdf}}, }

Trace-usage papers

These entries are for papers that primarily focus on some other topic, but use the traces as inputs, e.g., in simulations.

 @inproceedings{clusterdata:Sharma2011,
   author = {Sharma, Bikash and Chudnovsky, Victor and Hellerstein,
 	Joseph L. and Rifaat, Rasekh and Das, Chita R.},
   title = {Modeling and synthesizing task placement constraints in
 	{Google} compute clusters},
   booktitle = {2nd ACM Symposium on Cloud Computing (SoCC'11)},
   year = 2011,
   month = Oct,
   isbn = {978-1-4503-0976-9},
   location = {Cascais, Portugal},
   pages = {3:1--3:14},
   articleno = {3},
   numpages = {14},
   url = {http://doi.acm.org/10.1145/2038916.2038919},
   doi = {10.1145/2038916.2038919},
   publisher = {ACM},
   keywords = {benchmarking, benchmarks, metrics, modeling, performance
 	evaluation, workload characterization},
   abstract = {Evaluating the performance of large compute clusters
 	requires benchmarks with representative workloads. At Google,
 	performance benchmarks are used to obtain performance metrics
 	such as task scheduling delays and machine resource
 	utilizations to assess changes in application codes, machine
 	configurations, and scheduling algorithms. Existing approaches
 	to workload characterization for high performance computing
 	and grids focus on task resource requirements for CPU, memory,
 	disk, I/O, network, etc. Such resource requirements address
 	how much resource is consumed by a task. However, in addition
 	to resource requirements, Google workloads commonly include
 	task placement constraints that determine which machine
 	resources are consumed by tasks. Task placement constraints
 	arise because of task dependencies such as those related to
 	hardware architecture and kernel version.
 

This paper develops methodologies for incorporating task placement constraints and machine properties into performance benchmarks of large compute clusters. Our studies of Google compute clusters show that constraints increase average task scheduling delays by a factor of 2 to 6, which often results in tens of minutes of additional task wait time. To understand why, we extend the concept of resource utilization to include constraints by introducing a new metric, the Utilization Multiplier (UM). UM is the ratio of the resource utilization seen by tasks with a constraint to the average utilization of the resource. UM provides a simple model of the performance impact of constraints in that task scheduling delays increase with UM. Last, we describe how to synthesize representative task constraints and machine properties, and how to incorporate this synthesis into existing performance benchmarks. Using synthetic task constraints and machine properties generated by our methodology, we accurately reproduce performance metrics for benchmarks of Google compute clusters with a discrepancy of only 13\% in task scheduling delay and 5\% in resource utilization.}, }

@InProceedings{clusterdata:Wang2011, 
    title = {Towards Synthesizing Realistic Workload Traces for Studying the {Hadoop} Ecosystem}, 
    author = {Wang, Guanying and Butt, Ali R. and Monti, Henry and Gupta, Karan}, 
    booktitle = {19th IEEE Annual International Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS'11)}, 
    year = 2011, 
    month = Jul, 
    isbn = {978-0-7695-4430-4}, 
    pages = {400--408}, 
    url = {http://people.cs.vt.edu/~butta/docs/mascots11-hadooptrace.pdf}, 
    doi = {10.1109/MASCOTS.2011.59}, 
    publisher = {IEEE Computer Society}, 
    location = {Raffles Hotel, Singapore}, 
    keywords = {Cloud computing, Performance analysis, Design optimization, Software performance modeling}, 
    abstract = {Designing cloud computing setups is a challenging task. It involves understanding the impact of a plethora of parameters ranging from cluster configuration, partitioning, networking characteristics, and the targeted applications' behavior. The design space, and the scale of the clusters, make it cumbersome and error-prone to test different cluster configurations using real setups. Thus, the community is increasingly relying on simulations and models of cloud setups to infer system behavior and the impact of design choices. The accuracy of the results from such approaches depends on the accuracy and realistic nature of the workload traces employed. Unfortunately, few cloud workload traces are available (in the public domain). In this paper, we present the key steps towards analyzing the traces that have been made public, e.g., from Google, and inferring lessons that can be used to design realistic cloud workloads as well as enable thorough quantitative studies of Hadoop design. Moreover, we leverage the lessons learned from the traces to undertake two case studies: (i) Evaluating Hadoop job schedulers, and (ii) Quantifying the impact of shared storage on Hadoop system performance.}
}

papers that use the traces

Papers that use the traces for other research, rather than describing them.

@Article{clusterdata:Mishra2010,
   author = {Mishra, Asit K. and Hellerstein, Joseph L. and Cirne,
 	Walfredo and Das, Chita R.},
   title = {Towards characterizing cloud backend workloads: insights
 	from {Google} compute clusters},
   journal = {SIGMETRICS Perform. Eval. Rev.},
   volume = {37},
   number = {4},
   month = Mar,
   year = 2010,
   issn = {0163-5999},
   pages = {34--41},
   numpages = {8},
   url = {http://doi.acm.org/10.1145/1773394.1773400},
   doi = {10.1145/1773394.1773400},
   publisher = {ACM},
   abstract = {The advent of cloud computing promises highly available,
 	efficient, and flexible computing services for applications
 	such as web search, email, voice over IP, and web search
 	alerts. Our experience at Google is that realizing the
 	promises of cloud computing requires an extremely scalable
 	backend consisting of many large compute clusters that are
 	shared by application tasks with diverse service level
 	requirements for throughput, latency, and jitter. These
 	considerations impact (a) capacity planning to determine which
 	machine resources must grow and by how much and (b) task
 	scheduling to achieve high machine utilization and to meet
 	service level objectives.
 

Both capacity planning and task scheduling require a good understanding of task resource consumption (e.g., CPU and memory usage). This in turn demands simple and accurate approaches to workload classification-determining how to form groups of tasks (workloads) with similar resource demands. One approach to workload classification is to make each task its own workload. However, this approach scales poorly since tens of thousands of tasks execute daily on Google compute clusters. Another approach to workload classification is to view all tasks as belonging to a single workload. Unfortunately, applying such a coarse-grain workload classification to the diversity of tasks running on Google compute clusters results in large variances in predicted resource consumptions.

This paper describes an approach to workload classification and its application to the Google Cloud Backend, arguably the largest cloud backend on the planet. Our methodology for workload classification consists of: (1) identifying the workload dimensions; (2) constructing task classes using an off-the-shelf algorithm such as k-means; (3) determining the break points for qualitative coordinates within the workload dimensions; and (4) merging adjacent task classes to reduce the number of workloads. We use the foregoing, especially the notion of qualitative coordinates, to glean several insights about the Google Cloud Backend: (a) the duration of task executions is bimodal in that tasks either have a short duration or a long duration; (b) most tasks have short durations; and (c) most resources are consumed by a few tasks with long duration that have large demands for CPU and memory.}, } 
