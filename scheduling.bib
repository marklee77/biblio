***************
** Letter: A **
***************

The authors of this paper actually do some PCA to build models of CPU
utilization! Of course, I'm not quite sure we can use their models or if their
applications are representative of the scenarios that we're interested in, but
it's better than nothing!

tags: WORKLOAD, RESOURCE REQUIREMENTS, CPU

@techreport{abrahao2004caw,
  author       = {Abrahao, Bruno and Zhang, Alex},
  title        = {Characterizing Application Workloads on {CPU} Utilization for Utility Computing},
  institution  = hpl,
  number       = {{HPL}-2004-157},
  month        = sep,
  year         = 2004,
  url          = {http://www.hpl.hp.com/techreports/2004/HPL-2004-157.html}
}

This is yet another autonomic task placement / resource allocation for service
hosting using VMs paper. They model SLAs based on the probability of exceeding a
resource time, and also separate SLAs for the case of normal operations
throughput and the case of surges. It is assumed that violations result in a
penalty, and the goal is to maximize profit. Their algorithm is to use a
nonlinear constraint satisfaction problem solver, which they empirically
determine runs quickly enough to generate solutions in real-world scenarios.
Unfortunately, they avoid deep discussion of their method of resource need
forecasting.

tags: SERVICE HOSTING, RESOURCE ALLOCATION, VM

@inproceedings{abrahao2006sas,
  author       = {Abrahao, Bruno and Almeida, Virgilio and Almeida, Jussara and Zhang, Alex and Beyer, Dirk and Safai, Fereydoon},
  title        = {Self-Adaptive {SLA}-Driven Capacity Management for Internet Services},
  crossref     = {noms06},
  pages        = {557--568}
}


Basically, most workstations have a lot of free memory most of the time, and we can model that.

tags: NOW, MEMORY

@article{acharya1999aui,
  author       = {Acharya, Anurag and Setia, Sanjeev K.},
  title        = {Availability and utility of idle memory in workstation clusters},
  journal      = acm_sigmetrics_per,
  volume       = 27,
  number       = 1,
  year         = 1999,
  pages        = {35--46}
}

This paper is about a linux module that implements co-scheduling for a cluster.
The implemented schemes are spin-block, priority boost, dynamic co-scheduling,
and the authors' own scheme, coordinated co-scheduling. They perform
experiments using a real 16-node linux cluster with a workload made up of apps
taken from the NAS parallel benchmark and determine that on the target platform
their proposed scheme is the best, followed by SB, PB, and DCS, in that order.

tags: COSCHEDULING

@inproceedings{agarwal2003cct,
  author       = {Agarwal, Saurabh and Choi, Gyu Sang and Das, Chita R. and Yoo, Andy B. and Nagar, Shailabh},
  title        = {Co-ordinated Coscheduling in Time-Sharing Clusters through a Generic Framework},
  crossref     = {cluster03},
  pages        = {84--91}
}

Article Fredo suggested...interesting.

tags: THEORY

@article{agrawal2008awp,
  author       = {Agrawal, Kunal and Leiserson, Charles E. and He, Yuxiong and Hsu, Wen Jing},
  title        = {Adaptive Work-Stealing with Parallelism Feedback},
  journal      = tocs,
  volume       = 26,
  number       = 3,
  year         = 2008,
  pages        = {1--32}
}

Basically, placement of tasks within a node can affect performance. Putting MPI
tasks on the same multicore chip can improve communication speed but also
increase cache contention (duh), while putting them on different chips within
the same node has the opposite affect.

tags: 

@inproceedings{alam2006csw,
  author       = {Alam, Sadif R. and Barrett, Richard F. and Kuehn, Jeffery A. and Roth, Philip C. and Vetter, Jeffrey S.},
  title        = {Characterization of scientific workloads on systems with multi-core processors},
  crossref     = {iiswc06},
  pages        = {225--236}
}

In this paper the author presents a 1.923 competitive online scheduling
algorithm.  Basically the hosts get split into two lists, one of heavily
loaded hosts and one of lightly loaded hosts.  If scheduling an incomming
job to the least loaded of the heavily loaded hosts would not put the 
competitive ratio above 1.923 it is scheduled to that host, otherwise it
is scheduled to the most lightly loaded host.

The author also shows that for m >= 80 no online algorithm can have a
competitive ratio better than 1.852.

tags: THEORY

@article{albers1999bbo,
  author       = {Albers, Susan},
  title        = {Better Bounds for Online Scheduling},
  journal      = siam_computing,
  volume       = 29,
  number       = 2,
  year         = 1999,
  pages        = {459--473}
}

In this paper the authors compare some heuristics for communicating task
placement within a network. The focus is on maintaining QoS guarantees and
minimizing migrations. I should probably read it more thoroughly.

tags: HEURISTICS

@article{ali2008shr,
  author       = {Ali, Shoukat and Kim, Jong-Kook and Siegel, Howard Jay and Maciejewski, Anthony A.},
  title        = {Static heuristics for robust resource allocation of continuously executing applications},
  journal      = jpdc,
  volume       = 68,
  number       = 8,
  year         = 2008,
  pages        = {1070--1080}
}

This paper gives a PTAS for general scheduling problems on machines and deals
specifically with optimizing functions of execution times.

tags: THEORY

@article{alon1998asp,
  author       = {Alon, Noga and Azar, Yossi and W{\"o}ginger, Gerhard J. and Yadid, Tal},
  title        = {Approximation Schemes for Scheduling on Parallel Machines},
  journal      = j_scheduling,
  volume       = 1,
  number       = 1,
  year         = 1998,
  pages        = {55--66}
}

This paper is somewhat relevant since we want to minimally cover the cpu
vector, but we also want to undercover the memory. Maybe we could add some e to
memory and use the proposed algorithm?

tags: THEORY

@article{alon1998ooa,
  author       = {Alon, Noga and Azar, Yossi and Csirik, J{\'a}nos and Epstein, Leah and Sevastianov, Sergey V. and Vestjens, Arjen P. A. and W{\"o}ginger, Gerhard J.},
  title        = {On-line and Off-line Approximation Algorithms for Vector Covering Problems},
  journal      = algorithmica,
  volume       = 21,
  number       = 1,
  year         = 1998,
  pages        = {104--118}
}

Recommended by Burton Smith to me, this one talks about strategies for
migrating jobs with minimal impact, basically. The need to minimize square of
total memory.

tags: HEURISTICS

@inproceedings{alverson1999stm,
  author       = {Alverson, Gail and Kahan, Simon and Korry, Richard and McCann, Cathy and Smith, Burton},
  title        = {Scheduling on the {T}era {MTA}},
  crossref     = {jsspp95},
  pages        = {19--44}
}

This is just the user's guide for LAPACK.

tags: 

@book{anderson1999lug,
  author       = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, S. and Demmel, J. and Dongarra, Jack and Du Croz, J. and Greenbaum, A. and Hammarling, S. and McKenney, A. and Sorensen, D.},
  title        = {{LAPACK} Users' Guide},
  edition      = {3rd},
  publisher    = siam,
  address      = {Philadelphia, {USA}},
  year         = 1999,
  url          = {http://www.netlib.org/lapack/lug/}
}

This paper basically just describes BOINC.

tags: 

@inproceedings{anderson2004bsp,
  author       = {Anderson, David P.},
  title        = {{BOINC}: A System for Public-Resource Computing and Storage},
  crossref     = {grid04},
  pages        = {4--10}
}

This paper talks a bit about the problem of identifying which hosts are
overloaded and which vms need to be moved, but doesn't say much about how to
choose where to put them...the assumption is that the number of overloaded
hosts is much smaller than the total number available. There is some useful
info on CPU needs estimation and trends.

tags: RESOURCE REQUIREMENTS, CLOUD

@inproceedings{andreolini2009dlm,
  author       = {Andreolini, Mauro and Casolari, Sara and Colajanni, Michele and Messori, Michele},
  title        = {Dynamic load management of virtual machines in a cloud architectures},
  crossref     = {cloudcomp09}
}

[1] G. R. Andrews, Foundations of Multithreaded, Parallel and Distributed Programing.
2000.

Addison-Wesley,

@book{andrews2000fmp,
  author       = {Andrews, G. R.},
  title        = {Foundations of Multithreaded, Parallel and Distributed Programming},
  publisher    = {Addison-Wesley},
  year         = 2000
}

This article basically details how knowing the total amount of work to be done
can be used to inform multiprocessor scheduling decisions. The paper is about
integer cpu allocations over a time period, which means that there's an obvious
transformation to a yield-based problem.

tags: HEURISTICS

@article{angelelli2004oms,
  author       = {Angelelli, E. and Nagy, A. B. and Speranza, Maria Grazia and Tuza, Zsolt},
  title        = {The On-line Multiprocessor Scheduling Problem with Known Sum of the Tasks},
  journal      = j_scheduling,
  volume       = 7,
  number       = 6,
  year         = 2004,
  pages        = {421--428}
}

Basically, Dynamic co-scheduling (gang-scheduling only tasks that need it as
determined by network communication metrics) and techniques based on immediate
blocking seem to beat out out spin-locking.

In \cite{anglano2000cei} the authors compare a total of 12 different implicit
co-scheduling schemes based on combinations of the priority boost policies:
none, immediate, and periodic and the spin strategies: spin, block, spin-block
and spin-yield. They use simulation experiments in a simulator that sounds a
lot like simgrid, and workloads based on the CG, MG, LU, and IS benchmarks from
the NAS parallel benchmark suite. Since these benchmarks would not run
directly on the simulator they grabbed timing and communication message sizes
from the benchmarks running on a real cluster. For the simulations they assumed
16 nodes with constant network latencies and context switch times; all 4 of the
applications were started simultaneously running across all nodes. The slowdown
was computed as the ratio of the time for all of the applications to complete
divided by the sums of their individual run times when alone on the system.
They found that the best strategies are either Dynamic Co-scheduling with
spin-block (basically, gang scheduling) and priority boost with immediate
blocking. For the second experiment each of the parallel applications was run
while the individual nodes received cpu-intensive serial jobs with arrival
times based on a hyperexponential distribution and exponentially distributed
run times. This time there was some variation in the results, but still we
basically want to do either DCS or priority boost with some kind of blocking.
Finally, in cases where the jobs did a lot of I/O then priority boost did best,
perhaps because of the inherent priority boost that comes with performing i/o.

tags: COSCHEDULING

@inproceedings{anglano2000cei,
  author       = {Anglano, Cosimo},
  title        = {A comparative evaluation of implicit coscheduling strategies fornetworks of workstations},
  crossref     = {hpdc00},
  pages        = {221--228}
}

In some sense this may represent the competition for 3-tier type systems.

tags: SERVICE HOSTING

@inproceedings{appleby2001osb,
  author       = {Appleby, K. and Fakhouri, S. and Fong, Liana and Goldszmidt, G. and Kalantar, M. and Krishakumar, S. and Pazel, D. P. and Pershing, J. and Rochwerger, B.},
  title        = {{O}c{\'e}ano -- {SLA} based Management of a Computing Utility},
  crossref     = {im01},
  pages        = {855--868}
}

Greedy assignment of tasks to least-loaded hosts is optimal in the case of
restricted assignment with unknown duration when m <= 5, but not for general m.

tags: THEORY

@article{armon2003tta,
  author       = {Armon, Amitai and Azar, Yossi and Epstein, Leah and Regev, Oded},
  title        = {Temporary Tasks Assignment Resolved},
  journal      = algorithmica,
  volume       = 36,
  number       = 3,
  year         = 2003,
  pages        = {295--314}
}

This is basically about reserving portions of all of a resource (CPU) in a
cluster. It assumes a front end node and some resource containers. Probably
good for a foot note, not really our thing at all.

tags: 

@inproceedings{aron2000crm,
  author       = {Aron, Mohit and Druschel, Peter and Zwaenepoel, Willy},
  title        = {Cluster Reserves: A Mechanism for Resource Management in Cluster-based Network Servers},
  crossref     = {sigmetrics00},
  pages        = {90--101}
}

tags: THEORY

@article{arora1998pvh,
  author       = {Arora, Sanjeev and Lund, Carsten and Motwani, Rajeev and Sudan, Madhu and Szegedy, Mario},
  title        = {Proof verification and the hardness of approximation problems},
  journal      = jacm,
  volume       = 45,
  number       = 3,
  year         = 1998,
  pages        = {501--555}
}

This is about using networks of work stations to run parallel as well as
interactive jobs. In general, they found that the number of available machines
is fairly available throughout the day and that a NOW can support about the
same workload as a cluster with half the nubmer of nodes without huge
performance impacts. They also find that co-scheduling is fairly efficient so
long as either time quanta are large or clock-skew is small (check paper for
actual numbers)

tags: NOW, COSCHEDULING

@article{arpaci1995ips,
  author       = {Arpaci, Remzi H. and Dusseau, Andrea Carol and Vahdat, Amin M. and Liu, Lok T. and Anderson, Thomas E. and Patterson, David A.},
  title        = {The interaction of parallel and sequential workloads on a network of workstations},
  journal      = acm_sigmetrics_per,
  volume       = 23,
  number       = 1,
  year         = 1995,
  pages        = {267--278}
}

Spin for a little while and then block. There's a very simplistic workload
simulation that basically involves 3 simulated applications. I should probably
go over that section again in a little more detail.

tags: COSCHEDULING

@phdthesis{arpacidusseau1998icc,
  author       = {Arpaci-Dusseau, Andrea Carol},
  title        = {Implicit coscheduling: coordinated scheduling with implicit information in distributed systems},
  school       = ucb,
  type         = phd,
  month        = dec,
  year         = 1998
}

This is the cliff-notes version of the dissertation, however since it appears
in TOCS and is more recent it should robably be onsidered canonical if there
are any conflicts.

tags: COSCHEDULING

@article{arpacidusseau2001icc,
  author       = {Arpaci-Dusseau, Andrea Carol},
  title        = {Implicit coscheduling: coordinated scheduling with implicit information in distributed systems},
  journal      = tocs,
  volume       = 19,
  number       = 3,
  year         = 2001,
  pages        = {283--331}
}

In this case balancing in the L_p norm refers to minimizing the length of the
vector representing the loads on single-resource server. Minimizing this vector
distributes the load evenly.

tags: THEORY

@article{avidor1998aan,
  author       = {Avidor, A. and Azar, Yossi and Sgall, Ji{\v{r}}{\'i}},
  title        = {Ancient and New Algorithms for Balancing in the {$l_p$} Norm},
  journal      = algorithmica,
  volume       = 29,
  number       = 3,
  year         = 1998,
  pages        = {422--441}
}

The idea is that incoming tasks with known requirements and limited but
unknown duration must be scheduled so as to minimize the maximum load. Each
task can run on only a subset of the available servers, so there may be
something useful here. One caveat: the subset of servers capable of running the
tasks is purely task-dependent, whereas in our system it is dependent on both
the task and previous scheduling decisions. The paper shows that the simple
on-line greedy approach (always pick the least loaded host) has a competitive
ratio of ((3n)^(2/3))/2 and that any deterministic or randomized on-line
algorithm will have a ratio of no less than sqrt(n).

tags: THEORY

@inproceedings{azar1992olb,
  author       = {Azar, Yossi and Broder, Andrei Z. and Karlin, Anna R.},
  title        = {On-line load balancing},
  crossref     = {focs92},
  pages        = {218--225}
}

The competitive ratio for any on-line deterministic algorithm that assigns
temporary tasks to identical machines is 2-1/m, slightly better for randomized.
Also, list-scheduling (greedy) has this competitive ratio.

tags: THEORY

@inproceedings{azar1997bti,
  author       = {Azar, Yossi and Epstein, Leah},
  title        = {On-Line Load Balancing of Temporary Tasks on Identical Machines},
  crossref     = {istcs97},
  pages        = {119--125}
}

This paper describes ROBIN-HOOD, which is a potential task-placement
algorithm...  It has a competitive ratio of sqrt(n), which is as good as it
gets, but in practice we have a bigger problem with tasks having to wait to
run.

tags: THEORY

@article{azar1997lbt,
  author       = {Azar, Yossi and Kalyanasundaram, Bala and Plotkin, Serge and Pruhs, Kirk R. and Waarts, Orli},
  title        = {On-Line Load Balancing of Temporary Tasks},
  journal      = j_algorithms,
  volume       = 22,
  number       = 1,
  year         = 1997,
  pages        = {93--110}
}

tags: 

@article{azar2000ral,
  author       = {Azar, Yossi and Epstein, Leah and van Stee, Rob},
  title        = {Resource Augmentation in load balancing},
  journal      = j_scheduling,
  volume       = 3,
  year         = 2000,
  pages        = {249--258}
}

The worst-case competitive ratio of any deterministic on-line algorithm is at
least 4/3. They also give the heuristic that divides bins and tasks into short,
medium, and tall.

tags: THEORY

@article{azar2001obs,
  author       = {Azar, Yossi and Regev, Oded},
  title        = {On-Line Bin-Stretching},
  journal      = tcs,
  volume       = 268,
  year         = 2001,
  pages        = {17--41}
}

tags: THEORY

@article{azar2002ott,
  author       = {Azar, Yossi and Regev, Oded and Sgall, Ji{\v{r}}{\'i} and Woeginger, Gerhard J.},
  title        = {Off-line temporary tasks assignment},
  journal      = tcs,
  volume       = 287,
  year         = 2002,
  pages        = {419--428}
}

This is unrestricted assignment of temporary tasks to identical machines to
minimize the l_p norm. Greedy has a competitive ratio of at most 2.23, and any
deterministic algorithm will have a competitive ratio of at least 1.44. Has
some interesting arguments using formalized "shapes".

tags: THEORY

@inproceedings{azar2003lbl,
  author       = {Azar, Yossi and Epstein, Amir and Epstein, Leah},
  title        = {Load balancing of temporary tasks in the {$l_p$} norm},
  crossref     = {waoa03},
  pages        = {53--66}
}

***************
** Letter: B **
***************

The authors of this paper are mostly concerned with proper scheduling of vcpus
on real cpus for multithreaded parallel processes contained in a single VM. So
it's relevant but not quite our area at the moment.

tags: VM, COSCHEDULING, XEN

@inproceedings{bai2010tab,
  author       = {Bai, Yuebin and Xu, Cong and Li, Zhi},
  title        = {Task-aware based Co-scheduling for Virtual Machine System},
  crossref     = {sac10}
}

tags: BENCHMARK

@techreport{bailey1995npb,
  author       = {Bailey, David and Harris, Tim and Saphir, William and van der Wijngaart, Rob and Woo, Alex and Yarrow, Maurice},
  title        = {The {NAS} Parallel Benchmarks 2.0},
  institution  = nasa_supercomputing,
  number       = {{NAS}-95-020},
  year         = 1995,
  url          = {http://www.nas.nasa.gov/News/Techreports/1995/PDF/nas-95-020.pdf} 
}

The Authors basically discuss their method of predicting application
performance on different architectures. They talk about both extrapolating from
the signatures and using simulations. If there is too much of the latter this
method might be too slow to be practical to use for our purposes.

tags: PERFORMANCE MODELING

@incollection{bailey2005pmu,
  author       = {Bailey, David H. and Snavely, Allan},
  title        = {Performance Modeling: Understanding the Past and Predicting the Future},
  crossref     = {europar05},
  pages        = {185--195}
}

tags: 

@book{baker1974iss,
  author       = {Baker, Kenneth R.},
  title        = {Introduction to Sequencing and Scheduling},
  publisher    = wiley_and_sons,
  year         = 1974
}

This seems to be a whitepaper released by the TFCC, it actually contains a
number of smaller articles, so it might be appropriate to do inbook for those
instead of doing it this way...

tags: CLUSTER

@misc{baker2001ccw,
  author       = {Baker, Mark A.},
  title        = {Cluster Computing Whitepaper},
  month        = dec,
  year         = 2001,
  howpublished = {arXiv:cs/0004014v2}
}

@article{baldi2012gbp,
  author       = {Baldi, Mauri Maria and Crainic, Teodor Gabriel and Perboli, Guido and Tadei, Roberto},
  title        = {The generalized bin packing problem},
  journal      = {Transportation and Research Part E: Logistics and Transportation Review},
  volume       = 48,
  number       = 6,
  month        = nov,
  year         = 2012,
  pages        = {1205--1220}
}

This paper establishes some lower bounds on the competitive ratio for bin
packing problems where bounded repacking is allowed, which has obvious
relavance, but may not be directly applicable. It also talks about dynamic
bin-packing, which allows the deletion of elements.

tags: THEORY, ADAPTATION

@article{balogh2008lbo,
  author       = {Balogh, J{\'a}nos and B{\'e}k{\'e}si, J{\'o}zsef and Galambos, G{\'a}bor and Reinelt, Gerhard},
  title        = {Lower Bound for the Online Bin Packing Problem with Restricted Repacking},
  journal      = siam_computing,
  volume       = 38,
  number       = 1,
  year         = 2008,
  pages        = {398--410}
}

tags: THEORY, ADAPTATION

@article{balogh2009ilb,
  author       = {Balogh, J{\'a}nos and B{\'e}k{\'e}si, J{\'o}zsef and Galambos, G{\'a}bor and Mark{\'o}t, Mih{\'a}ly Csaba},
  title        = {Improved lower bounds for semi-online bin packing problems},
  journal      = computing,
  volume       = 84,
  number       = 1,
  year         = 2009,
  pages        = {139--148}
}

In a nutshell: even though slow inter-cluster links can negatively affect the
performance of jobs run on multiple clusters, the benefit to overall performance
can make the trade-off worthwhile.

tags: MULTICLUSTER

@incollection{banen2003mbs,
  author       = {Banen, S. and Bucur, Anca I.D. and Epema, Dick H.J.},
  title        = {A Measurement-Based Simulation Study of Processor Co-allocation in Multicluster Systems},
  crossref     = {jsspp03},
  pages        = {184--204}
}

The point of this paper is basically that SRPT isn't as bad as people think.
While it can theoretically lead to infinite wait times for large jobs, in
practice even for job size distributions with the heavy-tailed property,
meaning that less than 1% of the jobs contribute more than 90% of the load,
this doesn't really happen. In fact, for some workloads all jobs will prefer
SRPT to PS, while for others the penalties for large jobs aren't as severe as
commonly believed. This may have something to do with the fact that jobs tend
to shrink as they remain resident in the system, so eventually even those jobs
which start out as large will benefit from SRPT when they become sufficiently
small.

Very Theory oriented, may show a way to get more math in...

tags: THEORY

@inproceedings{bansal2001asi,
  author       = {Bansal, Nikhil and Harchol-Balter, Mor},
  title        = {Analysis of {SRPT} Scheduling: Investigating Unfairness},
  crossref     = {sigmetrics01},
  pages        = {279--290}
}

This is pretty cool. Might be worth a reference, but not directly applicable at
this time.

tags: THEORY, ENERGY

@inproceedings{bansal2004dss,
  author       = {Bansal, Nikhil and Kimbrel, Tracy and Pruhs, Kirk R.},
  title        = {Dynamic Speed Scaling to Manage Energy and Temperature},
  crossref     = {focs04},
  pages        = {520--529}
}

This paper basically provides some justification for our chosen priority function!

tags: THEORY, HEURISTICS

@article{bansal2004nsm,
  author       = {Bansal, Nikhil and Dhamdhere, Kedar and K{\"o}nemann, Jochen and Sinha, Amitabh},
  title        = {Non-Clairvoyant Scheduling for Minimizing Mean Slowdown},
  journal      = algorithmica,
  volume       = 40,
  number       = 4,
  year         = 2004,
  pages        = {305--318}
}

This paper explores approximation algorithms for 2-d rectangle and vector
packing. The basic idea is to get a basic feasible solution from the relaxation
of an ILP and then use a greedy algorithm with some specific properties to pack
the rest of the items.

tags: THEORY

@inproceedings{bansal2006iaa,
  author       = {Bansal, Nikhil and Caprara, Alberto and Sviridenko, Maxim},
  title        = {Improved approximation algorithms for multidimensional bin packing problems},
  crossref     = {focs06},
  pages        = {697--708}
}

They pack rectangles rather than vectors, but otherwise a very similar problem.

tags: THEORY

@article{bansal2007tdb,
  author       = {Bansal, Nikhil and Sviridenko, Maxim},
  title        = {Two-dimensional bin packing with one-dimensional resource augmentation},
  journal      = discopt,
  volume       = 4,
  number       = 2,
  year         = 2007,
  pages        = {143--153}
}

Give some props to the undergrads.

tags: VM CLUSTER

@inproceedings{barcelo2008pcv,
  author       = {Barcelo, Neal and Legg, Nick and Bressoud, Thomas},
  title        = {The Performance Cost of Virtual Machines on Big Data Problems in Compute Clusters},
  crossref     = {mcurcsm08},
  pages        = {22--29},
  url          = {http://www3.wooster.edu/cs/mcurcsm2008/papers/vmclusters.pdf}
}

This paper describes Xen. I should probably read it more thoroughly.

tags: VM

@inproceedings{barham2003xav,
  author       = {Barham, Paul and Dragovic, Boris and Fraser, Keir and Hand, Steven and Harris, Tim and Ho, Alex and Neugebauer, Rolf and Pratt, Ian and Warfield, Andrew},
  title        = {{X}en and the Art of Virtualization},
  crossref     = {sosp03},
  pages        = {164--177}
}

This work details the SCOJO-P scheduler, which makes predictions about future
system load and uses that information to make decisions about how many nodes to
allocate to moldable and malleable jobs. Also, periodically the system
considers whether changing the number of nodes allocated to malleable jobs
would improve efficiency. This is very similar to our own work in many ways,
though of course they only consider integral host assignments. It may be
possible to use this work to complement our own and support changing the number
of allocated VMs to jobs, though in this case the interaction would be very
complicated. This idea is probably best left for future work after Flexible
Scheduling is well established.

tags: HEURISTICS

@incollection{barsanti2006ajs,
  author       = {Barsanti, Lawrence and Sodan, Angela C.},
  title        = {Adaptive Job Scheduling Strategies via Predictive Job Resource Allocation},
  crossref     = {jsspp06},
  pages        = {115--140}
}

The authors of this paper study the classic on-line scheduling problem in depth
and come up with a 2-epsilon competitive algorithm, which beats out list
scheduling (greedy). The basic idea is to keep the hosts split up into lightly
and heavily loaded groups and only place tasks onto lightly loaded hosts if
placing it on a large one would raise the max makespan above 2-epsilon times
the average.

tags: THEORY

@inproceedings{bartal1992naa,
  author       = {Bartal, Yair and Fiat, Amos and Karloff, Howard and Vohra, Rakesh},
  title        = {New Algorithms for an Ancient Scheduling Problem},
  crossref     = {stoc92},
  pages        = {51--58}
}

The authors of this paper show that task-allocation as 2-d vector bin packing is
tractable if the number of possible types of tasks is limited and give a p-time
algorithm for this case.

tags: SCHEDULING, VECTOR PACKING

@inproceedings{baruah2004dpa,
  author       = {Baruah, Sanjoy and Fisher, Nathan},
  title        = {A Dynamic Programming Approach to Task Partitioning upon Memory-constrained Multiprocessors},
  crossref     = {rtcsa04}
}

In this paper the authors do a short study on predicting job memory
requirements both from an analysis of the static binary and records of past
behavior, and then proceed to show how these predictions can be used in a gang
scheduling system to improve real performance by lowering memory pressure and
thus avoiding swaps to disk. It's nice and relevant, but there are some
questions about whether the methods used to predict memory requirements are
accurate in general.

tags: GANG, MEMORY

@inproceedings{batat2000gsm,
  author       = {Batat, Anat and Feitelson, Dror G.},
  title        = {Gang Scheduling with Memory Considerations},
  crossref     = {ipdps00},
  pages        = {109--114}
}

This is kind of interesting in that it opens up a whole new area of research
exploration, but it addresses object, as opposed to vector, packing, and so is
not DIRECTLY applicable.

tags: THEORY, BIN PACKING

@inproceedings{batu1999fap,
  author       = {Batu, Tu{\v{g}}kan and Rubinfeld, Ronitt and White, Patrick},
  title        = {Fast Approximate {PCP}s for Multidimensional Bin-Packing Problems},
  crossref     = {random99},
  pages        = {245--256}
}

Okay, they assume serial batch jobs that are run multiple times and base a model
on resource utilization when alone on the system. Their model is to assume that
applications have distinct "stages", where resource consumption of cpu, disk
i/o, network bandwidth and memory is mostly consistent during a stage. That is,
the model consists of a sequence of average values. The use aggregate cpu
consumption to try to guess which stage the current execution is in. They note a
linear correspondence between resource requirements, (e.g., throttling cpu to
50% also reduces disk i/o and network bandwidth by the same factor), except for
memory, where underprovisioning leads to swapping and unpredictable impacts.

tags: BATCH, VM, RESOURCE ALLOCATION, RESOURCE REQUIREMENTS

@inproceedings{becerra2009bjp,
  author       = {Becerra, Yolanda and Carrera, David and Ayguad{\'e}, Eduard},
  title        = {Batch Job Profiling and Adaptive Profile Enforcement for Virtualized Environments},
  crossref     = {pdp09},
  pages        = {414--418}
}

Heterogeneous vector bin packing heuristics for jobs scheduling! It also
considers the need to avoid overloading the data bus.

tags: VM, VECTOR PACKING

@inproceedings{beck1996mmt,
  author       = {Beck, James E. and Siewiorek, Daniel P.},
  title        = {Modeling Multicomputer Task Allocation as a Vector Packing Problem},
  crossref     = {isss96},
  pages        = {115--120}
}

The primary contribution of this paper is that it defines the max-flow and
max-stretch metrics. The heuristics they propose to optimize max-stretch are
based on an ideal algorithm called Dynamic Earliest Deadline First (DEDF). The
variations are ALL, SYS, and LASTK, so called because they use all jobs, active
jobs in the system, or last k jobs for estimating job completion deadlines.

tags: THEORY, STRETCH, METRICS

@inproceedings{bender1998fsm,
  author       = {Bender, Michael A. and Chakrabarti, Soumen and Muthukrishnan, S.},
  title        = {Flow and Stretch Metrics for Scheduling Continuous Job Streams},
  crossref     = {soda98},
  pages        = {270--279}
}

They give some PTAS's for average-stretch optimization when processing times
are known and competitive ratios for the non-clairvoyant case. Doesn't Fredo
have a PTIME algorithm for max-stretch optimization? Need to compare this with
avg stretch optimization. Also, gives a simple proof that optimizing avg
weighted completion time is the same as avg weighted flow time.

tags: THEORY, STRETCH

@article{bender2004aaa,
  author       = {Bender, Michael A. and Muthukrishnan, S. and Rajaraman, Rajmohan},
  title        = {Approximation Algorithms for Average Stretch Scheduling},
  journal      = j_scheduling,
  volume       = 7,
  number       = 3,
  year         = 2004,
  pages        = {195--222}
}

In this paper the authors examine the Sum-of-Squares bin packing heuristic.
This heuristic seeks to minimize the sum of the squres of the counts of bins
with given gap-sizes. It's sort-of the dual of the Lp norm idea above.

tags: HEURISTICS, BIN PACKING

@article{bender2008shb,
  author       = {Bender, Michael A. and Bradley, Bryan and Jagannathan, Geetha and Pillaipakkamnatt, Krishnan},
  title        = {Sum-of-Squares Heuristics for Bin Packing and Memory Allocation},
  journal      = jea,
  volume       = 12,
  month        = jun,
  year         = 2008,
  pages        = {1--19}
}

@misc{bender2013rps,
  author       = {Bender, Michael A. and Farach-Colton, Martin and Fekete, S{\'a}ndor P. and Fineman, Jeremy T. and Gilbert, Seth}
  title        = {Reallocation Problems in Scheduling},
  month        = may,
  year         = 2013,
  howpublished = {arXiv:1305.6555}
}

A vm cluster paper that considers both online (service hosting) and batch
workloads. Some discussion of traces and how they run their simulations, but
nothing we can work with directly.

tags: RESOURCE REQUIREMENTS, PERFORMANCE MODELING, RESOURCE ALLOCATION, SERVICE HOSTING, UTILITY

@inproceedings{bennani2005raa,
  author       = {Bennani, Mohamed N. and Menasc{\'e}, Daniel A.},
  title        = {Resource allocation for autonomic data centers using analytic performance models},
  crossref     = {icac05},
  pages        = {229--240}
}

tags:

@book{bertsekas1992dn,
  author       = {Bertsekas, D. P. and Gallager, R.G.},
  title        = {{Data Networks}},
  edition      = {2nd},
  year         = 1992,
  publisher    = prentice_hall
}

@inproceedings{bhatia2006bme,
  author       = {Bhatia, Nikhil and Alam, Sadaf R. and Vetter, Jeffrey S.},
  title        = {Performance Modeling of Emerging {HPC} Architectures},
  crossref     = {hpcmp06},
  pages        = {367--373}  
}

This paper bascially describes some software developed by the authors for
managing virtual clusters. In many ways it is similar in concept to Usher. The
authors yet again say that they leave load balancing policies to the
administrator, which is where we come in.

tags: VM CLUSTER

@incollection{bhatia2007vcm,
  author       = {Bhatia, Nikhil and Vetter, Jeffrey S.},
  title        = {Virtual Cluster Management with {X}en},
  crossref     = {epw07},
  pages        = {185--194}
}

This is a simple-ish paper, but also has a bin-packing angle.

Contains reference to Gartner Group report claiming typical server utilization
is around 20%.

There are also some references here on using virtualization to improve
utilization, but the exact sources are unclear.

tags: VM CLUSTER

@inproceedings{bichler2006cpv,
  author       = {Bichler, Martin and Setzer, Thomas and Speitkamp, Benjamin},
  title        = {Capacity Planning for Virtualized Servers},
  crossref     = {wits06}
}

README README README CITEME

Oh wow. This paper is super relevant. Identification of candidates for moving
based on predictors, charatarization of CPU utilization, the works. Need to
re-read thoroughly and go over citations.

tags: VM CLUSTER, SLA, RESOURCE REQUIREMENTS, SERVICE HOSTING

@inproceedings{bobroff2007dpv,
  author       = {Bobroff, Norman and Kochut, Andrzej and Beaty, Kirk},
  title        = {Dynamic Placement of Virtual Machines for Managing {SLA} Violations},
  crossref     = {im07},
  pages        = {119--128}
}

This paper basically says that it's feasible to launch thousands of virtual
machines at a time.

tags: VM

@incollection{bobroff2009saj,
  author       = {Bobroff, Norman and Coppinger, Richard and Fong, Liana and Seelam, Seetharami and Xu, Jing},
  title        = {Scalability Analysis of Job Scheduling using Virtual Nodes},
  crossref     = {jsspp09},
  pages        = {190--206}
}

These guys run a simulation based on the 1998 world cup web server request
trace, which is a little dated. They found a linear regression based on the last
5 minutes, which seems consistent with a lot of these kind of studies.  They
suggest using an approach based on statistical machine learning to generate
models of expected needs.

tags: ENERGY, VM, RESOURCE ALLOCATION

@techreport{bodik2008acf,
  author       = {Bodik, Peter and Armbrust, Michael and Canini, Kevin and Fox, Armando and Jordan, Michael and Patterson, David},
  title        = {A Case for Adaptive Datacenters to Conserve Energy and Improve Reliability},
  institution  = ucbeecs,
  number       = {{UCB/EECS}-2008-127},
  month        = sep,
  year         = 2008
}

tags: MPI, GRID

@inproceedings{bouteiller2004hps,
  author       = {Bouteiller, Aur{\'e}lien and Bouziane, Hinde-Lilia and Herault, Thomas and Lemarinier, Pierre and Cappello, Frank},
  title        = {Hybrid Preemptive Scheduling of {MPI} Applications on Grids},
  crossref     = {grid04},
  pages        = {130--137}
}

This paper describes CoolRunnings, which is basically a system that uses
real-time measurements of load and predictions about likely future load based
on historical information to try to maintain a "capacity window" of a certain
size around the current and predicted values. That is, it turns machines on and
off depending on whether they are currently needed or predicted to be likely to
be needed in the near future. They do some modeling of 3-tier system workloads
that includes modeling CPU and memory utilization, but it isn't the main thrust
and useful info on this topic is difficult to extract from just the paper.

tags: ENERGY

@article{bradley2003wpm,
  author       = {Bradley, D. J. and Harper, R. E. and Hunter, S. W.},
  title        = {Workload-based Power Management for parallel computer systems},
  journal      = ibm_jrd,
  volume       = 47,
  number       = {5--6},
  year         = 2003,
  pages        = {703--718}
}

This rather lenghty and pedantic work compares 11 different static scheduling
heuristics for distributed envirnonments with heterogeneous resources. This
might have some applicability on scheduling new tasks onto partially loaded
hosts. Unfortunately, they do assume that the schedule is computed offlie and
runtimes are known in advance. They compare Opportunistic Load Balancing,
Minimum Execution Time, Minimum Completion Time, Min-min, Max-min, Duplex,
Genetic Algorithm, Simulated Annealing, Genetic Simulated Annealing, Tabu and
A*. Long story short: genetic wins, but Min-min and duplex perform nearly as
well and have orders of magnitude shorter run times.

tags: HEURISTICS

@article{braun2001ces,
  author       = {Braun, Tracy D. and Siegel, Howard Jay and Beck, Noah and B{\"o}l{\"o}ni, Ladislau L. and Maheswaran, Muthucumaru and Reuther, Albert I. and Robertson, James P. and Theys, Mitchell D. and Yao, Bin and Hensgen, Debra and Freund, Richard F.},
  title        = {A Comparison of Eleven Static Heuristics for Mapping a Class of Indepedent Tasks onto Heterogeneous Distributed Computing Systems},
  journal      = jpdc,
  volume       = 61,
  year         = 2001,
  pages        = {810--837}
}

tags:

@inproceedings{brecht1993oip,
  author       = {Brecht, Timothy},
  title        = {On the importance of parallel application placement in {NUMA} multiprocessors},
  crossref     = {sedms93},
  pages        = {1--18}
}

tags:

@phdthesis{brecht1994mpa,
  author       = {Brecht, Timothy},
  title        = {Multiprogrammed Parallel Application Scheduling in {NUMA} Multiprocessors},
  type         = phd,
  school       = uto,
  month        = jun,
  year         = 1994
}

tags:

@book{brucker2007sal,
  author       = {Brucker, P.},
  title        = {Scheduling Algorithms},
  edition      = {5th},
  publisher    = springer,
  year         = 2007
}

tags:

@inproceedings{bruno2004rms,
  author       = {Bruno, G. and Katz, M. J. and Sacerdoti, F. D. and Papadopoulos, P. M.},
  title        = {Rolls: modifying a standard system installer to support user-customizable cluster frontend appliances},
  crossref     = {cluster04},
  pages        = {421--430}
}

This paper basically describes the VAMPIR approach for performance profiling on
clusters. No actual profiling results.

tags: 

@incollection{brunst2001pol,
  author       = {Brunst, Holger and Winkler, Manuela and Nagel, Wolfgang E. and Hoppe, Hans-Christian},
  title        = {Performance Optimization for Large Scale Computing: The Scalable {VAMPIR} Approach},
  crossref     = {iccs01},
  pages        = {751--760}
}

more on Vampir, also Kojak

tags:

@incollection{brunst2005pal,
  author       = {Brunst, Holger and Mohr, Bernd},
  title        = {Performance Analysis of Large-Scale {OpenMP} and Hybrid {MPI/OpenMP} Applications with {Vampir NG}},
  crossref     = {iwomp05},
  pages        = {5--14}
}

Just a description of a framework for dynamic adaptation.

tags:

@inproceedings{buisson2005fda,
  author       = {Buisson, J{\'e}r{\'e}my and Andr{\'e}, Fran{\c{c}}oise and Pazat, Jean-Louis},
  title        = {A Framework for Dynamic Adaptation of Parallel Components},
  crossref     = {parco05},
  pages        = {65--72}
}

An example of exotic cluster hardware.

tags:

@techreport{buttari2007lp3,
  author       = {Buttari, Alfredo and Kurzak, Jakub and Dongarra, Jack},
  title        = {Limitations of the {P}lay{S}tation 3 for High Performance Cluster Computing},
  institution  = utkicl,
  number       = {{UT}-{CS}-07-597},
  month        = apr,
  year         = 2007,
  pdf          = {http://icl.cs.utk.edu/~buttari/mypapers/tech_summacell.pdf}
}

***************
** Letter: C **
***************

tags:

@inproceedings{calandrino2008crs,
  author       = {Calandrino, John M. and Anderson, James H.},
  title        = {Cache-aware real-time scheduling on multicore platforms: Heuristics and a case study},
  crossref     = {ecrts08},
  pages        = {209--308}
}

The objective of workload characterization is to derive a model to be able to
show, capture, and reproduce workload behavior. Traditionally, researchers try
to find a probability distribution which matches the given statistics and also
attempt to look for clustering in multidimensional space. There's some very
general description of the problems and process of workload characterization,
but hard data on real workloads is included mostly anecdotally.

tags: WORKLOAD

@incollection{calzarossa2000wci,
  author       = {Calzarossa, Maria and Massari, Luissa and Tessera, Daniele},
  title        = {Workload Characterization Issues and Methodologies},
  booktitle    = {Performance Evaluation: Origins and Directions},
  editor       = {Haring, G{\"u}nter and Lindemann, Christoph and Reiser, Martin},
  series       = lncs,
  volume       = 1769,
  year         = 2000,
  pages        = {459--482},
  publisher    = springer
}

tags: VM CLUSTER

@phdthesis{calzolari2006hav,
  author       = {Calzolari, Federico},
  title        = {High Availability Using Virtualization},
  type         = phd,
  school       = udp,
  year         = 2006
}

@inproceedings{campbell2009occ,
  author      = {Campbell, Roy and Gupta, Indranil and Heath, Michael and Ko, Steven Y. and Kozuch, Michael and Kunze, Marcel and Kwan, Thomas and Lai, Kevin and Lee, Hing Yan and Lyons, Martha and Milojicic, Dejan and O'Halloran, David and Soh, Yeng Chai},
  title       = {{Open Cirrus\textsuperscript{\texttrademark}} Cloud Computing Testbed: Federated Data Centers for Open Source Systems and Services Research},
  crossref    = {hotcloud09}
}

Okay, so they basically give a MILP formulation, some heuristics, and some
exact algorithms and compare them. One nice thing they do that we don't is
prove some approximability results, so maybe we should consider that problem.
Also, the heuristics look good and we should consider them.

tags: THEORY, BIN PACKING, VECTOR PACKING

@article{caprara2001lba,
  author       = {Caprara, Alberto and Toth, Paolo},
  title        = {Lower bounds and algorithms for the 2-dimensional vector packing problem},
  journal      = dam,
  volume       = 111,
  number       = 3,
  year         = 2001,
  pages        = {231--262}
}

This paper has an APTAS for the vector packing when there is a total order on
the vectors s.t.  a <= b iff a_i <= b_i for all i.  Not directly relevant to
our problem.

tags: THEORY, BIN PACKING, VECTOR PACKING

@article{caprara2003aso,
  author       = {Caprara, Alberto and Kellerer, Hans and Pferschy, Ulrich},
  title        = {Approximation schemes for ordered vector packing problems},
  journal      = nrl,
  volume       = 50,
  number       = 1,
  year         = 2003,
  pages        = {58--69}
}

This is pretty nifty. They define a utility function based on application
performance and priority, and then turn it around and define a function that
computes appropriate cpu allocations for a given application and utility. Next
they try to maximize the minimum utility using established techniques.

tags: UTILITY, FAIRNESS

@inproceedings{carrera2008upd,
  author       = {Carrera, David and Steinder, Malgorzata and Whalley, Ian and Torres, Jordi and Ayguad{\'e}, Eduard},
  title        = {Utility-based placement of dynamic web applications with fairness goals},
  crossref     = {noms08},
  pages        = {9--16}
}

@inproceedings{casanova2008sgf,
  author       = {Casanova, Henri and Legrand, Arnaud and Quinson, Martin},
  title        = {SimGrid: a Generic Framework for Large-Scale Distributed Experiments},
  crossref     = {uksim08}
}

Simulation study of scheduling competing parallel task graphs. Not much in the
way of theoretical results, but interesting and potentially relevant to
yield-based scheduling approaches.

tags: SCHEDULING, PTG

@article{casanova2010ocr,
  author       = {Casanova, Henri and Desprez, Fr{\'e}d{\'e}ric and Suter, Fr{\'e}d{\'e}ric},
  title        = {On Cluster Resource Allocation for Multiple Parallel Task Graphs},
  journal      = jpdc,
  volume       = 70,
  number       = 12,
  year         = 2010,
  pages        = {1193--1203}
}

tags: SCHEDULING, PTG

@inproceedings{casanova2010msm,
  author       = {Casanova, Henri and Desprez, Fr{\'e}d{\'e}ric and Suter, Fr{\'e}d{\'e}ric},
  title        = {Minimizing Stretch and Makespan of Multiple Parallel Task Graphs via Malleable Allocations},
  crossref     = {icpp10},
}

On the targeted multicore system around 50% of the communication events seem to
be intra-node, which is surprising. Thus middleware should optimize for
intra-node communication and be aware of potential cache contention.

tags:

@inproceedings{chai2007uim,
  author       = {Chai, Lei and Gao, Qi and Panda, Dhabaleswar K.},
  title        = {Understanding the Impact of Multi-Core Architecture in Cluster Computing: A Case Study with Intel Dual-Core System},
  crossref     = {ccgrid07},
  pages        = {471--478}
}

@article{chakraborty2011sov,
  author       = {Chakraborty, Koushik and Wells, Philip M. and Sohi, Gurindar S.},
  title        = {Supporting Overcommitted Virtual Machines Through Hardware Spin Detection},
  journal      = tpds,
  year         = 2011,
  note         = {preprint}
}

tags: THEORY, DYNAMIC BIN PACKING

@inproceedings{chan2005dbp,
  author       = {Chan, Wun-Tat and Lam, Tak-Wah and Wong, Prudence W. H.},
  title        = {Dynamic Bin Packing of Unit Fractions Items},
  crossref     = {icalp05},
  pages        = {614--626}
}

tags: THEORY, DYNAMIC BIN PACKING

@article{chan2009odb,
  author       = {Chan, Joseph Wun-Tat and Wong, Prudence W. H. and Yung, Fencol C. C.},
  title        = {On Dynamic Bin Packing: An Improved Lower Bound and Resource Augmentation Analysis},
  journal      = algorithmica,
  volume       = 53,
  number       = 2,
  year         = 2009,
  pages        = {172--206}
}

They have a system monitoring model, a prediction module, and an allocation
module. They try to allocate resources to applications so discontent (a
function roughly linear in how much a service misses its targeted response
time) is minimized.

tags: RESOURCE REQUIREMENTS

@inproceedings{chandra2003dra,
  author       = {Chandra, Abhishek and Gong, Weibo and Shenoy, Prashant},
  title        = {Dynamic resource allocation for shared data centers using online measurements},
  crossref     = {iwqos03},
  pages        = {381--400}
}

Talks about Muse, which is a system that can assign and move around jobs. They
use an economic model to minimize the number of active servers while
maintaining SLAs.

tags: ENERGY, SLA, SERVICE HOSTING

@inproceedings{chase2001mes,
  author       = {Chase, Jeffrey S. and Anderson, Darrell C. and Thakar, Prachi N. and Vahdat, Amin M. and Doyle, Ronald P.},
  title        = {Managing Energy and Server Resources in Hosting Centers},
  crossref     = {sosp01},
  pages        = {103--116}
}

A couple of nice algorithms and adversarial arguments, but even the online
algorithms assume knowledge of processing times and ratio of largest to smallest
job sizes.  Should mention.

tags: THEORY, STRETCH

@inproceedings{chekuri2001amw,
  author       = {Chekuri, Chandra and Khanna, Sanjeev and Zhu, An},
  title        = {Algorithms for minimizing weighted flow time},
  crossref     = {stoc01},
  pages        = {84--93}
}

README?

tags: THEORY

@inproceedings{chekuri2004msm,
  author       = {Chekuri, Chandra and Khanna, Sanjeev and Goel, Ashish and Kumar, Amit},
  title        = {Multi-processor Scheduling to Minimize Flow Time with $\epsilon$ Resource Augmentation},
  crossref     = {stoc04},
  pages        = {363--372}
}

This paper provides a poly-time approximation scheme for vector scheduling and
an approximation algorithm for vector packing. This is the Round & Approximate
paper.

tags: THEORY, BIN PACKING, VECTOR PACKING

@article{chekuri2004omp,
  author       = {Chekuri, Chandra and Khanna, Sanjeev},
  title        = {On Multi-dimensional Packing Problems},
  journal      = siam_computing,
  volume       = 33,
  number       = 4,
  year         = 2004,
  pages        = {837--851}
}

@article{chen2004sjs,
  author       = {Chen, Zhi-Long},
  title        = {Simultaneous Job Scheduling and Resource Allocation on Parallel Machines},
  journal      = aor,
  volume       = 129,
  year         = 2004,
  pages        = {135--153}
}

Title says it all.

tags: RESOURCE REQUIREMENTS

@article{chen2008tsl,
  author       = {Chen, Yuan and Iyer, Subu and Liu, Xue and Milojicic, Dejan and Sahai, Akhil},
  title        = {Translating Service Level Objectives to lower level policies for multi-tier services},
  journal      = cluster,
  volume       = 11,
  number       = 3, 
  year         = 2008,
  pages        = {299--311}
}

@inproceedings{chen2013aaa,
  author       = {Chen, Jianhai and Chiew, Kevin and Ye, Deshi and Zhu, Liang Wei and Chen, Wenzhi},
  title        = {{AAGA}: Affinity-Aware Grouping for Allocation of Virtual Machines},
  crossref     = {aina13},
  pages        = {235--242}
}

This algorithm describes a semi-on-line algorithm for the bin stretching
problem with a competitive ratio of 1.6.

tags: THEORY, BIN STRETCHING

@article{cheng2004sms,
  author       = {Cheng, T. C. Edwin and Kellerer, Hans and Kotov, Vladimir},
  title        = {Semi-on-line multiprocessor scheduling with given total processing time},
  journal      = tcs,
  volume       = 337,
  number       = {1--3},
  year         = 2005,
  pages        = {134--146}
}

This paper just shows some statistics for CPU load caused by I/O activity under
Xen, not sure I have much use for it.

REFERENCE ME

tags: VM, I/O

@inproceedings{cherkasova2005mco,
  author       = {Cherkasova, Ludmila and Gardner, Rob},
  title        = {Measuring {CPU} Overhead for {I/O} Processing in the {X}en Virtual Machine Monitor},
  crossref     = {usenix05}
}

So this is a system that uses VMs to create virtual clusters to run grid jobs.
Among other things, they find that the majority of Grid jobs have low CPU
utilization. This paper basically argues that for the given application it
really makes sense to use virtualization for consolidation. They perform
simulation experiments and show that using vm tech they can support 99% of
their target workload with 50% of the nodes used in the real system.

tags: VM CLUSTER, RESOURCE REQUIREMENTS

@inproceedings{cherkasova2006ogs,
  author       = {Cherkasova, Ludmila and Gupta, Diwaker and Ryabinkin, Eygene and Kurakin, Roman and Dobretsov, Vladimir and Vahdat, Amin},
  title        = {Optimizing Grid Site Manager Performance with Virtual Machines},
  crossref     = {worlds06}
}

tags: RESOURCE REQUIREMENTS

@inproceedings{cherkasova2006rcf,
  author       = {Cherkasova, Ludmila and Rolia, Jerome A.},
  title        = {{R-Opus}: A Composite Framework for Application Performability and {QoS} in Shared Resource Pools},
  crossref     = {dsn06},
  pages        = {526--535} 
}

tags: VM, RESOURCE REQUIREMENTS

@techreport{cherkasova2007wvh,
  author       = {Cherkasova, Ludmila and Gupta, Diwaker and Vahdat, Amin},
  title        = {When Virtual is Harder than Real: Resource Allocation Challenges in Virtual Machine Based {IT} Environments},
  institution  = hpl,
  number       = {{HPL}-2007-25},
  year         = 2007
}

tags: PREEMPTION

@inproceedings{chiang1994uac,
  author       = {Chiang, Su-Hui and Mansharamani, Rajesh K. and Vernon, Mary K.},
  title        = {Use of Application Characteristics and Limited Preemption for Run-To-Completion Parallel Processor Scheduling Policies},
  crossref     = {sigmetrics94},
  pages        = {33--44}
}

This paper actually examines the memory usage requirements of a production
workload! Unfortunately, they contradict previous observations:
  -- jobs that request more nodes use less memory per node...
  -- OTOH mem size vs probability looks like the table in LANL CM-5 paper... 

tags: WORKLOAD, MEMORY

@incollection{chiang2001cls,
  author       = {Chiang, Su-Hui and Vernon, Mary K.},
  title        = {Characteristics of a Large Shared-Memory Production Workload},
  crossref     = {jsspp01},
  pages        = {159--187}
}

This paper compares LSF, Priority-backfill, and FCFS-backfill policies using
the logs of production HPCC centers (simulation?). This might server as a good
model.

tags: BATCH

@inproceedings{chiang2001pjs,
  author       = {Chiang, Su-Hui and Vernon, Mary K.},
  title        = {Production Job Scheduling for Parallel Shared Memory Systems},
  crossref     = {ipdps01}
}

The authors basically show that while improving accuracy of runtime estimates doesn't
do a lot for FCFS-backfill policy, when smarter policies are used more accurate estimates
help out a great deal.

tag: RUNTIME ESTIMATES

@incollection{chiang2002ima,
  author       = {Chiang, Su-Hui and Arpaci-Dusseau, Andrea Carol and Vernon, Mary K.},
  title        = {The Impact of More Accurate Requested Runtimes on Production Job Scheduling Performance},
  crossref     = {jsspp02},
  pages        = {103--127}
}

The authors compare spin-blocking vs. immediate blocking co-scheduling schemes
and conclude that the immediate blocking schemes are more robust in the face of
varying workload demands. In particular, the spin blocking schemes are more
robust when the multiprogramming level is high or there is a lot of
communication, as these things make it more likely that nodes will not be
co-scheduled when needed. They use a real cluster and standard benchmarks.

tags: COSCHEDULING

@incollection{choi2003ija,
  author       = {Choi, Gyu Sang and Agarwal, Saurabh and Kim, Jin-Ha and Yoo, Andy B. and Das, Chita R.},
  title        = {Impact of Job Allocation Strategies for Communication Driven Coscheduling in Clusters},
  crossref     = {europar03},
  pages        = {160--168}
}

This compares several forms of implicit co-scheduling and also introduces a new
one, called HYBRID. I'm not quite sure why it's called hybrid, but the basic
idea is to block tasks immediately if they're waiting for communication and
then to swap them in immediately upon receiving communication. They decided
that co-scheduling is probably a good idea and have an interesting analysis of
power consumption that we should probably also consider mentioning.

tags: COSCHEDULING

@inproceedings{choi2004ccv,
  author       = {Choi, Gyu Sang and Kim, Jin-Ha and Ersoz, Deniz and Yoo, Andy B.  and Das, Chita R.},
  title        = {Coscheduling in clusters: Is it a viable alternative?},
  crossref     = {sc04}
}

tags: 

@inproceedings{chun2002upa,
  author       = {Chun, Brent N. and Culler, David E.},
  title        = {User-centric performance analysis of market-based cluster batch schedulers},
  crossref     = {ccgrid02},
  pages        = {30--38}
}

tags:

@inproceedings{chun2004bpg,
  author       = {Chun, G. and Dail, H. and Casanova, Henri and Snavely, Allan E.},
  title        = {Benchmark Probes for Grid Assessment},
  crossref     = {ipdps04}
}

This paper addresses requested job length and the possibility of job
cancellation. This means that their model doesn't so much characterize
supercomputer workloads as supercomputer users.

tags: WORKLOAD

@inproceedings{cirne2001cms,
  author       = {Cirne, Walfredo and Berman, Francine},
  title        = {A Comprehensive Model of the Supercomputer Workload},
  crossref     = {iiwwc01},
  pages        = {140--148}
}

@inproceedings{cirne2012sag,
  author       = {Cirne, Walfredo},
  title        = {Scheduling at Google},
  crossref     = {jsspp12},
  note         = {Keynote address}
}

There's not a lot to this paper; they basically confirm the results claimed in
the original Xen paper, but it's a good reference and we should probably
support this kind of work if we want to call ourselves scientists.

tags: VM

@inproceedings{clark2004xar,
  author       = {Clark, Bryan and Deshane, Todd and Dow, Eli and Evanchik, Stephen and Finlayson, Matthew and Herne, Jason and Matthews, Jeanna Neefe},
  title        = {{X}en and the Art of Repeated Research},
  crossref     = {usenixfr04},
  pages        = {135--144}
}

The authors detail a process migration scheme for Xen. There are three phases:
pre-copy, wherin the process continues execution on the source server, but
pages which are not currently being used are sent to the destination, there is
stop and transmit actively used pages, and there is a pull phase where anything
left over (primarily dirty pages sent during the first phase) still need to be
synced. During the pull phase the process is running on the destination server
and if it tries to access a dirty page it generates a network page fault. The
authors sucessfully migrated a game of quake with less than 60ms downtime,
which is barely noticeable. Web Server under load had 210ms downtime.

tags: VM, MIGRATION

@inproceedings{clark2005lmv,
  author       = {Clark, Christopher and Fraser, Keir and Hand, Steven and Hansen, Jacob Gorm and Jul, Eric and Limpach, Christian and Pratt, Ian and Warfield, Andrew},
  title        = {Live Migration of Virtual Machines},
  crossref     = {nsdi05},
  pages        = {273--286}
}

tags: I/O

@misc{cochrane2009shi,
  author       = {Cochrane, S. and Kutzer, K. and McIntosh, L.},
  title        = {Solving the {HPC} {I/O} Bottleneck: {S}un\textsuperscript{\texttrademark} {L}ustre\textsuperscript{\texttrademark} Storage System},
  month        = apr,
  year         = 2009,
  howpublished = {Sun BluePrints\textsuperscript{\texttrademark} Online, Sun Microsystems}
}

tags: THEORY, BIN PACKING

@article{coffman1978abp,
  author       = {Coffman, Jr., Edward G. and Garey, M. R. and Johnson, D. S.},
  title        = {An application of bin-packing to multiprocessor scheduling},
  journal      = siam_computing,
  volume       = 7,
  pages        = {1--17},
  year         = 1978
}

tags: THEORY, BIN PACKING, BIN STRETCHING

@article{coffman2006aae,
  author       = {Coffman, Jr., Edward G. and Lueker, George S.},
  title        = {Approximation Algorithms for Extensible Bin Packing},
  journal      = j_scheduling,
  volume       = 9,
  number       = 1,
  year         = 2006,
  pages        = {63--69}
}

Having a lot of trouble finding a viable copy of this one.

tags: THEORY, BIN PACKING

@article{coffman2007csb,
  author       = {Coffman, Jr., Edward G. and Csirik, J{\'a}nos},
  title        = {A Classification Scheme for Bin Packing Theory},
  journal      = acta_cybernetica,
  volume       = 18,
  number       = 1,
  year         = 2007,
  pages        = {47--60}
}

@misc{consuegra2012vra,
  author       = {Consuegra, Mario E. and Narasimhan, Giri and Rangaswami, Raju},
  title        = {Vector Repacking Algorithms for Power-Aware Computing},
  year         = 2012,
  url          = {http://users.cis.fiu.edu/~mcons004/templateGraduate_files/vectorRepacking.pdf}
}

This paper addresses using job malleability to minimize the number of time
slots allocated under gang scheduling. The authors also propose making periodic
measurements of runtime performance in order to ensure that processors are
being used at maximum efficiency. The goal is to be able to dynamically expand
and contract the processor allocation, depending on job efficiency and system
load.

tags: GANG, RESOURCE REQUIREMENTS

@inproceedings{corbalan2001igs,
  author       = {Corbal{\'a}n, Julita and Martorell, Xavier and Labarta, Jes{\'u}s},
  title        = {Improving Gang Scheduling through job performance analysis and malleability},
  crossref     = {ics01},
  pages        = {303--311}
}

look at ref [16] hochbaum and schmoys

tags: THEORY, BIN PACKING, BIN STRETCHING

@article{correa2008bpc,
  author       = {Correa, Jos{\'e} R. and Epstein, Leah},
  title        = {Bin Packing with Controllable Item Sizes},
  journal      = infocomp,
  volume       = 206,
  number       = 8,
  year         = 2008,
  pages        = {1003--1016}
}

@techreport{crainic2010ehv,
  author       = {Crainic, Teodor Gabriel and Perboli, Guido and Rei, Walter and Tadei, Roberto},
  title        = {Efficient Heuristics for the Variable Size Bin Packing Problem with Fixed Costs},
  institution  = {Centre interuniversitaire de recherche sur les r{\'e}seaux d'enreprise, la logistique et le transport},
  number       = {{CIRRELT}-2010-18},
  month        = mar,
  year         = 2010
}

This is a slightly earlier journal paper on the same topic as the next one.

tags: ENERGY, RESOURCE REQUIREMENTS, PERFORMANCE MODELING

@article{curtismaury2008pbp,
  author       = {Curtis-Maury, Matthew and Blagojevic, Filip and Antonopoulos, Christos D. and Nikolopoulos, Dimitrios S.},
  title        = {Prediction-Based Power-Performance Adaptation of Multithreaded Scientific Codes},
  journal      = tpds,
  volume       = 19,
  number       = 10,
  month        = oct,
  year         = 2008,
  pages        = {1396--1410}
}

This paper proposes a model for predicting the performance impact of using
various combinations of voltage scaling and concurrency control on various
scientific and parallel workloads. It requires instrumenting the code and doing
some benchmarking; however, they seem to think that once they have a model it
can be applied to a variety of applications.

tags: ENERGY, RESOURCE REQUIREMENTS, PERFORMANCE MODELING

@inproceedings{curtismaury2008pmm,
  author       = {Curtis-Maury, Matthew and Shah, Ankur and Blagojevic, Filip and Nikolopoulos, Dimitrios S. and de Supinski, Bronis R. and Schulz, Martin},
  title        = {Prediction Models for Multi-dimensional Power-Performance Optimization on Many Cores},
  crossref     = {pact08}
}

The authors define the Generalized First-Fit Decreasing Heuristic, show that
it's optimal when the number of dimensions is 2 and every item has at least one
coordinate > 1/2. They also show that for 3 dimensions there's a P-time
reductions from numerical 3-dimensional matching, which is np-hard.

tags: THEORY, BIN PACKING, VECTOR PACKING

@article{csirik1990omv,
  author       = {Csirik, J{\'a}nos and Frenk, J. B. G. and Labbe, M. and Zhang, S.},
  title        = {On multidimensional vector bin packing},
  journal      = acta_cybernetica,
  volume       = 9,
  number       = 4,
  year         = 1990,
  pages        = {361--369}
}

***************
** Letter: D **
***************

maybe use in examples of non-hpc clusters?

tags:

@inproceedings{dean2004msd,
  author       = {Dean, Jeffrey and Ghemawat, Sanjay},
  title        = {{MapReduce}: Simplified data processing on large clusters},
  crossref     = {osdi04},
  pages        = {137--150}
}

@article{dean2008msd,
  author       = {Dean, Jeffrey and Ghemawat, Sanjay},
  title        = {{MapReduce}: Simplified data processing on large clusters},
  journal      = cacm,
  volume       = 51,
  number       = 1,
  month        = jan,
  year         = 2008,
  pages        = {107--113}
}

@inproceedings{deelman2008cds,
  author       = {Deelman, E. and Singh, G. and Livny, M. and Berriman, B. and Good, J.},
  title        = {The cost of doing science on the cloud: the montage example},
  crossref     = {sc08}
}

tags: VIRTUAL MACHINE, VM, HPC
@inproceedings{delgado2011psc,
  author       = {Delgado, Javier and Eddin, Anas Salah and Adjouadi, Malek and Sadjadi, S. Masoud},
  title        = {Paravirtualization for Scientific Computing: Performance Analysis and Prediction},
  crossref     = {ahpcn11}
}

Sorting the items in non-increasing order and then going through the list and
assigning each item to the least-loaded bin has a competitive ratio of 13/12.

tags: THEORY, BIN PACKING, BIN STRETCHING

@article{dellolmo1997aaa,
  author       = {Dell'Olmo, P. and Kellerer, Hans and Speranza, Maria Grazia and Tuza, Zsolt},
  title        = {A 13/12 Approximation Algorithm for Bin Packing with Extendable Bins},
  journal      = ipl,
  volume       = 65,
  number       = 5,
  year         = 1998,
  pages        = {229--233}
}

tags: THEORY, BIN PACKING, BIN STRETCHING

@article{dellolmo1999aap,
  author       = {Dell'Olmo, P. and Speranza, Maria Grazia},
  title        = {Approximation Algorithms for Partitioning small items in unequal bins to minimize total size},
  journal      = dam,
  volume       = 94,
  number       = 1,
  year         = 1999,
  pages        = {181--191}
}

The authors try to find resource allocations that maximize "weighted
energy-delay product gain", where energy-delay product (EDP) is the ratio of
energy to performance (lower is better) and the gain is the improvement they get
with their resource allocation. The focus here is on a single node with multiple
processors, and the allocation is basically how many cores to give to each
running process, so it's not directly applicable, really, but it is relevant.

tags: ENERGY, RESOURCE REQUIREMENTS

@inproceedings{ding2010dcp,
  author       = {Ding, Yang and Kandemir, Mahmut and Irwin, Mary Jane and Raghavan, Padma},
  title        = {Dynamic Core Partitioning for Energy Efficiency},
  crossref     = {ipdps10}
}

tags: RESOURCE ALLOCATION, FAIRNESS

@misc{dolev2011njc,
  author       = {Dolev, Dany and Feitelson, Dror G. and Halpern, Joseph Y. and Kupferman, Raz and Linial, Nati},
  title        = {No Justified Complaints: On Fair Sharing of Multiple Resources},
  month        = jun,
  year         = 2011,
  howpublished = {arXiv:1106.2673v1}
}

This paper basically takes some data and proposes a model for parallel speedup
of programs and proposes ways of computing the location of the ``knee'', i.e.,
the point where improvement starts to level off considerably.

tags: RESOURCE REQUIREMENTS, PERFORMANCE MODELING

@techreport{downey1997msp,
  author       = {Downey, Allen B.},
  title        = {A model for the speedup of parallel programs},
  institution  = ucbeecs,
  number       = {{UCB/CSD}-97-933},
  month        = jan,
  year         = 1997
}

This paper is super relevant in that they use VMs for provisioning
semi-permanent tasks based on models of the workloads.

tags: RESOURCE REQUIREMENTS, VM
@inproceedings{doyle2003mrp,
  author       = {Doyle, Ronald P. and Chase, Jeffrey S. and Asad, Omer M. and Jin, Wei and Vahdat, Amin M.},
  title        = {Model-Based Resource Provisioning in a Web Service Utility},
  crossref     = {usits03},
  pages        = {57--71}
}

Hybrid implementations can perform better, but are not well supported, so you
have to be careful.

tags: MPI, OpenMP

@inproceedings{drosinos2004pcp,
  author       = {Drosinos, Nikolaos and Koziris, Nectarios},
  title        = {Performance Comparison of Pure {MPI} vs. Hybrid {MPI-OpenMP} Parallelization Models on {SMP} Clusters},
  crossref     = {ipdps04}
}

***************
** Letter: E **
***************

README

tags: THEORY

@inproceedings{edmonds1999sid,
  author       = {Edmonds, Jeff},
  title        = {Scheduling in the dark},
  crossref     = {stoc99},
  pages        = {179--188}
}

This is a good reference and should definitely be cited. The focus here is on
allowing for custom OS images for jobs and federating separate clusters into a
larger supercluster.  There is also some discussion of preempting and migrating
jobs, but the focus seems to be on node-level (i.e., not time-shared)
allocations.

tags: VM CLUSTER

@incollection{emeneker2006dvc,
  author       = {Emeneker, Wesley and Jackson, Dave and Butikofer, Joshua and Stanzione, Dan},
  title        = {Dynamic Virtual Clustering with {X}en and {M}oab},
  crossref     = {xhpc06},
  pages        = {440--451}
}

The basic message isn't really that different from everything else:
virtualization can be done with low overhead, there are some obvious benefits,
much work remains.

tags: VM CLUSTER, VIRTUAL MACHINE, HPC

@inproceedings{emeneker2006hcr,
  author       = {Emeneker, Wesley and Stanzione, Dan},
  title        = {{HPC} Cluster Readiness of {X}en and {U}ser {M}ode {L}inux},
  crossref     = {cluster06}
}

Virtual cluster checkpointing

tags: VM CLUSTER, PREEMPTION

@inproceedings{emeneker2006irt,
  author       = {Emeneker, Wesley and Stanzione, Dan},
  title        = {Increasing Reliability through Dynamic Virtual Clustering},
  crossref     = {hapcw06}
}

@inproceedings{engelmann2007cvs,
  author       = {Engelmann, Christian and Scott, Stephen L. and Ong, Hong and Vall{\'e}e, Geoffroy and Naughton, Thomas},
  title        = {Configurable Virtualized System Environments for High Performance Computing},
  crossref     = {hpcvirt07}
}

tags: VECTOR PACKING, VECTOR ASSIGNMENT

@incollection{epstein2002vap,
  author       = {Epstein, Leah and Tassa, Tamir},
  title        = {Vector Assignment Problems: A General Framework},
  crossref     = {esa02},
  pages        = {461--472}
}


tags: THEORY, BIN PACKING, BIN STRETCHING
@article{epstein2003bsr,
  author       = {Epstein, Leah},
  title        = {Bin stretching revisited},
  journal      = acta_informatica,
  volume       = 39,
  number       = 2,
  year         = 2003,
  pages        = {97--117}
}

tags: THEORY, BIN PACKING, BIN STRETCHING

@article{epstein2003omn,
  author       = {Epstein, Leah and Favrholdt, Lene M.},
  title        = {On-Line Maximizing the Number of Items packed in Variable-Sized Bins},
  journal      = acta_cybernetica,
  volume       = 16,
  year         = 2003,
  pages        = {57--66}
}

This article has obvious relevance, and even provides an interpretation of
packing jobs requiring varying amount of resources onto servers. In this
formulation though it is the size of the bins which can vary rather than the
size of the job. In principal that's not really a problem. The formulation is
on-line, so maybe there's some help for what to do between repacking times?

tags: THEORY, BIN PACKING, BIN STRETCHING, VECTOR PACKING

@article{epstein2003ovs,
  author       = {Epstein, Leah},
  title        = {On Variable-Sized Vector Packing},
  journal      = acta_cybernetica,
  volume       = 16,
  year         = 2003,
  pages        = {47--56}
}

tags: VECTOR PACKING, VECTOR ASSIGNMENT

@article{epstein2003vap,
  author       = {Epstein, Leah and Tassa, Tamir},
  title        = {Vector Assignment Problems: A General Framework},
  journal      = j_algorithms,
  volume       = 48,
  year         = 2003,
  pages        = {360--384}
}


tags: THEORY, BIN PACKING, ADAPTATION

@article{epstein2009ras,
  author       = {Epstein, Leah and Kleiman, Elena},
  title        = {Resource augmented semi-online bounded space bin packing},
  journal      = dam,
  volume       = 157,
  number       = 13,
  year         = 2009,
  pages        = {2785--2798}
}

This paper is about a method for scaling the size of jobs in a workload in
terms of node count.

tags: WORKLOAD

@incollection{ernemann2003swt,
  author       = {Ernemann, Carsten and Song, Baiyi and Yahyapour, Ramin},
  title        = {Scaling of Workload Traces},
  crossref     = {jsspp03},
  pages        = {166--182}
}

***************
** Letter: F **
***************

tags: BIN PACKING, GENETIC, HEURISTICS

@article{falkenauer1996hgg,
  author       = {Falkenauer, Emanuel},
  title        = {A hybrid grouping genetic algorithm for bin packing},
  journal      = j_heuristics,
  volume       = 2,
  number       = 1,
  year         = 1996,
  pages        = {5--30}
}

This paper details a scheduling solution based on Xen. It is in many ways
similar to our work, but designed to solve a specific issue: Chemistry
researchers want to be able to run large parallel jobs with quick turnaround
while Physics researchers want to be able to submit lots of serial jobs that
have both short and long runtimes. The proposed solution creates two virtual
clusters with two virtual machines on each node and suspends serial jobs as
needed.  Parallel jobs are not suspended for technical reasons (is this going to
be a problem for us?) to make up for this virtual nodes from the parallel
cluster can be borrowed for serial jobs, but not vice versa.

tags: VM CLUSTER

@inproceedings{fallenbeck2006xac,
  author       = {Fallenbeck, Niels and Picht, Hans-Joachim and Smith, Matthew and Freisleben, Bernd},
  title        = {{X}en and the Art of Cluster Scheduling},
  crossref     = {vtdc07}
}

Parallel Workloads Archive

tags: WORKLOAD

@misc{feitelson-pwa,
  author       = {Feitelson, Dror G.},
  title        = {Parallel Workloads Archive},
  url          = {http://www.cs.huji.ac.il/labs/parallel/workload/}
}

This is basically feitelson's approach, it's a binary tree branching down from
the cluster to the individual nodes, at each stop a job moves down the less
loaded branch until it reaches a node of the appropriate size, it's then gang
scheduled equally with everything below it.

tags: GANG

@article{feitelson1990dhc,
  author       = {Feitelson, Dror G. and Rudolph, Larry},
  title        = {Distributed hierarchical control for parallel processing},
  journal      = computer,
  volume       = 23,
  number       = 5,
  year         = 1990,
  pages        = {65--77}
}

This article is basically about the performance benefits of scheduling gangs of
threads in a coordinated manner and letting the threads busy-wait for
communications rather than having threads block and sleep and allowing the cpu
to be used by another process while waiting. For finely-grained applications
with a lot of communication this latter policy can lead to major slowdowns.
This is more about coordinated scheduling in multiprocessor shared memory
systems, but it can be applied in a more limited fashion to distributed memory
message passing systems.

tags: GANG

@article{feitelson1992gsp,
  author       = {Feitelson, Dror G. and Rudolph, Larry},
  title        = {Gang scheduling performance benefits for fine-grain synchronization},
  journal      = jpdc,
  volume       = 16,
  number       = 4,
  month        = dec,
  year         = 1992,
  pages        = {306--318}
}

This is basically a survey paper comparing gang scheduling with other methods
of dividing up cluster resources. The other methods considered are a managed
global queue, variable partitioning and dynamic partitioning.

tags: GANG

@incollection{feitelson1995pjs,
  author       = {Feitelson, Dror G. and Rudolph, Larry},
  title        = {Parallel Job Scheduling: Issues and Approaches},
  crossref     = {jsspp95},
  pages        = {1--18}
}

This paper is pretty relevant in that the paper refers to coordinated context
switching. The author runs simulated workloads on simulated schedulers and
tries to pack jobs into the minimum number of time slots for coordinated
context switches. Best fit does okay, minimizing maximum load and average load
does poorly because it leads to high fragmentation, approaches allowing for
migration seem to do well. A lesson here?  There's also a "Buddy System"
algorithm where processors are arranged into a sort of tree hierarchy and tasks
must be assigned to groups of processors within the group that also works well.

tags: GANG

@incollection{feitelson1996psg,
  author       = {Feitelson, Dror G.},
  title        = {Packing schemes for gang scheduling},
  crossref     = {jsspp96},
  pages        = {89--110}
}

tags: GANG

@incollection{feitelson1997iur,
  author       = {Feitelson, Dror G. and Jette, Morris A.},
  title        = {Improved Utilization and Responsiveness with Gang Scheduling},
  crossref     = {jsspp97},
  pages        = {238--261}
}

This is one of the few papers that looks at job memory requirements on
large-scale parallel systems. Unformtunately Dr. Casanova thinks the
information may be a little dated and we might need to do our own survey.
Still, there are some interesting observations:
  -- memory usage has strong discrete components
  -- many jobs use a small proportion of the memory available
  -- larger jobs tend to use more memory, but it is difficult to characterize

tags: WORKLOAD, MEMORY

@incollection{feitelson1997mul,
  author       = {Feitelson, Dror G.},
  title        = {Memory usage in the {LANL CM-5} workload},
  crossref     = {jsspp97},
  pages        = {78--94}
}

Scheduler metrics are really just trying to formalize two ideas:
1) satisfy users
2) maximize profit (utility?)

dynamically changing processor allocations in response to job needs and system
loads leads to much better overall performance

really need preemption and time slicing

also describes some ideas for standardizing scheduler interfaces and a
discussion of the interaction between theory and practice

tags: UTILITY, METRICS, GANG

@incollection{feitelson1997tpp,
  author       = {Feitelson, Dror G. and Rudolph, Larry and Schwiegelshohn, Uwe and Sevcik, Kenneth C. and Wong, Parkson},
  title        = {Theory and Practice in Parallel Job Scheduling},
  crossref     = {jsspp97},
  pages        = {1--34}
}

This paper goes on a bit about synthetic workload models, why they should be
used, metrics for comparing schedulers, and their problems. Of particular note
are the claim that we can't increase system load by simply shortening the
arrival intervals and the fact that slowdown is pretty darned erratic as
a metric.

tags: WORKLOAD, METRICS

@incollection{feitelson1998mbp,
  author       = {Feitelson, Dror G. and Rudolph, Larry},
  title        = {Metrics and benchmarking for parallel job scheduling},
  crossref     = {jsspp98},
  pages        = {1--24}
}

This paper basically says that the performance of a scheduler under a given
metric depends heavily on the system load, and no metric converges particularly
better than any other. 

tags: METRICS

@incollection{feitelson2001mpj,
  author       = {Feitelson, Dror G.},
  title        = {Metrics for parallel job scheduling and their convergence},
  crossref     = {jsspp01},
  pages        = {188--205}
} 

Feitelson again looks at Gang vs. Batch scheduling, also Coscheduling. This
paper admits that memory might be a problem with GS, but doesn't look at memory
directly, instead considering "multiprogramming levels", which is basically the
number of available time-slots, 1-6. The design of the experiment is
potentially useful. Tentative conclusions are that GS and Backfilling are
pretty close, and combining them is a bit better but the benefits of GS with
Backfilling are less than the improvement of FCFS->EASY. The coscheduling
approach seems to be best since it's the most flexible (is this good for us?)
and combining it with backfilling is a win.

tags: GANG, BATCH, COSCHEDULING

@incollection{feitelson2004pjs,
  author       = {Feitelson, Dror G. and Rudolph, Larry and Schwiegelshohn, Uwe},
  title        = {Parallel Job Scheduling -- a Status Report},
  crossref     = {jsspp04},
  pages        = {1--16}
}

The basic premise of this paper is that workloads actually consist of several
distinct classes of jobs merged together, and some of these classes should be
removed from models when testing out scheduling algorithms as they represent
anomalous activity which we do not want to optimize to account for. In
particular user flurries, or surges of activity by a single user which consume
a significant percentage of system resources, should be ignored.

tags: WORKLOAD

@inproceedings{feitelson2006wsp,
  author       = {Feitelson, Dror G. and Tsafrir, Dan},
  title        = {Workload sanitation for performance evaluation},
  crossref     = {ispass06},
  pages        = {221--230}
}

This paper may contain the germ of an idea for a more tractable paper for this
fall. This paper is about improving scheduling algorithms using virtualization.
The authors basically state that when there are dependencies and both
preemption and restarts are not allowed, the best strategy is to use
virtualization to run jobs on fewer than the requested number of processors.
The author here makes the same assumptions we do about the affects on runtime.

tags: VM

@techreport{feldmann1992oos,
  author       = {Feldmann, Anja and Kao, Ming-Yang and Sgall, Ji{\v{r}}{\'i} and Teng, Shang-Hua},
  title        = {Optimal Online Scheduling of Parallel Jobs with Dependencies},
  institution  = cmucs,
  number       = {{CMU}-{CS}-92-189},
  month        = sep,
  year         = 1992
}

tags: THEORY, BIN PACKING

@article{fernandez1981bpc,
  author       = {Fernandez de la Vega, W. and Lueker, George S.},
  title        = {Bin packing can be solved within {$1 + \epsilon$} in linear time},
  journal      = combinatorica,
  volume       = 1, 
  number       = 4,
  year         = 1981,
  pages        = {349--355}
}

This paper establishes that queue length is a better metric for load balancing
than resource utilization.

tags: METRICS, RESOURCE REQUIREMENTS

@inproceedings{ferrari1987eil,
  author       = {Ferrari, Domenico and Zhou, Songnian},
  title        = {An Empirical Investigation of Load Indices for Load Balancing Applications},
  crossref     = {performance87},
  pages        = {515--528}
}

The authors of this paper give a MILP for the resource allocation problem
considering 2 resources (CPU and memory) and then use the relaxation of the MILP
to develop a heuristic that is a bit more sophisticated than RRNZ.

tags: EMBEDDED, SCHEDULING, VECTOR PACKING, MILP, HEURISTIC

@inproceedings{fisher2005tpu,
  author       = {Fisher, Nathan and Anderson, James H. and Baruah, Sanjoy},
  title        = {Task partitioning upon memory-constrained multiprocessors},
  crossref     = {rtcsa05}
}

hidden cloud risks

@misc{ford2012ic0,
  title        = {Icebergs in the Clouds: the Other Risks of Cloud Computing},
  author       = {Ford, Bryan},
  year         = {2012},
  howpublished = {arXiv:1203.1979}
}

This paper describes Flexible co-scheduling, wherein processes are divided into
groups: Co-scheduled, Frustrated, and Don't-care, based on their network
activity. Co-scheduled jobs are gang scheduled, as are frustrated jobs, but
frustrated jobs can be preempted if the local scheduler thinks that it's
important. Don't care-jobs are scheduled locally. Tests seem to show that it's
competitive with other scheduling techniques, but it's not a slam-dunk.

tags: COSCHEDULING

@inproceedings{frachtenberg2003fcm,
  author       = {Frachtenberg, Eitan and Feitelson, Dror G. and Petrini, Fabrizio and Fernandez, Juan},
  title        = {Flexible CoScheduling: Mitigating load imbalance and improving utilization of heterogeneous resources},
  crossref     = {ipdps03}
}

Gang scheduling reduces response time and slowdown, while backfilling lets this
be done with a minimum of multiprogramming (time slicing)

tags: GANG

@incollection{frachtenberg2003pjs,
  author       = {Frachtenberg, Eitan and Feitelson, Dror G. and Fernandez, Juan and Petrini, Fabrizio},
  title        = {Parallel Job Scheduling under Dynamic Workloads},
  crossref     = {jsspp03},
  pages        = {208--227}

}

This is an add on to the 2003 paper where they implement their scheme on a 64
node cluster with 4 processers per node. The more interesting experiment used
the lublin model to generate job characteristics, and then a basic BSP model
which used the number of nodes (cpus) and runtime as input and had random
computation and communication characteristics. They did the same thing a second
time using SWEEP3D with random input files, but keeping the same job arrival
times and node count. For the first part they restricted MPL to 6, while for
the second 2 in order to model memory constraints.

tags: COSCHEDULING

@article{frachtenberg2005apj,
  author       = {Frachtenberg, Eitan and Feitelson, Dror G. and Petrini, Fabrizio and Fernandez, Juan},
  title        = {Adaptive parallel job scheduling with flexible coscheduling},
  journal      = tpds,
  volume       = 16,
  number       = 11,
  year         = 2005,
  pages        = {1066--1077}
}

This paper lists common pitfalls in evaluating different job scheduling schemes,
and particularly lists a lot of pitfalls for simulation experiments.

FIXME: list all of the pitfalls and develop a response to them, then find places
to include these responses in the dissertation. Make references to the concerns
listed wherever relevant and note either how we avoid them or why they cannot be
avoided.

CITEME: it specifically says that while changing interarrival times is not
ideal, it may be the least objectionable approach. Mention this.

tags: WORKLOAD

@incollection{frachtenberg2005ppj,
  author       = {Frachtenberg, Eitan and Feitelson, Dror G.},
  title        = {Pitfalls in Parallel Job Scheduling Evaluation},
  crossref     = {jsspp05},
  pages        = {257--282}
}

Nice paper on energy consumption in HPC clusters with good model for simulation. In some circumstances it may
make sense to run more nodes in slower mode.

tags: ENERGY, MPI

@inproceedings{freeh2005eet,
  author       = {Freeh, Vincent W. and Pan, Feng and Kappiah, Nandini and Lowenthal, David K. and Springer, Rob},
  title        = {Exploring the Energy-Time Tradeoff in {MPI} Programs},
  crossref     = {ipdps05}
}

tags:

@inproceedings{fu2003sas,
  author       = {Fu, Yun and Chase, Jeffrey S. and Chun, Brent and Schwab, Stephen and Vahdat, Amin M.},
  title        = {{SHARP}: An Architecture for Secure Resource Peering},
  crossref     = {sosp03},
  pages        = {133--148}
}

***************
** Letter: G **
***************

@inproceedings{gabay2012gam,
  author       = {Gabay, Micha{\"e}l and Zaourar, Sofia},
  title        = {A {GRASP} approach for the machine reassignment problem},
  crossref     = {euro12}
}

This paper basically gives some lower bounds for the competitive ratios of
on-line vector bin packing algorithms using some intelligent adversary type
arguments. These bounds increase dependent on d, but remain pretty low. This
paper also discusses the HARMONIC algorithm proposed by Lee & Lee might be a
good candidate for extension to the vector case.

tags: THEORY, BIN PACKING, VECTOR PACKING

@article{galambos1993lbo,
  author       = {Galambos, G{\'a}bor and Kellerer, Hans and W{\"o}ginger, Gerhard J.},
  title        = {A Lower Bound for On-Line Vector-Packing Algorithms},
  journal      = acta_cybernetica,
  volume       = 11,
  number       = {1--2},
  year         = 1993,
  pages        = {23--34}
} 

tags: THEORY, BIN PACKING, ADAPTATION

@article{gambosi2000aro,
  author       = {Gambosi, Giorgio and Postiglione, Alberto and Talamo, Maurizio},
  title        = {Algorithms for the relaxed on-line bin packing model},
  journal      = siam_computing,
  volume       = 30,
  number       = 5,
  year         = 2000,
  pages        = {1532--1551}
}

tags: THEORY, BIN PACKING

@article{garey1975bms,
  author       = {Garey, M. R. and Graham, R. L.},
  title        = {Bounds for Multiprocessor Scheduling with Resource Constraints},
  journal      = siam_computing,
  volume       = 4,
  number       = 2,
  year         = 1975,
  pages        = {187--200}
}

tags: THEORY

@book{garey1979cig,  
  author       = {Garey, M. R. and Johnson, D. S.},
  title        = {Computers and Intractability, a Guide to the Theory of {NP}-Completeness},
  publisher    = freeman,
  address      = {New York, USA},
  year         = 1979
}

The only part of this paper that's directly relevant is the part where they
talk about their algorithm for load-balancing multi-resource query operations.
Kind of similar to what we're doing.

tags: HEURISTICS

@inproceedings{garofalakis1996mrs,
  author       = {Garofalakis, Minos N. and Ioannidis, Yannis E.},
  title        = {Multi-dimensional Resource Scheduling for Parallel Queries},
  crossref     = {sigmod96},
  pages        = {365--376}
}

Time and space shared resources!

tags:

@inproceedings{garofalakis1997pqs,
  author       = {Garofalakis, Minos N. and Ioannidis, Yannis E.},
  title        = {Parallel Query Scheduling and Optimization with Time- and Space-Shared Resources},
  crossref     = {vldb97},
  pages        = {296--305}
}

This paper has a lot of theorems about specific ways that job run times could
be perturbed and what this would mean for online algorithms in general and
Graham's in particular. The upshot is basically that Graham's is pretty good to
begin with and very resistant to perturbation.

tags: THEORY

@inproceedings{gatto2007org,
  author       = {Gatto, Michael and Widmayer, Peter},
  title        = {On the Robustness of {G}raham's algorithm for online scheduling},
  crossref     = {wads07},
  pages        = {349--361}
}

This is basically a position paper that simply argues that VM tech has many
advantages for HPC, but there are a few things that need to be worked on,
including speeding up i/o and handling multicore more scaleably.

tags: VM CLUSTER

@inproceedings{gavrilovska2007hha,
  author       = {Gavrilovska, Ada and Kumar, Sanjay and Raj, Himanshu and Schwan, Karsten and Gupta, Vishakha and Nathuji, Ripal and Niranjan, Radhika and Ranadive, Adit and Saraiya, Purav},
  title        = {High-Performance Hypervisor Architectures: Virtualization in {HPC} Systems}, 
  crossref     = {hpcvirt07}
}

Probably not a great reference since it is mostly concerned with finding a
robust allocation of jobs on embedded systems, but still, there are some
striking similarities.

tags: MILP

@inproceedings{gertphol2002mmi,
  author       = {Gertphol, Sethavidh and Yu, Yang and Gundala, Shriram B. and Prasanna, Viktor K. and Ali, Shoukat and Kim, Jong-Kook and Maciejewski, Anthony A. and Siegel, Howard Jay},
  title        = {A Metric and Mixed-Integer-Programming-Based Approach for Resource Allocation in Dynamic Real-Time Systems},
  crossref     = {ipdps02}
}

tags: MEMORY, COSCHEDULING, NOW

@incollection{gine2001cmc,
  author       = {Gin{\'e}, Francesc and Solsona, Francesc and Hern{\'a}ndez, Porfidio and Luque, Emilio},
  title        = {Coscheduling under memory constraints in a {NOW} environment},
  crossref     = {jsspp01},
  pages        = {41--65}
}

The focus of this study is a cluster that hosts enterprise applications. The
workload is thus closer in nature to the web service scenario than the hpc
scenario. They monitor resource usage, attempt to build a predictive model, and
make application placement recommendations based on that model. This paper
actually mostly reports on statistics and analyses of workloads.

tags: WORKLOAD, RESOURCE REQUIREMENTS

@inproceedings{gmach2007wad,
  author       = {Gmach, Daniel and Rolia, Jerry and Cherkasova, Ludmila and Kemper, Alfons},
  title        = {Workload Analysis and Demand Prediction of Enterprise Data Center Applications},
  crossref     = {iiswc07},
  pages        = {171--180}
}

So, they use a basic algorithm to generate placements based on predicted
requirements, and then a genetic algorithm combined with a forecasting
algorithm to refine this placement and find initial placements that also
minimize migration. Looks good, technical, not much of a theoretical model.

The related work and bibliography may be useful in the future when we're doing
more to study the non-hpc problems.

tags: RESOURCE REQUIREMENTS, GENETIC

@inproceedings{gmach2008iar,
  author       = {Gmach, Daniel and Rolia, Jerry and Cherkasova, Ludmila and Belrose, Guillaume and Turicchi, Tom and Kemper, Alfons},
  title        = {An integrated approach to resource pool management: Policies, efficiency and quality metrics},
  crossref     = {dsn08},
  pages        = {326--335}
}

README

make sure this isn't too close to what we're doing

tags:

@phdthesis{gmach2009msr,
  author       = {Gmach, Daniel},
  title        = {Managing Shared Resource Pools for Enterprise Applications},
  type         = phd,
  school       = tum,
  year         = 2009
}

README

tags:

@article{gmach2009rpm,
  author       = {Gmach, Daniel and Rolia, Jerry and Cherkasova, Ludmila and Kemper, Alfons},
  title        = {Resource pool management: Reactive versus proactive or let's be friends},
  journal      = computer_networks,
  volume       = 53, 
  number       = 17, 
  year         = 2009,
  pages        = {2905--2922}
}

A very outdated survey of VMs that says they are really slow. Used to compare
old ideas with new ideas about virtualization.

tags: VM

@article{goldberg1974svm,
  author       = {Goldberg, Robert P.},
  title        = {Survey of virtual machine research},
  journal      = computer,
  volume       = 7,
  number       = 6,
  year         = 1974,
  pages        = {34--45}
}

tags: BIN PACKING, HEURISTICS, GENETIC

@techreport{goodman1994gaa,
  author       = {Goodman, Erik D. and Tetelbaum, Alexander Y. and Kureichik, Victor M.},
  title        = {A Genetic Algorithm Approach to Compaction, Bin Packing, and Nesting Problems},
  institution  = msucccaem,
  number       = 940702,
  year         = 1994
}

Okay, this basically implements dynamic virtual clusters on a big
multiprocessor shared memory machine. Kind of interesting in concept, though
going the wrong way...

tags: VIRTUAL CLUSTER

@article{govil2000cdr,
  author       = {Govil, Kinshuk and Teodosiu, Dan and Huang, Yongqiang and Rosenblum, Mendel},
  title        = {Cellular disco: resource management using virtual clusters on shared-memory multiprocessors},
  journal      = tocs,
  volume       = 18,
  number       = 3,
  year         = 2000,
  pages        = {229--262}
}

The authors develop a communication aware co-scheduler that skews priority for
communicating jobs and Domain-0 and perform experiments to show that it improves
the performance of the TPC-W benchmark. Unfortunately, I don't know if there's
any code available.

tags: VM CLUSTER, COSCHEDULING, XEN

@inproceedings{govindan2007xcc,
  author       = {Govindan, Sriram and Nath, Arjun R. and Das, Amitayu and Urgaonkar, Bhuvan and Sivasubramaniam, Anand},
  title        = {{X}en and Co.: Communication-aware {CPU} Scheduling for Consolidated {X}en-based Hosting Platforms},
  crossref     = {vee07}
}

README

tags: VM CLUSTER

@inproceedings{grit2006vmh,
  author       = {Grit, Laura and Irwin, David and Yumerefendi, Aydan and Chase, Jeffrey S.},
  title        = {Virtual machine hosting for networked clusters: Building the foundations for autonomic orchestration},
  crossref     = {vtdc06}
}

So this describes JAWS, which is basically a sophisticated Usher, with some
more support for building models of job performance profiles using NIMO.

tags: VM CLUSTER

@inproceedings{grit2007hvm,
  author       = {Grit, Laura and Irwin, David and Marupadi, Varun and Shivam, Piyush and Yumerefendi, Aydan and Chase, Jeffrey S. and Albrecht, Jeannie},
  title        = {Harnessing Virtual Machine Resource Control for Job Management},
  crossref     = {hpcvirt07}
}

README FIXME

tags:

@inproceedings{gueyoung2009par,
  author       = {Gueyoung, J. and Joshi, K. and Hiltunen, M.},
  title        = {Performance Aware Regeneration in Virtualized Multitier Applications},
  booktitle    = {Proceedings of Proactive Failure Avoidance Recovery and Maintenance (PFARM)},
  month        = jun,
  year         = 2009
}

tags:

@techreport{gupta2005xqm,
  author       = {Gupta, Diwaker and Gardner, Rob and Cherkasova, Ludmila},
  title        = {{XenMon}: {QoS} Monitoring and Performance Profiling Tool},
  institution  = hpl,
  number       = {{HPL}-2005-187},
  year         = 2005
} 

tags: VM

@inproceedings{gupta2006epi,
  author       = {Gupta, Diwaker and Cherkasova, Ludmila and Gardner, Rob and Vahdat, Amin},
  title        = {Enforcing performance isolation across virtual machines in {X}en},
  crossref     = {middleware06},
  pages        = {342--362}
}


Yeah, so we can mess with the clock and make some resources appear to run much
faster/slower relative to the clock than they do in the real world, with some
caveats. Also, we can scale everything so that nothing appears to be slowed, but
it is.

tags: RESOURCE REQUIREMENTS, PERFORMANCE MODELING

@inproceedings{gupta2006tib,
  author       = {Gupta, Diwaker and Yocum, Kenneth and McNett, Marvin and Snoeren, Alex C. and Vahdat, Amin M. and Voelker, Geoffrey M.},
  title        = {To Infinity and Beyond: Time Warped Network Emulation},
  crossref     = {nsdi06},
  pages        = {87--100}
}

This paper compares the Borrowed Virtual Time (BVT), Simple Earliest Deadline
First (SEDF) and Credit CPU schedulers which can be used by Xen. BVT is
preemptive, work conserving, optimally fair, and low overhead. SEDF has non
work conserving mode. Credit is not preemptive, but can transparently schedule
across multiprocessors. 

tags:

@article{gupta2007ctc,
  author       = {Gupta, Diwaker and Cherkasova, Ludmila and Vahdat, Amin M.},
  title        = {Comparison of the Three {CPU} Schedulers in {X}en},
  journal      = acm_sigmetrics_per,
  volume       = 35,
  number       = 2,
  year         = 2007,
  pages        = {42--51}
}

tags: THEORY, BIN PACKING, ADAPTATION

@article{gutin2005bbp,
  author       = {Gutin, Gregory and Jensen, Tommy and Yeo, Anders},
  title        = {Batched bin packing},
  journal      = discopt,
  volume       = 2,
  number       = 1,
  year         = 2005,
  pages        = {71--82}
}

***************
** Letter: H **
***************

In this article the authors explore the multiple-type 2-d bin packing problem.
They focus a lot on whether or not the two columns are really scalable, which
seems like a bit of a red herring to me. Though the relationship between the
scaled values would certainly seem relevant. In the end they do an exact
solution based on a set-covering formulation and column generation, as well as
a greedy algorithm and a simulated annealing algorithm.

tags: THEORY, BIN PACKING

@article{han1994mtt,
  author       = {Han, Bernard T. and Diehr, George and Cook, Jack S.},
  title        = {Multiple-type, two-dimensional bin packing problems: Applications and algorithms},
  journal      = aor,
  volume       = 50,
  number       = 1,
  year         = 1994,
  pages        = {239--261}
}

This is a fairly high level paper that just proposes to have virtual hypervisors
manage local hypervisors.  The propose a basic reallocation algorithm, but it
basically just says that for each vm on an overloaded server, find another
server that can host it--continue until no longer overloaded. If no host can be
found for a given vm then they say to just turn on another server.

tags: VM, CLOUD 

@inproceedings{head2010vhe,
  author       = {Head, Michael R. and Kochut, Andrzej and Schulz, Charles and Shaikh, Hidayatullah},
  title        = {Virtual Hypervisor: Enabling Fair and Economical Resource Partitioning in Cloud Environments},
  crossref     = {noms10},
  pages        = {104--111}
}

tags: ENERGY, VM CLUSTER

@inproceedings{hermenier2006pmg,
  author       = {Hermenier, Fabien and Loriant, Nicolas and Menaud, Jean-Marc},
  title        = {Power Management in Grid Computing with {X}en},
  crossref     = {xhpc06}
}

tags: VM CLUSTER

@techreport{hermenier2008ecm,
  author       = {Hermenier, Fabien and Lorca, Xavier and Menaud, Jean-Marc and Muller, Gilles and Lawall, Julia},
  title        = {{E}ntropy: a Consolidation Manager for Clusters},
  institution  = inria,
  number       = {{RR}-6639},
  year         = 2008
}

tags: VM CLUSTER

@inproceedings{hermenier2008rdp,
  author       = {Hermenier, Fabien and Lorca, Xavier and Cambazard, Hardien and Menaud, Jean-Marc and Jussien, Narendra},
  title        = {Reconfiguration dynamique du placement dans les grilles de calcul dirig{\'e}e par des objectifs},
  crossref     = {cfse08}
}

tags: VM CLUSTER

@inproceedings{hermenier2009cdc,
  author       = {Hermenier, Fabien and L{\`e}bre, Adrien and Menaud, Jean-Marc},
  title        = {Changement de contexte pour t{\^{a}}ches virtualis{\'e}es {\`a} l'{\'e}chelle des grappes},
  crossref     = {cfse09}
}

tags: VM CLUSTER, PREEMPTION

@techreport{hermenier2009cwc,
  author       = {Hermenier, Fabien and L{\`e}bre, Adrien and Menaud, Jean-Marc},
  title        = {Cluster-Wide Context Switch of Virtualized Jobs},
  institution  = inria,
  number       = {{RR}-6929},
  year         = 2009,
  url          = {http://hal.inria.fr/inria-00383325/en/}
}

tags: VM CLUSTER

@inproceedings{hermenier2009ecm,
  author       = {Hermenier, Fabien and Lorca, Xavier and Menaud, Jean-Marc and Muller, Gilles and Lawall, Julia},
  title        = {{E}ntropy: a Consolidation Manager for Clusters},
  crossref     = {vee09}
}

tags: VM CLUSTER, PREEMPTION

@inproceedings{hermenier2010cwc,
  author       = {Hermenier, Fabien and L{\`e}bre, Adrien and Menaud, Jean-Marc},
  title        = {Cluster-Wide Context Switch of Virtualized Jobs},
  crossref     = {vtdc10}
}

tags: VM

@misc{honeycutt2003mvp,
  author       = {Honeycutt, Jerry},
  title        = {{M}icrosoft {V}irtual {PC} 2004 Technical Overview},
  month        = nov,
  year         = 2003,
  url          = {http://download.microsoft.com/download/c/f/b/cfb100a7-463d-4b86-ad62-064397178b4f/Virtual_PC_Technical_Overview.doc}
}

tags: GANG

@inproceedings{hori1998heg,
  author       = {Hori, Atsushi and Tezuka, Hiroshi and Ishikawa, Yutaka},
  title        = {Highly efficient gang scheduling implementation},
  crossref     = {sc98}
}

Basically: there's some overhead, but it is possible to write a halfway decent
implementation.

tags: GANG

@incollection{hori1998oap,
  author       = {Hori, Atsushi and Tezuka, Hiroshi and Ishikawa, Yutaka},
  title        = {Overhead Analysis of Preemptive Gang Scheduling},
  crossref     = {jsspp98},
  pages        = {217--230}
}

The authors consider a data center that services a multi-tier web service
workload. Nodes are homogeneous and perfectly load balanced within a tier, but
may be heterogeneous globally. Nodes are also equipped with DVS and multiple
sleep states, and node state and workload placement can both be coordinated by
a central controller. They attempt to minimize power consumption while
maintaining performance that is within a bound of that without power saving
features, and achieve around 23\% savings on average. They validate their model
in simulation and on a physical cluster using the TPC-W benchmark.

tags: VM CLUSTER, ENERGY

@inproceedings{horvath2008mem,
  author       = {Horvath, Tibor and Skadron, Kevin},
  title        = {Multi-mode Energy Management for Multi-tier Server Clusters},
  crossref     = {pact08},
  pages        = {270--279}
}

tags: WORKLOAD

@incollection{hotovy1996wec,
  author       = {Hotovy, Steven},
  title        = {Workload evolution on the {Cornell Theory Center IBM SP2}},
  crossref     = {jsspp96},
  pages        = {27--40}
}

This one preceeds the ding paper in referencing energy delay product. Seems to
be a different group though. The real difference is that they hand-pick the
regions whereas the later paper tries to derive them automatically.

tags: ENERGY 

@inproceedings{hotta2006pbo,
  author       = {Hotta, Yoshihiko and Sato, Mitsuhisa and Kimura, Hideaki and Matsuoka, Satoshi and Boku, Taisuke and Takahashi, Daisuke},
  title        = {Profile-based Optimization of Power Performance by using Dynamic Voltage Scaling on a {PC} cluster},
  crossref     = {ipdps06}
}

The authors are somewhat inspired by Urgaonkar, but they try to make statistical
guarantees about the likelyhood of a QoS violation.

tags: VM, SERVICE HOSTING, SCHEDULING, RESOURCE REQUIREMENTS

@inproceedings{hoyer2010ssc,
  author       = {Hoyer, Marko and Schr{\"o}der and Nebel, Wolfgang},
  title        = {Statistical static capacity management in virtualized data centers supporting fine grained {QoS} specification},
  crossref     = {eenergy10},
  pages        = {51--60}
}

tags: SCICODE

@misc{hsu2012scc,
  author       = {Hsu, Jeremy},
  title        = {Secret Computer Code Threatens Science},
  journal      = {Scientific American Website},
  url          = {http://www.scientificamerican.com/article.cfm?id=secret-computer-code-threatens-science}
}

This paper basically says that overhead is low and we can reduce network
latencies through intelligent vmm-bypass. Uses MPI tests, HPL, NAS Parallel
benchmarks...

tags: VM CLUSTER

@inproceedings{huang2006chp,
  author       = {Huang, Wei and Liu, Jiuxing and Abali, Bulent and Panda, Dhabaleswar K.},
  title        = {A case for high performance computing with virtual machines},
  crossref     = {ics06},
  pages        = {125--134}
}

This is one of several articles that I've found lately on using a P-time
approach to solving a vector partition problem to optimize a function which is
convex on the vector sum of each partition. Obviously I need to learn a bit
more about that to decide if this is potentially relevant.

tags: THEORY, VECTOR PARTITION

@article{hwang1999pta,
  author       = {Hwang, Frank K. and Onn, Shmuel and Rothblum, Uriel G.},
  title        = {A Polynomial Time Algorithm for Shaped Partition Problems},
  journal      = siam_optimization,
  volume       = 10,
  number       = 1,
  year         = 1999,
  pages        = {70--81}
}

***************
** Letter: I **
***************

tags: SCICODE

@article{ioannidis2011ivp,
  author       = {Ioannidis, John P.A. and Khoury, Muin J.},
  title        = {Improving Validation Practices in ``Omics'' Research},
  journal      = sci,
  volume       = 334,
  number       = 6060,
  month        = dec,
  year         = 2011,
  pages        = {1230--1232}
}

Blah, no good analytic model, in fact they explicitly avoid trying to come up
with one arguing instead that we should just use neural networks.

tags: PERFORMANCE MODELING, NEURAL NETWORKS

@incollection{ipec2005app,
  author       = {Ipek, Engin and de Supinski, Bronis R. and Schulz, Martin and McKee, Sally A.},
  title        = {An Approach to Performance Prediction for Parallel Applications},
  crossref     = {europar05},
  pages        = {196--205}
}

tags:

@inproceedings{irwin2006snr,
  author       = {Irwin, David and Chase, Jeffrey S. and Grit, Laura and Yumerefendi, Aydan and Becker, D. and Yocum, Kenneth},
  title        = {Sharing Networked Resources with Brokered Leases},
  crossref     = {usenix06}
}

tags:

@inproceedings{isard2007ddd,
  author       = {Isard, Michael and Budiu, Mihai and Yu, Yuan and Birrell, Andrew and Fetterly, Dennis},
  title        = {{D}ryad: Distributed Data-Parallel Programs from Sequential Building Blocks},
  crossref     = {eurosys07},
  pages        = {59--72}
} 

tags: THEORY, DYNAMIC BIN PACKING, ADAPTATION

@article{ivkovic1996frf,
  author       = {Ivkovi{\'c}, Zoran and Lloyd, Errol L.},
  title        = {A fundamental restriction on fully dynamic maintenance of bin packing},
  journal      = ipl,
  volume       = 59,
  number       = 4,
  year         = 1996,
  pages        = {229--232},
}

tags: THEORY, DYNAMIC BIN PACKING, ADAPTATION

@article{ivkovic1999fda,
  author       = {Ivkovi{\'c}, Zoran and Lloyd, Errol L.},
  title        = {Fully Dynamic Algorithms for Bin Packing: Being (Mostly) Myopic Helps},
  journal      = siam_computing,
  volume       = 28,
  number       = 2,
  year         = 1999,
  pages        = {574--611}
}

***************
** Letter: J **
***************

Okay, so the thrust of this is using Maui to test out different scheduling
policies (apparently it contains an integrated simulator). Some interesting
points: a call for an open user model back in 2001, an attempt to apply
Leinberger's idea (without significant impact because of the setup), and the
same sort of user-estimate error modeling error that Tsafrir later rails
against. Also, the different backfilling strategies tried all did better than no
backfilling, but none stood out as clearly superior.

tags: BATCH, WORKLOAD, RUNTIME ESTIMATES

@inproceedings{jackson2001sbh,
  author       = {Jackson, David B. and Jackson, Heather L. and Snell, Quinn},
  title        = {Simulation Based {HPC} Workload Analysis},
  crossref     = {ipdps01}
}

@misc{jansen2013rao,
  author       = {Jansen, Klaus and Klein, Kim-Manuel},
  title        = {A Robust {AFPTAS} for Online Bin Packing with Polynomial Migration},
  month        = feb,
  year         = 2013,
  howpublished = {arXiv:1302.4213}
}

GangLL gang scheduler

tags: GANG

@inproceedings{jette1997pcg,
  author       = {Jette, Morris A.},
  title        = {Performance characteristics of gang scheduling in multiprogrammed environments},
  crossref     = {sc97}
}

They found that in general FCFS seems to get 40-60 percent utilization,
backfilling can boost it up to around 70 percent, and these changes were
consistent across changes in hardware. They posit that achieving near 100
percent utilization may be impossible. I should probably read this in greater
detail.

tags: BATCH

@incollection{jones1999sps,
  author       = {Jones, James Patton and Nitzberg, Bill},
  title        = {Scheduling for Parallel Supercomputing: A Historical Perspective on Achievable Utilization},
  crossref     = {jsspp99},
  pages        = {1--16}
}

tags: VM

@inproceedings{jones2006atp,
  author       = {Jones, Stephen T. and Arpaci-Dusseau, Andrea Carol and Arpaci-Dusseau, Remzi H.},
  title        = {{A}ntfarm: Tracking Processes in a Virtual Machine Environment},
  crossref     = {usenix06},
  pages        = {1--14}
}

tags: VM

@inproceedings{jones2006gmb,
  author       = {Jones, Stephen T. and Arpaci-Dusseau, Andrea Carol and Arpaci-Dusseau, Remzi H.},
  title        = {{G}eiger: Monitoring the Buffer Cache in a Virtual Machine Environment},
  crossref     = {asplos06},
  pages        = {14--24}
} 
  
***************
** Letter: K **
***************

This paper is basically about how great malleable jobs are.

tags:

@inproceedings{kale2002mjs,
  author       = {Kal{\'e}, Laxmikant V. and Kumar, Sameer and DeSouza, Jayant},
  title        = {A Malleable-Job System for Time-Shared Parallel Machines},
  crossref     = {ccgrid02},
  pages        = {230--237}
}

This paper proposes using virtual machines to allow more effective multiplexing
of large parallel applications (sound familiar?). The author's approach is a
little vague and they never get around to proposing an objective function like
we do. There is a note that VM overhead is negligible and while it is necessary
to take coordinating communication into account, it isn't a real issue. There
is some discussion of Adaptive MPI, which is basically an MPI system that
allows for user process migration. May be useful.

tags: VM CLUSTER

@inproceedings{kale2004pap,
  author       = {Kal{\'e}, Laxmikant V.},
  title        = {Performance and Productivity in Parallel Programming via Processor Virtualization},
  crossref     = {pphec04},
  pages        = {40--49}
}

This paper describes a fairly clever algorithm that does somewhat better than
greedy in the worst case for a large number of machines. Basically, given a
parameter alpha, the algorithm schedules a incomming task on to the most
heavily loaded machine such that the load of that machine plus the weight of
the task are less than or equal to alpha times the average load of the less
heavily loaded machines. It is shown that this competitive ratio can be kept
below 1.945 for all m, wheras the competitive ratio of list scheduling goes to
2 as m goes to infinity.

tags: THEORY

@inproceedings{karger1994baa,
  author       = {Karger, David R. and Phillips, Steven J. and Torng, Eric},
  title        = {A Better Algorithm for an Ancient Scheduling Problem},
  crossref     = {soda94},
  pages        = {132--140}
}

Okay, so they do a resource constrained multiple knapsack and an iterative
algorithm. It's not directly applicable to the problem as we've formulated it,
but is related. Very similar to kimbrel2005dap.

tags: RESOURCE REQUIREMENTS

@inproceedings{karve2006dpc,
  author       = {Karve, A. and Kimbrel, Tracy and Pacifici, Giovanni and Spreitzer, Michael and Steinder, Malgorzata and Sviridenko, Maxim and Tantawi, Asser N.},
  title        = {Dynamic placement for clustered web applications},
  crossref     = {www06},
  pages        = {595--604}
}

@inproceedings{kash2013nal,
  author       = {Kash, Ian and Procaccia, Ariel D. and Shah, Nisarg},
  title        = {No agent left behind: dynamic fair division of multiple resources},
  crossref     = {aamas13},
  pages        = {351--358}
}

@inproceedings{keahey2008sce,
  author       = {Keahey, K. and Figueiredo, R. and Fortes, J. and Freeman, T. and Tsugawa, M.},
  title        = {Science clouds: Early experiences in cloud computing for scientific applications},
  crossref     = {cca08}
}


README

tags: THEORY, VECTOR PARTITION

@article{kellerer1997soa,
  author       = {Kellerer, Hans and Kotov, Vladimir and Speranza, Maria Grazia and Tuza, Zsolt},
  title        = {Semi on-line algorithms for the partition problem},
  journal      = orl,
  volume       = 21,
  number       = 5,
  year         = 1997,
  pages        = {235--242}
}

The presentation is a little convoluted, but basically take all the items with
at least one dimension bigger than one half. Do what you can go group them into
as few bins as possible, noting that items that large in one capacities are
alone while those with greater than 1/2 in only once capacity can probably be
grouped with a complimenting item. Then try to pack small items on top of them
while keeping in balance.

tags: THEORY, BIN PACKING, VECTOR BIN PACKING

@article{kellerer2003aaa,
  author       = {Kellerer, Hans and Kotov, Vladimir},
  title        = {An approximation algorithm with absolute worst-case performance ratio 2 for two-dimensional vector packing},
  journal      = orl,
  volume       = 31,
  number       = 1,
  year         = 2003,
  pages        = {35--41}
}

tags:

@article{kettimuthu2005sps,
  author       = {Kettimuthu, Rajkumar and Subramani, Vijay and Srinivasan, Srividya and Gopalsamy, Thiagaraja and Panda, Dhabaleswar K. and Sadayappan, P.},
  title        = {Selective preemption strategies for parallel job scheduling},
  journal      = ijhpcn,
  volume       = 3,
  number       = 2,
  year         = 2005,
  pages        = {122--152}
}

tags: APPLICATIONS

@inproceedings{kikuchi2003slc,
  author       = {Kikuchi, Hideaki and Kalia, Rajiv K. and Nakano, Aiichiro and Vashishta, Priya and Shimojo, Fuyuki and Saini, Subhash},
  title        = {Scalability of a Low-Cost Multi-Teraflop {L}inux Cluster for High-End Classical Atomistic and Quantum Mechanical Simulations},
  crossref     = {ipdps03}
}

tags: COSCHEDULING

@inproceedings{kim2004irt,
  author       = {Kim, Jin-Ha and Choi, Gyu Sang and Ersoz, Deniz and Das, Chita R.},
  title        = {Improving response time in cluster-based web servers through coscheduling},
  crossref     = {ipdps04}
}

Very similar to Karve.

tags: RESOURCE REQUIREMENTS

@incollection{kimbrel2005dap,
  author       = {Kimbrel, Tracy and Steinder, Malgorzata and Sviridenko, Maxim and Tantawi, Asser N.},
  title        = {Dynamic Application Placement Under Service and Memory Constraints},
  crossref     = {wea05},
  pages        = {391--402}
}

An okay treatment of the energy-vs-performance tradeoff that looks at scheduling
bag-of-task jobs with deadlines on a homogeneous cluster.

tags: ENERGY, RESOURCE REQUIREMENTS, RAJ

@inproceedings{kim2007pas,
  author       = {Kim, Kyong Hoon and Buyya, Rajkumar and Kim, Jong},
  title        = {Power Aware Scheduling of Bag-of-Tasks Applications with Deadline Constraints on {DVS}-enabled Clusters},
  crossref     = {ccgrid07}
}

Maestro-VC, yet another virtual cluster implementation.

tags: VM CLUSTER

@inproceedings{kiyanclar2006mpe,
  author       = {Kiyanclar, Nadir and Koenig, Gregory A. and Yurcik, William},
  title        = {{M}aestro-{VC}: A Paravirtualized Execution Environment for Secure On-Demand Cluster Computing},
  crossref     = {ccgrid06}
}

This might be a good one for the simulation parameters, particular if we
consider multisite.

tags:

@article{kravtsov2010dfl,
  author       = {Kravtsov, Velentin and Bar, Pavel and Carmeli, David and Schuster, Assaf and Swain, Martin},
  title        = {A scheduling framework for large-scale parallel and topology-aware applications},
  journal      = jpdc,
  volume       = 70,
  number       = 9,
  year         = 2010,
  pages        = {983--992}
}

tags: ENERGY

@misc{koomey2007etp,
  author       = {Koomey, Jonathan G.},
  title        = {Estimating Total Power Consumption by Servers in the {U}.{S}. and the World},
  month        = feb,
  year         = 2007,
}
  url          = {http://enterprise.amd.com/Downloads/svrpwrusecompletefinal.pdf}

Ancient vector packing paper.  

tags: THEORY, BIN PACKING, VECTOR BIN PACKING

@article{kou1977mbp,
  author       = {Kou, L. T. and Markowsky, G.},
  title        = {Multidimensional Bin Packing Algorithms},
  journal      = ibm_jrd,
  volume       = 21,
  number       = 5,
  year         = 1977,
  pages        = {443--448}
}

This paper basically breaks the process of designing a job scheduling system
down into 3 parts: the specification of the scheduling policy, the definition
of an objective function by which schedules may be compared, and the
development of algorithms that achieve good ratings by the scheduling function.
I'm not entirely certain that this is going to add a lot in itself, but it's a
good primer for our area and I should probably work in references to it.

tags: 

@incollection{krallmann1999ode,
  author       = {Krallmann, Jochen and Schwiegelshohn, Uwe and Yahyapour, Ramin},
  title        = {On the Design and Evaluation of Job Scheduling Algorithms},
  crossref     = {jsspp99},
  pages        = {17--42}
}

README

Henri just uses this as a userobjectives -> resource share amount reference.

tags: ENERGY?, RESOURCE REQUIREMENTS

@article{kusic2009ppm,
  author       = {Kusic, Dara and Kephart, Jeffrey O. and Hanson, James E. and Kandasamy, Nagarajan and Jiang, Guofei},
  title        = {Power and Performance Management of Virtualized Computing Environments via Lookahead Control},
  journal      = cluster,
  volume       = 12,
  number       = 1,
  year         = 2009,
  pages        = {1--15}
}

***************
** Letter: L **
***************

Hey, a paper on scheduling VMs with a goal of minimizing energy usage. They
assume performance needs are specified in order to avoid multi-objective
optimization. Otherwise there are some striking similarities to our own
problem...

tag: ENERGY, VIRTUAL MACHINE

@inproceedings{laszewski2009pas,
  author       = {von Laszewski, Gregor and Wang, Lizhe and Younge, Andrew J. and He, Xi},
  title        = {Power-Aware Scheduling of Virtual Machines in {DVFS}-enabled Clusters},
  crossref     = {cluster09}
}

This paper describes the Harmonic bin packing algorithm. It's interesting and
has some properties that make it useful for proving things about asymptotic
lower bounds on the performance ratio, but I'm not quite sure how practical it
is.

tags: THEORY, BIN PACKING

@article{lee1985sob,
  author       = {Lee, C. C. and Lee, D. T.},
  title        = {A simple on-line bin-packing algorithm},
  journal      = jacm,
  volume       = 32,
  number       = 3,
  year         = 1985,
  pages        = {562--572}
}

This paper makes the argument that gang scheduling might not be appropriate for
i/o intensive workloads since it can leave both the disk and cpu-under
utilized. Good for the intro.

tags: GANG, I/O

@incollection{lee1997iig,
  author       = {Lee, Walter and Frank, Matthew and Lee, Victor and Mackenzie, Kenneth and Rudolph, Larry},
  title        = {Implications of {I/O} for gang scheduled workloads},
  crossref     = {jsspp97},
  pages        = {215--237}
}

In a word, yes.

tags: RUNTIME ESTIMATES

@incollection{lee2004aur,
  author       = {Lee, Cynthia Bailey and Schwartzman, Yael and Hardy, Jennifer and Snavely, Allan E.},
  title        = {Are user runtime estimates inherently inaccurate?},
  crossref     = {jsspp04},
  pages        = {253--263}
}

User runtime estimates are wildly inaccurate.

tags: RUNTIME ESTIMATES, UTILITY

@article{lee2006ous,
  author       = {Lee, Cynthia Bailey and Snavely, Allan E.},
  title        = {On the user-scheduler dialogue: Studies of user-provided runtime estimates and utility functions},
  journal      = ijhpca,
  volume       = 20,
  number       = 4,
  year         = 2006,
  pages        = {495--506}
}

This paper is basically about letting users design their own utility functions
and submit them along with jobs. The system then scales them appropriately and
attempts to generate a schedule that maximizes average utility.

tags: UTILITY

@inproceedings{lee2007pru,
  author       = {Lee, Cynthia Bailey and Snavely, Allan E.},
  title        = {Precise and Realistic Utility Functions for User-Centric Performance Analysis of Schedulers},
  crossref     = {hpdc07},
  pages        = {107--116}
}

This paper addresses scheduling a DAG on a cluster of somewhat heterogeneous (2
different types) of DVFS-enabled nodes, each with a set of allowed
performance/voltage settings. The authors basically consider and try to optimize
a linear combination of energy consumption and performance in terms of makespan.

tags: ENERGY, SCHEDULING

@inproceedings{lee2009mec,
  author       = {Lee, Young Choon and Zomaya, Albert Y.},
  title        = {Minimizing Energy Consumption for Precedence-constrained Applications Using Dynamic Voltage Scaling},
  crossref     = {ccgrid09},
  pages        = {92--99}
}

README

tags: THEORY, STRETCH

@article{legrand2008mss,
  author       = {Legrand, Arnaud and Su, A. and Vivien, Fr{\'e}d{\'e}ric},
  title        = {Minimizing the Stretch when Scheduling Flows of Divisible Requests},
  journal      = j_scheduling,
  volume       = 11,
  number       = 5,
  year         = 2008,
  pages        = {381--404}
}

This tech report gives simulation results based on several proposed scheduling
heuristics based on leinberger's mcb algorithms.

tags: HEURISTICS, BIN PACKING, VECTOR PACKING

@techreport{leinberger1999jsp,
  author       = {Leinberger, William John and Kumar, Vipin and Karypis, George},
  title        = {Job Scheduling in the Presence of Multiple Resource Requirements},
  institution  = umcse,
  number       = {99-025},
  month        = may,
  year         = 1999,
}

This is basically about leinberger's MCB algorithm. There is some mention of
applying the results to job scheduling, but there is no direct application in
the paper.

tags: HEURISTICS, BIN PACKING, VECTOR PACKING

@inproceedings{leinberger1999mcb,
  author       = {Leinberger, William John and Karypis, George and Kumar, Vipin},
  title        = {Multi-Capacity Bin Packing Algorithms with Applications to Job Scheduling under Multiple Constraints},
  crossref     = {icpp99},
  pages        = {404--412}
}

tags: GANG

@techreport{leinberger2000gsd,
  author       = {Leinberger, William John and Karypis, George and Kumar, Vipin},
  title        = {Gang Scheduling for Distributed Memory Systems},
  institution  = umcse,
  number       = {00-014},
  month        = feb,
  year         = 2000
}

tags: HEURISTICS, BIN PACKING, VECTOR PACKING

@inproceedings{leinberger2000lba,
  author       = {Leinberger, William John and Karypis, George and Kumar, Vipin and Biswas, Rupak},
  title        = {Load Balancing Across Near-Homogeneous Multi-Resource Servers},
  crossref     = {hcw00},
  pages        = {60--71}
}

tags: HEURISTICS, BIN PACKING, VECTOR PACKING

@phdthesis{leinberger2001hiu,
  author       = {Leinberger, William John},
  title        = {Scheduling Heuristics for Improved Utilization in Multi-Resource Parallel Systems},
  type         = phd,
  school       = um,
  month        = oct,
  year         = 2001
}

This paper characterizes the workload of the DAS-2 Supercomputer. There is some
analysis of memory usage, but it is fairly shallow. 30% 0KB (?), 16% 324KB and
33% 2600-3000KB. Positive correlation between job-size and per-cpu memory
usage.

tags: MEMORY, WORKLOAD

@incollection{li2004wcm,
  author       = {Li, Hui and Groep, David and Wolters, Lex},
  title        = {Workload Characteristics of a Multi-cluster Supercomputer},
  crossref     = {jsspp04},
  pages        = {176--193}
}

README

tags: WORKLOAD

@article{li2009wdc,
  author       = {Li, Hui},
  title        = {Workload Dynamics on Clusters and Grids},
  journal      = j_supercomputing,
  volume       = 47,
  number       = 1,
  year         = 2009,
  pages        = {1--20}
}

@inproceedings{li2010ecm,
  author       = {Li, Jie and Agarwal, Deb and Humphrey, Marty and van Ingen, Catharine and Jackson, Keith and Ryu, Youngryel},
  title        = {{eScience} in the Cloud: A {MODIS} Satellite Data Reprojection and Reduction Pipeline in the {Windows Azure} Platform},
  crossref     = {ipdps10}
}

The basic idea here is to try a couple different algorithms for automatically
breaking a program up into computation and communication regions and then
lowering the power on the CPU during communications.

tags: ENERGY, DYNAMIC VOLTAGE FREQUENCY SCALING, REGION FINDING

@inproceedings{lim2006atf,
  author       = {Lim, Min Yeol and Freeh, Vincent W and Lowenthal, David K.},
  title        = {Adaptive, Transparent Frequency and Voltage Scaling of Communication Phases in {MPI} Programs},
  crossref     = {sc06}
}

tags: VM, I/O

@inproceedings{liu2006hpv,
  author       = {Liu, Jiuxing and Huang, Wei and Abali, Bulent and Panda, Dhabaleswar K.},
  title        = {High Performance {VMM}-bypass {I/O} in Virtual Machines},
  crossref     = {usenix06}
}

tags: VIRTUAL MACHINE, VM, HPC, UTILITY

@inproceedings{liu2011nms,
  author       = {Liu, Yanbin and Bobroff, Norman and Fong, Liana and Seelam, Seetharami and Delgado, Javier},
  title        = {New Metrics for Scheduling Jobs on a Cluster of Virtual Machines},
  crossref     = {ipdps11}
}

@article{liu2013pbc,
  author       = {Liu, Xiaocheng and Wang, Chen and Zhou, Bing Bing and Chen, Junliang and Yang, Ting and Zomaya, Albert Y.},
  title        = {Priority-Based Consolidation of Parallel Workloads in the Cloud}.
  journal      = tpds,
  node         = {preprint},
  url          = {http://www.computer.org/csdl/trans/td/preprint/ttd2012990194-abs.html}
}

This article describes EASY. It's really not that great for anything else.

tags: HEURISTICS

@incollection{lifka1995ais,
  author       = {Lifka, D.},
  title        = {The {ANL}/{IBM} {SP} Scheduling System},
  crossref     = {jsspp95},
  pages        = {295--303}
}

There are a lot of similarities to our work, but a lot of differences too. The
problem is more technical and less formal than ours. They use (period, slice)
values rather than yield and assume much larger time quanta.

tags: VM CLUSTER

@inproceedings{lin2005vmb,
  author       = {Lin, Bin and Dinda, Peter A.},
  title        = {{Vsched}: Mixing batch and interactive virtual machines using periodic real-time scheduling},
  crossref     = {sc05}
}

A pretty involved comparison of various synthetic workload models with real
traces. The biggest conclusions are that the choice of model/trace doesn't seem
to make a huge difference, but variations in ratio of power-of-2 job sizes and
the correlation between job size and runtime do.

Mention if we end up doing simulation experiments based on the synthetic
workload.

tags: WORKLOAD

@incollection{lo1998csr,
  author       = {Lo, Virginia and Mache, Jens and Windisch, Kurt},
  title        = {A Comparative Study of Real Workload Traces and Synthetic Workload Models for Parallel Job Scheduling},
  crossref     = {jsspp98},
  pages        = {25--46}
}

This is well thought out and systematic and probably a model for how to build
these sort of, er, models. That being said, it's based on data from 1995,
including the CM5 data and doesn't have any sort of analysis of how multicore
nodes might affect things.

tags: WORKLOAD

@article{lublin2003wps,
  author       = {Lublin, Uri and Feitelson, Dror G.},
  title        = {The Workload on Parallel Supercomputers: Modeling the Characteristics of Rigid Jobs},
  journal      = jpdc,
  volume       = 63,   
  number       = 11,
  year         = 2003,
  pages        = {1105--1122}
}

***************
** Letter: M **
***************

VMs are useful for a number of reasons. Tests with BLASTS and GROMACS seem to
show a performance overhead of around 6% for normal jobs and 9.7% for I/O
intensive jobs.

tags: VM CLUSTER

@inproceedings{macdonell2007pvm,
  author       = {Macdonell, Cam and Lu, Paul},
  title        = {Pragmatics of Virtual Machines for High-Performance Computing: A Quantitative Study of Basic Overheads},
  crossref     = {hpcs07}
}

Pretty sure this one is poorly written and primarily concerned with minimizing network communication costs.

tags: COSCHEDULING

@article{madheswari2008ecs,
  author       = {Madheswari, A. Neela and Banu, R. S. D. Wahida},
  title        = {Efficient Co-Scheduling of Parallel Jobs in Cluster Computing},
  journal      = ijcsns,
  volume       = 8,
  number       = 11,
  year         = 2008,
  pages        = {96--102}
}

something interesting about bandwidth utilization?

tags: WORKLOAD

@article{madheswari2010mbs,
  author       = {Madheswari, A. Neela and Banu, R. S. D. Wahida},
  title        = {Measuring Bandwidth for Super Computer Workloads},
  journal      = j_computing,
  volume       = 2,
  number       = 3,
  month        = mar,
  year         = 2010,
  pages        = {29--33}
}

tags:

@article{marchal2006sss,
  author       = {Marchal, L. and Yang, Y. and Casanova, Henri and Robert, Yves},
  title        = {Steady-State Scheduling of Multiple Divisible Load Applications on Wide-Area Distributed Computing Platforms},
  journal      = ijhpca,
  volume       = 20,
  number       = 3,
  year         = 2006,
  pages        = {365--381}
}

This paper breaks vector bin packing algorithms down into several parts and
shows how existing algorithms fit into the proposed scheme. They also provide a
novel metric, called utilization, which is basically the maxnorm of the vector
sum of all of the objects divided by the number of bins actually used by a
given algorithm. There are some obvious criticisms of this metric, since
utilization may vary wildly between different instances of essentially the same
problem, but it does have the advantage of not necessarily requiring
theoretical knowledge of the algorithm in question. They also suggest a novel
approach to the semi-online and offline problems, which is to compute a metric
called the degree of dominancy for each dimension and then use these values to
compute a weighted sum for each object and sort them in descending order by
this metric. The degree of dominancy is the percentage of the time a simple
next-fit algorithm has to open a new box because the given dimension's capacity
would be exceeded.

tags: THEORY, BIN PACKING, VECTOR PACKING

@article{maruyama1977gpa,
  author       = {Maruyama, K. and Chang, S. K. and Tang, D. T.},
  title        = {A General Packing Algorithm for Multidimensional Resource Requirements},
  journal      = ijpp,
  volume       = 6,
  number       = 2,
  year         = 1977,
  pages        = {131--149}
}

This paper basically considers the effect of whole-job memory requirements on
parallel job scheduling. The idea is that jobs need to have a certain minimum
amount of memory to run, which restricts their ability to be dynamically
repartitioned. The focus is still on integral processor allocations. The
authors introduce several epoch-based reallocation policies.

tags: MEMORY

@inproceedings{mccann1995smc,
  author       = {Mc{C}ann, Cathy and Zahorjan, John},
  title        = {Scheduling memory constrained jobs on distributed memory parallel computers},
  crossref     = {sigmetrics95},
  pages        = {208--219}
}

Basic description of Usher.

tags: VM CLUSTER

@inproceedings{mcnett2007uef,
  author       = {McNett, Marvin and Gupta, Diwaker and Vahdat, Amin M. and Voelker, Geoffrey M.},
  title        = {{U}sher: An Extensible Framework for Managing Clusters of Virtual Machines},
  crossref     = {lisa07},
  pages        = {167--181}
}

This is yet another sort of generic VMs for service hosting paper. They take
historic data (CPU utilization) and average it to get resource requirements, but
propose in the future to use smarter predictive modeling. They formulate a
generic constraint satisfaction problem that in theory can accomodate any number
of static or dynamic resource constraints and find a solution using a solver.
They mention Leinberger but don't use the heuristics.

tags: VM, RESOURCE ALLOCATION, SERVICE HOSTING, RESOURCE REQUIREMENTS

@inproceedings{mehta2008rtr,
  author       = {Mehta, Sameep and Neogi, Anindya},
  title        = {{ReCon}: A to to {\bf Re}commend Dynamic Server {\bf Con}solidation in Multi-Cluster Data Centers},
  crossref     = {noms08},
  pages        = {363--370}
}

Pretty theoretical; he basically makes the assumption that jobs execute in time
demand/total-load * cpu share, which is kind of obvious. Beam search is used to
search the space of possible cpu allocations, which is interesting, but there
isn't a lot of new information here for us to work with.

tags: VM, RESOURCE ALLOCATION

@inproceedings{menasce2006ave,
  author       = {Menasc{\'e}, Daniel A. and Bennani, Mohamed N.},
  title        = {Autonomic Virtualized Environments},
  crossref     = {icas06},
}

same ol', same ol'

tags: VM CLUSTER

@article{mergen2006vhp,
  author       = {Mergen, Mark F. and Uhlig, Volkmar and Krieger, Orran and Xenidis, Jimi},
  title        = {Virtualization for high-performance computing},
  journal      = acm_sigops_osr,
  volume       = 40,
  number       = 2,
  year         = 2006,
  pages        = {8--11}
}

tags: SCICODE

@article{mesirov2010arr,
  author       = {Mesirov, Jill P.},
  title        = {Accessible Reproducible Research},
  journal      = sci,
  volume       = 327,
  number       = 5964,
  month        = jan,
  year         = 2010,
  pages        = {415--416}
}

tags: BURSTINESS

@inproceedings{mi2008bma,
  author       = {Mi, Ningfang and Casale, Giuliano and Cherkasova, Ludmila and Smirni, Evgenia},
  title        = {Burstiness in Multi-Tier Applications: Symptoms, Causes, and New Models},
  crossref     = {middleware08},
  pages        = {265--286}
}

This paper talks about a model for distributing a workload in a data center
based on the idea of minimizing hotspots in order to reduce cooling costs. It's
certainly something to consider!

tags: ENERGY

@inproceedings{moore2005msc,
  author       = {Moore, Justin and Chase, Jeff and Ranganathan, Parthasarathy and Sharma, Ratnesh},
  title        = {Making Scheduling ``Cool'': Temperature-Aware Workload Placement in Data Centers},
  crossref     = {usenix05},
  pages        = {61--74}
}

GangLL -- 

tags: GANG

@inproceedings{moreira1998iep,
  author       = {Moreira, Jos{\'e} E. and Chan, Waiman and Fong, Liana and Franke, Hubertus and Jette, Morris A.},
  title        = {An Infrastructure for Efficient Parallel Job Execution in Terascale Computing Environments},
  crossref     = {sc98}
}

tags: SCICODE

@article{morin2012slb,
  author       = {Morin, A. and Urban, J. and Adams, P.D. and Foster, I. and Sali, A. and Baker, D. and Sliz, P.},
  title        = {Shining Light into Black Boxes},
  journal      = sci,
  volume       = 336,
  number       = 6078,
  month        = apr,
  year         = 2012,
  pages        = {159--160}
}

This is the paper that says decreasing accuracy in predictions seems to lead to
improved performance.

tags: WORKOAD, RUNTIME ESTIMATES

@article{mualem2001upw,
  author       = {Mu{'}alem, Ahuva W. and Feitelson, Dror G.},
  title        = {Utilization, Predictability, Workloads, and User Runtime Estimates in Scheduling the {IBM SP2} with Backfilling},
  journal      = tpds,
  volume       = 12,
  number       = 6,
  year         = 2001,
  pages        = {529--543}
}

README

tags: STRETCH

@inproceedings{muthukrishnan1999osm,
  author       = {Muthukrishnan, S. and Rajaraman, Rajmohan and Shaheen, A. and Gehrke, J.},
  title        = {Online Scheduling to Minimize Average Stretch},
  crossref     = {focs99},
  pages        = {433--443}
}

***************
** Letter: N **
***************

Yet another survey of co-scheduling techniques, based on dividing them into 3
categories for how they handle sends and 3 for how they handle receives, or a
matrix of 9 schemes. They do another experiment with real machines, 8 nodes,
and 4 sample apps, 3 taken from NAS, all nodes active for every app. They like
periodic boost.

tags: COSCHEDULING

@article{nagar1999acn,
  author       = {Nagar, Shailabh and Banerjee, Ajit and Sivasubramaniam, Anand and Das, Chita R.},
  title        = {Alternatives to Coscheduling a Network of Workstations},
  journal      = jpdc,
  volume       = 59,
  number       = 2,
  year         = 1999,
  pages        = {302--327}
}

VM-based energy budgeting for heterogeneous platforms. Not super exciting. No new models or algorithms really.

tags: VM, ENERGY

@article{nathuji2009vtv,
  author       = {Nathuji, Ripal and Schwan, Karsten and Somani, Ankit and Joshi, Yogendra},
  title        = {{VPM} tokens: virtual machine-aware power budgeting in datacenters},
  journal      = cluster,
  volume       = 12,
  number       = 2,
  year         = 2009,
  pages        = {189--203}
}

virtualization of memory cache hierarchy

tags:

@inproceedings{nesbit2007vpc,
  author       = {Nesbit, Kyle and Laudon, James and Smith, James E.},
  title        = {Virtual Private Caches},
  crossref     = {isca07},
  pages        = {57--68}
}

scaling across multicores.

tags:

@article{nesbit2008mrm,
  author       = {Nesbit, Kyle and Moreto, M. and Cazorla, F. and Ramirez, A. and Valero, M.},
  title        = {Multicore Resource Management},
  journal      = micro,
  month        = may,
  year         = 2008
} 

tags: FAIRNESS

@article{ngubiri2010cfm,
  author       = {Ngubiri, J. and van Vliet, M.},
  title        = {Characteristics of Fairness Metrics and Their Effect on Perceived Scheduler Effectiveness},
  journal      = ijca,
  volume       = 32,
  number       = 2,
  year         = 2010
}

They use an adaptive approach to paging prevention wherein jobs are suspended
if their working set will not fit into memory. There is some trade-off between
this paging prevention and co-scheduling, and tests seem to show that pagining
prevention should get slightly higher priority. For their experiments they used
randomly generated values for memory consumption using a pareto distribution.

tags: COSCHEDULING, MEMORY

@inproceedings{nikolopoulos2002asu,
  author       = {Nikolopoulos, Dimitrios S. and Polychronopoulos, Constantine D.},
  title        = {Adaptive Scheduling under Memory Pressure on Multiprogrammed Clusters},
  crossref     = {ccgrid02},
  pages        = {22--29}
}

This is presumably related to the last paper

tags: COSCHEDULING, MEMORY, NOW

@article{nikolopoulos2003asu,
  author       = {Nikolopoulos, Dimitrios S. and Polychronopoulos, Constantine D.},
  title        = {Adaptive Scheduling under Memory Constraints on Non-dedicated Computational Farms},
  journal      = future,
  volume       = 19,
  number       = 4,
  year         = 2003,
  pages        = {505--519}
}

README

tags: VM CLUSTER

@inproceedings{nishimura2007vco,
  author       = {Nishimura, Hideo and Maruyama, Naoya and Matsuoka, Satoshi},
  title        = {Virtual Clusters on the Fly -- Fast, Scalable, and Flexible Installation},
  crossref     = {ccgrid07},
  pages        = {549--556}
}

README

tags: VM CLUSTER, CLOUD

@inproceedings{nurmi2008eoc,
  author       = {Nurmi, Daniel and Wolski, Rich and Grzegorczyk, Chris and Obertelli, Graziano and Soman, Sunil and Youseff, Lamia and Zagorodnov, Dmitrii},
  title        = {The {E}ucalyptus Open-source Cloud-computing System},
  crossref     = {cca08}
}


***************
** Letter: O **
***************

This one is in here primarily to talk about the need for scalability in internet
services.

tags:

@incollection{odlyzko2003itg,
  author       = {Odlyzko, Andrew M.},
  title        = {Internet Traffic Growth: Sources and Implications},
  booktitle    = {Optical Transmission Systems and Equipment for WDM Networking II},
  editor       = {Dingel, Benjamin B. and Weiershausen, Werner and Dutta, Achyut K. and Sato, Ken-Ichi},
  series       = spie,
  volume       = 5247,
  year         = 2003,
  pages        = {1--15}
}

This paper talks about the issues involved with scheduling i/o in VMs. Not
directly our thing, but a good supporting resource.

tags: VM, I/O

@inproceedings{ongaro2008siv,
  author       = {Ongaro, Diego and Cox, Alan L. and Rixner, Scott},
  title        = {Scheduling {I/O} in Virtual Machine Monitors},
  crossref     = {vee08}
}

This would seem to be another paper on the vector partitioning problem to
optimize convex objective functions. How does this differ from Hwang?

tags: VECTOR PARTITION

@article{onn2001vpp,
  author       = {Onn, Shmuel and Schulman, Leonard J.},
  title        = {The vector partition problem for convex objective functions},
  journal      = mor,
  volume       = 26,
  number       = 3,
  year         = 2001,
  pages        = {583--590}
}

This is the paper that defines the ideas of Coscheduling and Gang Scheduling.
The basic idea is that groups of threads with a significant number of
interactions, that is, back and forth communication events, should be scheduled
across multiple processors in a coordinated manner in order to avoid process
thrashing, which is somewhat akin to page thrashing.

tags: GANG, COSCHEDULING

@inproceedings{ousterhout1982stc,
  author       = {Ousterhout, J. K.},
  title        = {Scheduling Techniques for Concurrent Systems},
  crossref     = {icdcs82},
  pages        = {22--30}
}

***************
** Letter: P **
***************

README 

tags:

@article{pacifici2005pmc, 
  author       = {Pacifici, Giovanni and Spreitzer, Michael and Tantawi, Asser N. and Youssef, Alaa},
  title        = {Performance management for cluster-based web services},
  journal      = ieee_jsac,
  volume       = 23, 
  number       = 12, 
  month        = dec, 
  year         = 2005, 
  pages        = {2333--2343}
}

README

Henri uses as user objectives -> resource shares reference

tags: RESOURCE REQUIREMENTS

@inproceedings{padala2007acv,
  author       = {Padala, Pradeep and Shin, Kang G. and Zhu, Xiaoyun and Uysal, Mustafa and Wang, Zhikui and Singhal, Sharad and Merchant, Arif and Salem, Kenneth},
  title        = {Adaptive control of virtualized resources in utility computing environments},
  crossref     = {eurosys07},
  pages        = {289--302}
}

This is yet another dynamic-resource-allocation-using-vms-for-service-hosting
paper. They propose an "online model estimator", which is really what we want
too. They primarily look at CPU and disk I/O resources. They define a quantity
called the "normalized performance", very similar to the yield, and try to
optimize a utility function based on deviation from optimal performance and
amount of change in resource allocation (to maintain stability). They also
handle complex relationships between related tasks. They perform benchmarks
based on RuBiS, TPC-W and CPU traces from a SAP service. The system discussed
in this paper does not use migration, but they discuss using it on persistently
overloaded systems and note that adding facilities for migration is left to
future work...so maybe we can combine with this?

tags: SERVICE HOSTING, VM, RESOURCE ALLOCATION, PERFORMANCE MODELING, RESOURCE REQUIREMENTS, CPU, I/O

@inproceedings{padala2009acm,
  author       = {Padala, Pradeep and Hou, Kai-Yuan and Shin, Kang G. and Zhu, Xiaoyun and Uysal, Mustafa and Wang, Zhikui and Singhal, Sharad and Merchant, Arif},
  title        = {Automated Control of Multiple Virtualized Resources},
  crossref     = {eurosys09},
  pages        = {13--26}
}

@techreport{panigrahy2011hvb},
  author       = {Panigrahy, Rina and Talwar, Kunal and Uyeda, Lincoln and Wieder, Udi},
  title        = {Heuristics for Vector Bin Packing},
  institution  = {Microsoft Research},
  year         = 2011
}

tags:

@inproceedings{patterson1995ipc,
  author       = {Patterson, R. H. and Gibson, G. A. and Ginting, E. and Stodolsky, D. and Zelenka, J.},
  title        = {Informed Prefetching and Caching},
  crossref     = {sosp95},
  pages        = {79--95}
}

README

tags: THEORY, BIN PACKING, VECTOR PACKING

@misc{pattshamir2009vbp,
  author       = {Patt-Shamir, Boaz and Rawitz, Dror},
  title        = {Vector Bin Packing with Multiple-Choice},
  year         = 2009,
  howpublished = {arXiv:0910.5599}
} 

tags: SCICODE

@article{peng2011rrc,
  author       = {Peng, Roger D.},
  title        = {Reproducible Research in Computational Science},
  journal      = sci,
  volume       = 334,
  number       = 6060,
  month        = dec,
  year         = 2011,
  pages        = {1226--1227}
}

This paper describes buffered co-scheduling; a potential competitor idea, but
without the caps on cpu utilization.

tags: COSCHEDULING

@incollection{petrini2000tpj,
  author       = {Petrini, Fabrizio and Feng, Wu-chun},
  title        = {Time-sharing Parallel Jobs in the Presence of Multiple Resource Requirements},
  crossref     = {jsspp00},
  pages        = {113--136}
}

The authors present the problem of scheduling jobs to meet QoS constraints as a
varible-sized bin packing problem.  They provide a MILP and run some simulation
experiments based on real trace data using CPLEX.

tags: VM CLUSTER, SERVICE HOSTING, BIN PACKING, ENERGY, MILP, OPTIMIZATION, DVFS

@inproceedings{pettruci2010ado,
  author       = {Petrucci, Vinicius and Loques, Orlando and Moss{\'e}, Daniel},
  title        = {A dynamic optimization model for power and performance management of virtualized clusters},
  crossref     = {eenergy10},
  pages        = {225--233}
}

This paper describes Ganeti, google's VM server platform. They just use Haskell
to write the utility that searches for valid resource allocations by their
criteria.

tags: VM CLUSTER

@inproceedings{pop2010erh,
  author      = {Pop, Iustin},
  title       = {Experience Report: {H}askell as a Reagent},
  crossref    = {icfp10},
  pages       = {369--374}
}

Okay, so they just monitor cpu use and try to keep all service delays within
some epsilon of the target.

tags: RESOURCE REQUIREMENTS

@inproceedings{pradhan2002oba,
  author       = {Pradhan, Prashant and Tewari, Renu and Sahu, Sambit and Chandra, Abhishek and Shenoy, Prashant},
  title        = {An observation-based approach towards self-managing web servers},
  crossref     = {iwqos02}
}

***************
** Letter: Q **
***************

***************
** Letter: R **
***************

README 

tags:

@inproceedings{ramakrishnan2006tdc,
  author       = {Ramakrishnan, Lavanya and Irwin, David and Grit, Laura and Yumerefendi, Aydan and Iamnitchi, Adriana and Chase, Jeffrey S.},
  title        = {Toward a Doctrine of Containment: Grid Hosting and Adaptive Resource Control},
  crossref     = {sc06}
}

Benchmarking shows that multiple jobs can share nodes. w00t. definitely reference.

tags: VM CLUSTER

@inproceedings{ranadive2008piv,
  author       = {Ranadive, Adit and Kesavan, Mukil and Gavrilovska, Ada and Schwan, Karsten},
  title        = {Performance implications of virtualizing multicore cluster machines},
  crossref     = {hpcvirt08},
  pages        = {1--8}
}

Just a ref for the cluster users.

tags: 

@article{rhee2003mre,
  author       = {Rhee, Young Min and Pande, Vijay S.},
  title        = {Multiplexed-replica exchange molecular dynamics method for protein folding simulation},
  journal      = biophys,
  volume       = 84,
  number       = 2,
  year         = 2003,
  publisher    = elsevier,
  pages        = {775--786}
}

README

tags:

@article{ribler2001apd,
  author       = {Ribler, Randy L. and Simitci, Huseyin and Reed, Daniel A.},
  title        = {The {A}utopilot Performance-Directed Adaptive Control System},
  journal      = future,
  volume       = 18,
  number       = 1,
  year         = 2001
}

Yet another vc implementation.

tags: VM CLUSTER

@incollection{rodriguez2008dpv,
  author       = {Rodriguez, Manuel and Tapiador, Daniel and Font{\'a}n, Javier and Huedo, Eduardo and Montero, Rub{\'e}n S. and Llorente, Ignacio M.},
  title        = {Dynamic Provisioning of Virtual Clusters for Grid Computing},
  crossref     = {epw08},
  pages        = {23--32}
}

README

tags: 

@incollection{rolia2003aea,
  author       = {Rolia, Jerry and Andrzejak, Artur and Arlitt, Martin},
  title        = {Automating enterprise application placement in resource utilities},
  crossref     = {dsom03},
  pages        = {118--129}
}

README

traced based simulations!!!

tags:

@inproceedings{rolia2005cms,
  author       = {Rolia, Jerry and Cherkasova, Ludmila and Arlitt, Martin and Andrzejak, Artur},
  title        = {A capacity management service for resource pools},
  crossref     = {wosp05},
  pages        = {229--237}
}


This seems particularly relevant considering the recent discussion between
Henri and I.

tags: I/O

@inproceedings{rosti1998iip,
  author       = {Rosti, Emila and Serazzi, Giuseppe and Smirni, Evgenia and Squillante, Mark S.},
  title        = {The impact of {I/O} on program behavior and parallel scheduling},
  crossref     = {sigmetrics98},
  pages        = {56--65}
}

This paper is also about vector packing. They also use the term slack. Despite
the title this is just another simulation of some basic algorithms on random
data.

tags: HEURISTICS, BIN PACKING, VECTOR PACKING

@inproceedings{roy2008tem,
  author       = {Roy, Nilabja and Kinnebrew, John S. and Shankaran, Nishanth and Biswas, Gautam and Schmidt, Douglas C.},
  title        = {Toward Effective Multi-capacity Resource Allocation in Distributed Real-time and Embedded Systems},
  crossref     = {isorc08},
  pages        = {124--128}
}

This system looks a lot like what we want to develop. There's a lot of systems
work, but not much in the way of algorithms though.

tags: VM CLUSTER, ADAPTATION

@inproceedings{ruth2006ala,
  author       = {Ruth, Paul and Rhee, Junghwan and Xu, Dongyan and Kennell, Rick and Goasguen, Sebastien},
  title        = {Autonomic Live Adaptation of Virtual Computational Environments in a Multi-Domain Infrastructure},
  crossref     = {icac06},
  pages        = {5--14}
}

Memory pressure and paging is a problem for gang scheduled apps. They explore
some smarter ways of selecting pages to page in/out based on when jobs are
running, but performance is still not as good as on a dedicated system.

tags: GANG, MEMORY

@inproceedings{ryu2004amp,
  author       = {Ryu, Kyung Dong and Pachapurkar, Nimish and Fong, Liana L.},
  title        = {Adaptive Memory Paging for Efficient Gang Scheduling of Parallel Applications},
  crossref     = {ipdps04}
}

***************
** Letter: S **
***************

The resource-equality fairness metric proposed here is great. I think I can
show that bounding minyield bounds it as well.

tags: FAIRNESS

@incollection{sabin2005ums,
  author       = {Sabin, Gerald and Sadayappan, P.},
  title        = {Unfairness Metrics for Space-Sharing Parallel Job Schedulers},
  crossref     = {jsspp05},
  pages        = {238--256}
}

tags:

@inproceedings{sacerdoti2004gsd,
  author       = {Sacerdoti, F. D. and Chandra, S. and Bhatia, K.},
  title        = {Grid systems deploment {\&} management using {R}ocks},
  crossref     = {cluster04},
  pages        = {337--345}
}

README

also have version from Annals of Operations Research 34(2) 481--498, 2009

tags: SCHEDULING MIGRATION

FIXME: citeme, this is pretty cool, but not relevant to heterogeneous...

@inproceedings{sanders2004osb,
  author       = {Sanders, Peter and Sivadasan, Naveen and Skutella, Martin},
  title        = {Online Scheduling with Bounded Migration},
  crossref     = {icalp04},
  pages        = {1111-1122}
}

tags:

@article{sanders2009osb,
  author       = {Sanders, Peter and Sivadasan, Naveen and Skutella, Martin},
  title        = {Online Scheduling with Bounded Migration},
  journal      = mor,
  volume       = 34,
  number       = 2,
  year         = 2009,
  pages        = {481--498}
}

tags: WORKLOAD

@misc{sandgren2006hpc,
  author       = {Sandgren, Ake and Feitelson, Dror G. and Jack, Michael},
  title        = {The {HPC2N} log},
  year         = 2006,
  url          = {http://www.cs.huji.ac.il/labs/parallel/workload/l_hpc2n/index.html},
}

David's report that, basically, Xen does a good job at performance limiting and
isolation, so long as there isn't too much i/o.

tags: VM

@techreport{schanzenbach2008arc,
  author       = {Schanzenbach, David and Casanova, Henri},
  title        = {Accuracy and Responsiveness of {CPU} Sharing Using {X}en's Cap Values},
  institution  = uhmics,
  number       = {ICS2008-05-01},
  month        = may,
  year         = 2008,
  url          = {http://www.ics.hawaii.edu/research/tech-reports/ICS2008-05-01.pdf}
}

README

tags: FAIRNESS

@article{schwiegelshohn2000fpj,
  author       = {Schwiegelshohn, Uwe and Yahyapour, Ramin},
  title        = {Fairness in parallel job scheduling},
  journal      = j_scheduling,
  volume       = 3,
  number       = 5,
  year         = 2000,
  pages        = {297--320}
}

tags: THEORY, BIN PACKING, BIN STRETCHING

@article{seiden2001ooa,
  author       = {Seiden, Steven S.},
  title        = {An Optimal Online Algorithm for Bounded Space Variable-Sized Bin Packing},
  journal      = siam_discrete_math,
  volume       = 14,
  number       = 4,
  year         = 2001,
  pages        = {458--470}
}

tags: THEORY, BIN PACKING

@article{seiden2002oob,
  author       = {Seiden, Steven S.},
  title        = {On the Online Bin Packing Problem},
  journal      = jacm,
  volume       = 49,
  number       = 5,
  month        = sep,
  year         = 2002,
  pages        = {640--671}
}

tags: THEORY, BIN PACKING, BIN STRETCHING

@article{seiden2003nbv,
  author       = {Seiden, Steven S. and van Stee, Rob and Epstein, Leah},
  title        = {New Bounds for Variable-Sized Online Bin Packing},
  journal      = siam_computing,
  volume       = 32,
  number       = 2,
  year         = 2003,
  pages        = {455--469}
}

README

tags:

@inproceedings{seltzsam2006aaa,
  author       = {Seltzsam, Stefan and Gmach, Daniel and Krompass, Stefan and Kemper, Alfons},
  title        = {{AutoGlobe}: An Automatic Administration Concept for Service-Oriented Database Applications},
  crossref     = {icde06}
}

tags: VM, CLOUD, MIDDLEWARE

@inproceedings{sempolinski2010cce,
  author       = {Sempolinski, Peter and Thain, Douglas},
  title        = {A comparison and critique of {Eucalyptus, OpenNebula and Nimbus}},
  crossref     = {cloudcom10},
  pages        = {417--426}
}

README 

tags: GANG

@inproceedings{setia1997tda,
  author       = {Setia, Sanjeev K.},
  title        = {Trace-driven Analysis of Migration-based Gang Scheduling Policies for Parallel Computers},
  crossref     = {icpp97},
  pages        = {489--492}
}

This paper examines the impact of memory requirements on gang-scheduling
performance (duh). As part of the analysis the authors helpfully examine the
workload on the Cray T3E at the San Diego Supercomputer Center, as well as some
of the jobs run on the LLNL ASCI computer.

For the Cray T3E they report:
  -- the distribution is wide, but more than half the jobs use less than 10MB
     per node (out of 128MB total
  -- longer running jobs tend to use both more processors and more per-processor 
     memory

This might provide a model for our future paper.

tags: GANG, MEMORY

@article{setia1999ijm,
  author       = {Setia, Sanjeev K. and Squillante, Mark S. and Naik, Vijay K.},
  title        = {The impact of job memory requirements on gang-scheduling performance},
  journal      = acm_sigmetrics_per,
  volume       = 26,
  number       = 4,
  year         = 1999,
  pages        = {30--39}
}

README

tags: THEORY, BIN PACKING, VECTOR PACKING, HEURISTICS

@incollection{shachnai2003asg,
  author       = {Shachnai, Hadas and Tamir, Tami},
  title        = {Approximation schemes for generalized 2-dimensional vector packing with application to data placement},
  crossref     = {random03},
  pages        = {165--177}
}

README

tags:

@inproceedings{shen2002irm,
  author       = {Shen, Kai and Tang, Hong and Yang, Tao and Chu, Lingkun},
  title        = {Integrated Resource Management for Cluster-based Internet Services},
  crossref     = {osdi02}
}

Cluster applications.

tags: 

@inproceedings{shingu2002tga,
  author       = {Shingu, Satoru and Takahara, Hiroshi and Fuchigami, Hiromitsu and Yamada, Masayuki and Tsuda, Yoshinori and Ohfuchi, Wataru and Sasaki, Yuji and Kobayashi, Kazuo and Hagiwara, Takashi and Habata, Shin-ichi and Yokokawa, Mitsuo and Itoh, Hiroyuki and Otsuka, Kiyoshi},
  title        = {A 26.58 {T}flops global atmospheric simulation with the spectral transform method on the Earth Simulator},
  crossref     = {sc02},
  pages        = {1--19}
}

README

tags: RESOURCE REQUIREMENTS

@inproceedings{shivam2006aal,
  author       = {Shivam, Piyush and Babu, Shivnath and Chase, Jeffrey S.},
  title        = {Active and accelerated learning of cost models for optimizing scientific applications},
  crossref     = {vldb06},
  pages        = {535--546},
}

README

tags: HEURISTICS

@article{shulz2002sum,
  author       = {Shulz, Andreas S. and Skutella, Martin},
  title        = {Scheduling Unrelated Machines by Randomized Rounding},
  journal      = siam_discrete_math,
  volume       = 15,
  number       = 4,
  year         = 2002,
  pages        = {450--469}
}

tags:

@article{sievert2004smp,
  author       = {Sievert, Otto and Casanova, Henri},
  title        = {A Simple {MPI} Process Swapping Architecture for Iterative Applications}, 
  journal      = ijhpca,
  volume       = 18,
  number       = 3,
  year         = 2004,
  pages        = {341--352}
} 

Basically, we can use preemption to do more aggressive backfilling. This paper
semi-pre-dates VM tech, so there are some assumptions that preempting big
parallel jobs means they'll have to be started over from scratch.

tags: BATCH, PREEMPTION

@incollection{snell2002pbb,
  author       = {Snell, Quinn O. and Clement, Mark J. and Jackson, David B.},
  title        = {Preemption Based Backfill},
  crossref     = {jsspp02},
  pages        = {24--37}
}

yacp, has a fairness metric, empiracle study based on myrinet and fast messages

tags: COSCHEDULING, NOW

@incollection{sobalvarro1998dcw,
  author       = {Sobalvarro, Patrick G. and Pakin, Scott and Weihl, William E. and Chien, Andrew A.},
  title        = {Dynamic coscheduling on workstation clusters},
  crossref     = {jsspp98},
  pages        = {231--256}
}

README

tags: COSCHEDULING

@incollection{sodan2005llm,
  author       = {Sodan, Angela C. and Lau, Lei},
  title        = {{LOMARC} -- Lookahead Matchmaking for Multi-resource Coscheduling},
  crossref     = {jsspp04},
  pages        = {288--315}
}

tags:

@article{sodan2006ats,
  author       = {Sodan, Angela C. and Huang, X.},
  title        = {Adaptive Time/Space Sharing with {SCOJO}},
  journal      = ijhpcn,
  volume       = 4,
  number       = {5/6},
  year         = 2006,
  pages        = {256--269}
}

README

Henri uses for perfromance->resource shares

tags: RESOURCE REQUIREMENTS

@inproceedings{song2009mto,
  author       = {Song, Ying and Wang, Hui and Li, Yaqiong and Feng, Binquan and Sun, Yuzhong},
  title        = {Multi-Tiered On-Demand Resource Scheduling for {VM}-Based Data Center},
  crossref     = {ccgrid09},
  pages        = {148--155}
}

okay: the physical footprint, the actual resources consumed by a virtual machine
may be consistent across a homogeneous cluster, but it can be affected by such
things as its interaction with other vms, the physical characteristics of its
host, distance from data, etc. This is a great "future work" paper, but we're
basically ignorning it for our current research.

tags: RESOURCE REQUIREMENTS?, VM

@inproceedings{sonnek2009vpr,
  author       = {Sonnek, Jason and Chandra, Abhishek},
  title        = {Virtual Putty: Reshaping the Physical Footprint of Virtual Machines},
  crossref     = {hotcloud09}
}

This paper is about using virtualization to set up advance reservations and
parallel job checkpointing in a grid environment. It's not really what we do,
but we should mention it.

tags: VM CLUSTER, BATCH

@inproceedings{sotomayor2008cbe,
  author       = {Sotomayor, Borja and Keahey, Kate and Foster, Ian},
  title        = {Combining Batch Execution and Leasing Using Virtual Machines},
  crossref     = {hpdc08},
  pages        = {87--96}
}

tags: THEORY

@article{speranza1999oaa,
  author       = {Speranza, Maria Grazia and Tuza, Zsolt},
  title        = {On-line approximation algorithms for scheduling tasks on identical machines with extendable working time},
  journal      = aor,
  volume       = 86,
  year         = 1999,
  pages        = {491--506}
}
  
They give some very basic heuristics and ways to compute lower bounds, then
show a branch-and-bound algorithm for computing the exact solution.

tags: HEURISTICS, VECTOR PACKING

@article{spieksma1994bba,
  author       = {Spieksma, Frits C. R.},
  title        = {A branch-and-bound algorithm for the two-dimensional vector packing problem},
  journal      = cor,
  volume       = 21,
  number       = 1,
  year         = 1994,
  pages        = {19--25}
}

This paper basically compares EASY vs Conservative backfilling under several
different queing priorities: FCFS, Shortest-Job-First, and Expansion Factor
(estimated stretch). Not surprisingly, EASY with SF or XF seems to do the
best... Xfactor

tags: BATCH

@inproceedings{srinivasan2002cbs,
  author       = {Srinivasan, Srividya and Kettimuthu, Rajkumar and Subramani, Vijay and Sadayappan, P.},
  title        = {Characterization of backfilling strategies for parallel job scheduling},
  crossref     = {icppw02},
  pages        = {514--522}
}

FIXME: the formulation here should be mentioned in the section on the on-line
algorithm
README

tags: THEORY, BIN PACKING

@article{srivastav1997tar,
  author       = {Srivastav, Anand and Stangier, Peter},
  title        = {Tight Approximations for Resource Constrained Scheduling and Bin Packing},
  journal      = dam,
  volume       = 79,
  number       = 1,
  year         = 1997,
  pages        = {223--245}
}

tags:

@inproceedings{staadt2008spa,
  author       = {Staadt, Oliver G. and Walker, Justin and Nuber, Christof and Hamann, Bernd},
  title        = {A survey and performance analysis of software platforms for interactive cluster-based multi-screen rendering},
  crossref     = {siggraphac08}
}

CITEME!

@phdthesis{stee2002osb,
  author       = {van Stee, Rob},
  title        = {On-line scheduling and bin packing},
  school       = {Leiden University},
  year         = 2002
}  

@phdthesis{stee2006cap,
  author       = {van Stee, Rob},
  title        = {Combinatorial algorithms for packing and scheduling problems},
  school       = {Universit\{"a}t Karlsruhe},
  year         = 2006,
  note         = {Habilitation}
}

README

tags: RESOURCE REQUIREMENTS, PERFORMANCE MODELING

@inproceedings{stewart2005pms,
  author       = {Stewart, C. and Shen, K.},
  title        = {Performance modeling and system management for multi-component online services},
  crossref     = {nsdi05},
  pages        = {71--84}
}

tags: SCICODE

@techreport{stodden2010smp,
  author       = {Stodden, Victoria},
  title        = {The Scientific Method in Practice: Reproducibility in the Computational Sciences},
  institution  = {MIT Sloan Research},
  number       = {4773-10},
  month        = feb,
  year         = 2010,
  url          = {http://dx.doi.org/10.2139/ssrn.1550193}
}

tags: GANG

@inproceedings{strazdins2004clg,
  author       = {Strazdins, Peter and Uhlmann, John},
  title        = {A Comparison of Local and Gang Scheduling on a {B}eowulf Cluster},
  crossref     = {cluster04},
  pages        = {55--62}
}

The focus of this paper is on embedded multiprocessor synthesis, that is,
designing multiprocessors to run known dags with minimal resource requirements.
The authors present a heuristic for packing temporary communicating tasks with
dependencies on nodes with limited cpu and memory capacities. 

tags: EMBEDDED, SCHEDULING, VECTOR PACKING

@inproceedings{szymanek2000tas,
  author       = {Szymanek, Radoslaw and Kuchcinski, Krzysztof},
  title        = {Task Assignment and Scheduling under Memory Constraints},
  crossref     = {euromicro00},
  pages        = {1084--1090}
}
  

***************
** Letter: T **
***************

Try to switch back and forth between two different backfilling policies as appropriate.

tags: BATCH

@inproceedings{talby2005isp,
  author       = {Talby, David and Feitelson, Dror G.},
  title        = {Improving and Stabilizing Parallel Computer Performance Using Adaptive Backfilling},
  crossref     = {ipdps05}
}

tags: ADAPTATION

@article{tan2008osr,
  author       = {Tan, Zhiyi and Yu, Shaohua},
  title        = {Online Scheduling with Reassignment},
  journal      = orl,
  volume       = 36,
  year         = 2008,
  pages        = {250--254}
}

README! 

tags:

@inproceedings{tang2007sap,
  author       = {Tang, Chunqiang and Steinder, Malgorzata and Spreitzer, Michael and Pacifici, Giovanni},
  title        = {A scalable application placement controller for enterprise data centers},
  crossref     = {www07},
  pages        = {331--340}
}

tags: DYNAMIC, OPTIMIZATION, MULTI-OBJECTIVE

@misc{tantar2010dcd,
  author       = {Tandar, Alexandru-Adrian and Tantar, Emilia and Bouvry, Pascal},
  title        = {Design and classification of dynamic multi-objective optimization problems},
  month        = nov,
  year         = 2010,
  howpublished = {arXiv:1103.4820v1}
}

Okay, this paper considers a sort of multi-criteria fuzzy optimization problem
version. Lots of good review and related work. CITEME! Also consider for
ADAPTATION!!!

Also, lots more refs!!!!
runtime inference paper

FIXME: verify

tags: VM CLUSTER

@article{tarighi2010nmv,
  author       = {Tarighi, M. and Motamedi, S.A. and Sharifian, S.},
  title        = {A new model for virtual machine migration in virtualized cluster server based on fuzzy decision making},
  journal      = j_telecomm,
  volume       = 1,
  number       = 1,
  year         = 2010, 
  pages        = {40--51}
}

This paper is primarily about the Vampir/Guide View instrumentation and
profiling suite. So the results show that instrumenting apps doesn't adversely
affect their run-times overmuch (if done properly), but we don't have any nice
models for resource utilization or anything useful like that.

tags:

@inproceedings{thiffault2003dil,
  author       = {Thiffault, Christian and Voss, Michael and Healey, Steven T. and Kim, Seon Wook},
  title        = {Dynamic Instrumentation of Large-Scale {MPI} and {OpenMP} Applications},
  crossref     = {ipdps03}
}

README

tags: VM CLUSTER

@incollection{tikotekar2008ahb,
  author       = {Tikotekar, Anand and Vall{\'e}e, Geoffroy and Naughton, Thomas and Ong, Hong and Engelmann, Christian and Scott, Stephen L.},
  title        = {An Analysis of {HPC} Benchmarks in Virtual Machine Environments},
  crossref     = {epw08},
  pages        = {63--71}
}

tags: WORKLOAD

@inproceedings{tsafrir2006ipj,
  author       = {Tsafrir, Dan and Feitelson, Dror G.},
  title        = {Instability in Parallel Job Scheduling Simulation: The Role of Workload Flurries},
  crossref     = {ipdps06}
}

tags: BATCH, RUNTIME ESTIMATES

@article{tsafrir2007bus,
  author       = {Tsafrir, Dan and Etsion, Yoav and Feitelson, Dror G.},
  title        = {Backfilling Using System-Generated Predictions Rather than User Runtime Estimates},
  journal      = tpds,
  volume       = 18,
  number       = 6,
  year         = 2007,
  pages        = {789--803}
}

Everyone else has been modeling inaccurate user runtime estimates incorrectly.

tags: RUNTIME ESTIMATES

@inproceedings{tsafrir2010uie,
  author       = {Tsafrir, Dan},
  title        = {Using Inaccurate Estimates Accurately},
  crossref     = {jsspp10}
}

GETME
README!

tags: RESOURCE REQUIREMENTS?

@inproceedings{turgeon2000apu,
  author       = {Turgeon, Andr{\'e} and Snell, Quinn and Clement, Mark J.},
  title        = {Application placement using performance surfaces},
  crossref     = {hpdc00},
  pages        = {229--236}
}

***************
** Letter: U **
***************

README

tags: RESOURCE REQUIREMENTS, SERVICE HOSTING

@article{urgaonkar2002roa,
  author       = {Urgaonkar, Bhuvan and Shenoy, Prashant and Roscoe, Timothy},
  title        = {Resource Overbooking and Application Profiling in Shared Hosting Platforms},
  journal      = acm_sigops_osr,
  volume       = 36,
  year         = 2002,
  pages        = {239--254}
}

README

tags:  RESOURCE REQUIREMENTS

@inproceedings{urgaonkar2005dpm,
  author       = {Urgaonkar, Bhuvan and Shenoy, Prashant and Chandra, Abhishek and Goyal, Pawan and Wood, Timothy},
  title        = {Dynamic provisioning of multi-tier internet applications},
  crossref     = {icac05},
  pages        = {217--228}
}

README!

tags: RESOURCE REQUIREMENTS

@article{urgaonkar2007apc,
  author       = {Urgaonkar, Bhuvan and Rosenberg, Arnold L. and Shenoy, Prashant},
  title        = {Application Placement on a Cluster of Servers},
  journal      = ijfcs,
  volume       = 18,
  number       = 5,
  year         = 2007,
  pages        = {1023--1041}
}

README

tags: RESOURCE REQUIREMENTS

@article{urgaonkar2008adp,
  author       = {Urgaonkar, Bhuvan and Shenoy, Prashant and Chandra, Abhishek and Goyal, Pawan and Wood, Timothy},
  title        = {Agile dynamic provisioning of multi-tier Internet applications},
  journal      = acm_taas,
  volume       = 3,
  number       = 1,
  year         = 2008,
  pages        = {1--39}
}

tags: ENERGY

@misc{usepa2007rcs,
  title        = {Report to Congress on Server and Data Center Energy Efficiency},
  organization = {{U}.{S}. Environmental Protection Agency},
  month        = aug,
  year         = 2007,
  url          = {http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf}
}

tags:

@inproceedings{utrera2004imm,
  author       = {Utrera, G. and Corbal{\'a}n, Julita and Labarta, Jes{\'u}s},
  title        = {Implementing Malleability on {MPI} Jobs},
  crossref     = {pact04},
  pages        = {215--224}
}

***************
** Letter: V **
***************

tags:

@article{vadhiyar2003sfd,
  author       = {Vadhiyar, S. and Dongarra, Jack},
  title        = {{SRS}: A Framework for Developing Malleable and Migratable Parallel Applications for Distributed Systems},
  journal      = ppl,
  volume       = 13,
  number       = 2,
  year         = 2003,
  pages        = {291--312}
}

README

tags:

@inproceedings{van2009avr,
  author       = {Nguyen Van, Hien and Dang Tran, Fr{\'e}d{\'e}ric and Menaud, Jean-Marc},
  title        = {Autonomic virtual resource management for service hosting platforms},
  crossref     = {icse-cloud09}
}

README

tags: SLA, VM CLUSTER, CLOUD, SERVICE HOSTING

@inproceedings{van2009sav,
  author       = {Nguyen Van, Hien and Dang Tran, Fr{\'e}d{\'e}ric and Menaud, Jean-Marc},
  title        = {{SLA}-aware Virtual Resource Management for Cloud Infrastructures},
  crossref     = {cit09},
  pages        = {357--362}
}

***************
** Letter: W **
***************

README

tags: UTILITY

@inproceedings{walsh2004ufa,
  author       = {Walsh, William E. and Tesauro, Gerald and Kephart, Jeffrey O. and Das, Rajarshi},
  title        = {Utility Functions in Autonomic Systems},
  crossref     = {icac04},
  pages        = {70--77}
}

README 

tags: VM CLUSTER

@inproceedings{walters2008cvt,
  author       = {Walters, J. P. and Chaudhary, Vipin and Cha, Minsuk and Guercio, Jr., Salvatore and Gallo, Steve},
  title        = {A Comparison of Virtualization Technologies for {HPC}},
  crossref     = {aina08},
  pages        = {861--868}
}

README

tags: GANG

@incollection{wang1997peg,
  author       = {Wang, Fang and Papaefthymiou, Marios and Squillante, Mark S.},
  title        = {Performance evaluation of gang scheduling for parallel and distributed multiprogramming},
  crossref     = {jsspp97},
  pages        = {277--298}
}

README

Henri uses for performance -> resources

tags: RESOURCE REQUIREMENTS?

@inproceedings{wang2009dcf,
  author       = {Wang, R. and Kandasamy, N.},
  title        = {A distributed control framework for performance management of virtualized computing environments: some preliminary results},
  crossref     = {acdc09},
  pages        = {7--12}
}

Cluster applications.

tags:

@article{warfield1998hpc,
  author       = {Warfield, Simon and Jolesz, Ferenc and Kikinis, Ron},
  title        = {A high performance computing approach to the registration of medical imaging data},
  journal      = pc,
  volume       = 24,
  number       = {9-10},
  year         = 1998,
  pages        = {1345--1368}
}

README

tags:

@techreport{warfield2002isn,
  author       = {Warfield, Andrew and Hand, Steven and Harris, Tim and Pratt, Ian},
  title        = {Isolation of Shared Network Resources in {XenoServers}},
  institution  = {PlanetLab Project},
  number       = {PDN-02-2006},
  month        = nov,
  year         = 2002,
  url          = {http://www.planet-lab.org/files/pdn/PDN-02-006/pdn-02-006.pdf}
}

tags:

@inproceedings{watts1997lbt,
  author       = {Watts, Jerrell and Rieffel, Marc and Taylor, Stephen},
  title        = {A Load Balancing Technique for Multiphase Computations},
  crossref     = {hpc97}
}

@inproceedings{wells2006hss,
  author       = {Wells, Philip M. and Chakraborty, Koushik and Sohi, Gurindar S.},
  title        = {Hardware Support for Spin Management in Overcommitted Virtual Machines},
  crossref     = {pact06},
  pages        = {124--133}
}

@inproceedings{wilcox2010pvm,
  author       = {Wilcox, David and McNabb, Andrew and Seppi, Kevin and Flanagan, Kelly},
  title        = {Probabilistic Virtual Machine Assignment},
  crossref     = {cloudcomp10}
}

README!

tags: VM

@inproceedings{willmann2007cdn,
  author       = {Willmann, Paul and Shafer, Jeffrey and Carr, David and Menon, Aravind and Rixner, Scott and Cox, Alan L. and Zwaenepoel, Willy},
  title        = {Concurrent Direct Network Access for Virtual Machine Monitors},
  crossref     = {hpca07},
  pages        = {306--317}
}

README 

tags: WORKLOAD

@inproceedings{windisch1996cwt,
  author       = {Windisch, Kurt and Lo, Virginia and Feitelson, Dror G. and Nitzberg, Bill and Moore, Reagan },
  title        = {A Comparison of Workload Traces from Two Production Parallel Machines},
  crossref     = {frontiers96},
  pages        = {319--326}
}

This paper is actually pretty useful and provides a good model of the proposed
simulation...

tags: GANG, I/O

@article{wiseman2003pgs,
  author       = {Wiseman, Yair and Feitelson, Dror G.},
  title        = {Paired Gang Scheduling},
  journal      = tpds,
  volume       = 14,
  number       = 6,
  year         = 2003,
  pages        = {581--592}
}

tags: THEORY, BIN PACKING, VECTOR PACKING

@article{woginger1997tna,
  author       = {W{\"o}ginger, Gerhard J.},
  title        = {There is no asymptotic {PTAS} for two-dimensional vector packing},
  journal      = ipl,
  volume       = 64,
  number       = 6,
  year         = 1997,
  pages        = {293--297}
}

README

tags: VM, MIGRATION

@inproceedings{wood2007bgs,
  author       = {Wood, Timothy and Shenoy, Prashant and Venkataramani, Arun and Yousif, Mazin},
  title        = {Black-box and Gray-box Strategies for Virtual Machine Migration},
  crossref     = {nsdi07},
  pages        = {229--242}
}

So, this paper looks like it should be relevant, but isn't really. It's about benchmarking native
applications and then predicting their resource requirements when run under virtualization.

tags: RESOURCE REQUIREMENTS

@inproceedings{wood2008pmr,
  author       = {Wood, Timothy and Cherkasova, Ludmila and Ozonat, Kivanc and Shenoy, Prashant},
  title        = {Profiling and Modeling Resource Usage of Virtualized Applications},
  crossref     = {middleware08},
  pages        = {366--387}
}

***************
** Letter: X **
***************

README

tags: MEMORY

@article{xiao2002dcr,
  author       = {Xiao, Li and Chen, Songqing and Zhang, Xiaodong},
  title        = {Dynamic cluster resource allocations for jobs with known and unknown memory demands},
  journal      = tpds,
  volume       = 13,
  number       = 3,
  year         = 2002,
  pages        = {223--240}
}

@inproceedings{xu2012vla,
  author       = {Xu, Cong and Gamage, Sahan and Rao, Pawan N. and Kangarlou, Ardalan and Kompella, Ramana Rao and Xu, Dongyan},
  title        = {{vSlicer}: latency-aware virtual machine scheduling via differentiated-frequency {CPU} slicing},
  crossref     = {hpdc12}
}

***************
** Letter: Y **
***************

README

tags: VM CLUSTER

@inproceedings{yamasaki2007mrs,
  author       = {Yamasaki, Shohei and Maruyama, Naoya and Matsuoka, Satoshi},
  title        = {Model-based Resource Selection for Efficient Virtual Cluster Deployment},
  crossref     = {vtdc07}
}

@inproceedings{yaoyuenyong2009sba,
  author       = {Yaoyuenyong, Sorawit},
  title        = {A Search-Based Algorithm for the Multi-Dimensional Vector Packing Problem},
  crossref     = {apiems09},
  pages        = {81--85}
}

tags: THEORY, BIN PACKING, BIN STRETCHING

@inproceedings{ye2003oeb,
  author       = {Ye, Deshi and Zhang, Guochuan},
  title        = {On-line extensible bin packing with unequal bin sizes},
  crossref     = {waoa03},
  pages        = {235--247}
}

tags:

@article{ye2003ose,
  author       = {Ye, Deshi and Zhang, Guochuan},
  title        = {On-line scheduling with extendable working time on a small number of machines},
  journal      = ipl,
  volume       = 85,
  number       = 4,
  year         = 2003,
  pages        = {171--177}
}

README

tags: COSCHEDULING

@incollection{yoo2001esc,
  author       = {Yoo, A. B. and Jette, M. A.},
  title        = {An efficient and scalable coscheduling technique for large symmetric multiprocessor clusters},
  crossref     = {jsspp01},
  pages        = {21--40}
}

tags: VIRTUAL MACHINE, HPC

@inproceedings{younge2011avt,
  author       = {Younge, Andrew J. and Henschel, Robert and Brown, James T. and von Laszewski, Gregor and Qiu, Judy and Fox, Geoffrey C.},
  title        = {Analysis of Virtualization Technologies for High Performance Computing Environments},
  crossref     = {cloud11}
}

Virtualization is good, but not often employed for HPC because of perceived
overheads. They do benchmarks to show that Xen/Linux performs nearly as well as
just Linux. They use the t-test, which is something we might want to consider.

benchmarks: HPC Challenge, LLNL ASC Purple, Linpack

tags: VM CLUSTER

@inproceedings{youseff2006epi,
  author       = {Youseff, Lamia and Wolski, Rich and Gorda, Brent and Krintz, Chandra},
  title        = {Evaluating the Performance Impact of {X}en on {MPI} and Process Execution For {HPC} Systems},
  crossref     = {vtdc06}
}

This is mostly the same stuff as the other paper. 

tags: VM CLUSTER

@incollection{youseff2006phs,
  author       = {Youseff, Lamia and Wolski, Rich and Gorda, Brent and Krintz, Chandra},
  title        = {Paravirtualization for {HPC} Systems},
  crossref     = {xhpc06},
  pages        = {474--486}
}

ATLAS and BLAS run about as well on paravirtualized systems as native.

tags: VM HPC

@inproceedings{youseff2008ipm,
  author       = {Youseff, Lamia and Seymour, Keith and You, Haihang and Dongarra, Jack and Wolski, Rich},
  title        = {The Impact of Paravirtualized Memory Heirarchy on Linear Algebra Computational Kernels and Software},
  crossref     = {hpdc08},
  pages        = {141--152}
}

***************
** Letter: Z **
***************

This paper explores the idea of combining gang scheduling with a queue and
backfilling in order to keep the number of time quanta from becoming too large.
We could do the same thing with Flexible Scheduling.

tags: GANG

@inproceedings{zhang2000ipj,
  author       = {Zhang, Yanyong and Franke, Hubertus and Moreira, Jos{\'e} E. and Sivasubramaniam, Anand},
  title        = {Improving Parallel Job Scheduling by Combining Gang Scheduling and Backfilling Techniques},
  crossref     = {ipdps00},
  pages        = {133--142}
}

So, basically they simulate on traces and show that using all three of the
techniques below is better than any subset. Using the three techniques can
achieve 98% utilization, or 94% if they limit the number of time slots for gang
scheculing to 20. No examination of mean cpu load or memory pressure.

tags: GANG

@article{zhang2001iap,
  author       = {Zhang, Yanyong and Franke, Hubertus and Moreira, Jos{\'e} E. and Sivasubramaniam, Anand},
  title        = {An Integrated Approach to Parallel Scheduling Using Gang-Scheduling, Backfilling and Migration},
  journal      = tpds,
  volume       = 14,
  number       = 3,
  year         = 2003,
  pages        = {236--247}
}

README

tags: RESOURCE REQUIREMENTS

@techreport{zhang2007rba,
  author       = {Zhang, Qi and Cherkasova, Ludmila and Smirni, Evgenia},
  title        = {A Regression-based Analytic Model for Dynamic Resource Provisioning of Multi-Tier Applications},
  institution  = hpl,
  number       = {{HPL}-2007-85},
  year         = 2007
}

README

tags: VM CLUSTER, VM MIGRATION

@inproceedings{zhao2007esv,
  author       = {Zhao, Ming and Figueiredo, Renato J.},
  title        = {Experimental Study of Virtual Machine Migration in Support of Reservation of Cluster Resources},
  crossref     = {vtdc07}
}

README

tags: GANG, ADAPTATION

@incollection{zhou1999jre,
  author       = {Zhou, Bing Bing and Brent, Richard P. and Johnson, C. W. and Walsh, David},
  title        = {Job re-packing for enhancing the performance of gang scheduling},
  crossref     = {jsspp99},
  pages        = {129--143}
}

tags: GANG

@incollection{zhou2000ras,
  author       = {Zhou, Bing Bing and Walsh, David and Brent, Richard P.},
  title        = {Resource Allocation Schemes for Gang Scheduling},
  crossref     = {jsspp00},
  pages        = {74--86}
}

README

tags:

@inproceedings{zhu2008iic,
  author       = {Zhu, Xiaoyun and Young, Don and Watson, Brian J. and Wang, Zhikui and Rolia, Jerry and Singhal, Sharad and McKee, Bret and Hyser, Chris and Gmach, Daniel and Gardner, Rob and Christian, Tom and Cherkasova, Lucy},
  title        = {1000 Islands: Integrated Capacity and Workload Management for the Next Generation Data Center},
  crossref     = {icac08},
  pages        = {172--181}
}

README

tags: RUNTIME ESTIMATES, BATCH

@inproceedings{zotkin1999jle,
  author       = {Zotkin, Dmitry and Keleher, Peter J.},
  title        = {Job-length estimation and performance in backfilling schedulers},
  crossref     = {hpdc99},
  pages        = {236--243}
}

***************
** Misc.     **
***************

@misc{cplex,
  title        = {{CPLEX}},
  url          = {http://www.ilog.com/products/cplex/}
}

@misc{cpulimit,
  title        = {cpulimit},
  url          = {http://cpulimit.sourceforge.net/}
}

@misc{galib,
  title        = {{GAlib}: A {C}++ Library of Genetic Algorithm Components},
  url          = {http://lancet.mit.edu/ga/}
}

@misc{googleclusterdata,
  title        = {Google Cluster Data Archive},
  url          = {http://code.google.com/p/googleclusterdata/}
}

@misc{hadoop,
  title        = {Apache Hadoop Project},
  url          = {http://hadoop.apache.org/}
}

@misc{intel_vtech,
  title        = {Intel Virtualization Technology},
  url          = {http://www.intel.com/technology/virtualization/index.htm}
}

@misc{linode,
  title        = {Linode Virtual Hosting Service},
  url          = {http://linode.com/}
}

@misc{usher_clients,
  title        = {Usher Clients},
  url          = {http://usher.ucsd.edu/trac/wiki/UsherClients}
}

@misc{usher_events,
  title        = {Usher Events},
  url          = {http://usher.ucsd.edu/trac/wiki/UsherDevelopment#UsherEvents}
}


@misc{usher_plugins,
  title        = {Usher Plugins},
  url          = {http://usher.ucsd.edu/trac/wiki/UsherPlugins}
}

@misc{virtual_center,
  title        = {{VirtualCenter}},
  url          = {http://www.vmware.com/products/vi/vc}
}

@misc{vmware, 
  title        = {{VMware}},
  url          = {http://www.vmware.com/}
}

@misc{xen_enterprise,
  title        = {Citric {XenServer} Enterprise Edition},
  url          = {http://www.xensource.com/products/Pages/XenEnterprise.aspx}
}

@misc{top500,
  title        = {Top 500 Supercomputer Sites},
  url          = {http://www.top500.org/}
}


@misc{ibm-blue-cloud,
  title        = {{IBM} Introduces Ready-to-Use Cloud Computing},
  year         = 2007,
  url          = {http://www-03.ibm.com/press/us/en/pressrelease/22613.wss}
}

@misc{ibm-smart-cloud,
  title        = {{IBM} SmartCloud},
  url          = {http://www.ibm.com/ibm/cloud}
}

@misc{amazon-ec2,
  title        = {{A}mazon Elastic Compute Cloud},
  url          = {http://aws.amazon.com/ec2}
}

@misc{openstack,
  title        = {{OpenStack}},
  url          = {http://docs.openstack.org/}
}

@misc{rubbos,
  title        = {{RUBBoS}: Bulletin Board Benchmark},
  url          = {http://jmob.ow2.org/rubbos.html}
}

@misc{rubis,
  title        = {{RUBiS}: {R}ice {U}niversity Bidding System},
  url          = {http://rubis.ow2.org/}
}

@misc{tpcw,
  title        = {{TPC-W}: a transactional web {e-Commerce} benchmark},
  url          = {http://www.tpc.org/tpcw/}
}

@misc{vmspernode,
  title        = {{IT} Knowledge Exchange: Virtual machines per server: A viable metric for hardware selection?}, 
  url          = {http://itknowledgeexchange.techtarget.com/server-farm/virtual-machines-per-server-a-viable-metric-for-hardware-selection/},
  note         = {Online, August 2008, retrieved September 2011}
}

@misc{hpanim,
  author       = {Hewlett-Packard},
  title        = {Servicing the Animation Industry: {HP}'s utility rendering service provides on-demand computing resources},
  url          = {http://www.hpl.hp.com/SE3D/whitepaper-urs.pdf},
  month        = jul,
  year         = 2004
}

@misc{windows-azure,
  title        = {{MicroSoft Windows Azure}},
  url          = {http://www.microsoft.com/windowsazure/}
}

@misc{google-app-engine,
  title        = {{Google AppEngine}},
  url          = {http://code.google.com/appengine/}
}

@inproceedings{clauss2011sno2,
  author       = {Clauss, Pierre-Nicolas and Stillwell, Mark and Genaud, St{\'e}phane and Suter, Fr{\'e}d{\'e}ric and Casanova, Henri and Quinson, Martin},
  title        = {Single Node On-Line Simulation of {MPI} Applications with {SMPI}},
  crossref     = {ipdps11}
}

@misc{data-bin-pack
  author       = {Unknown},
  title        = {{Data.BinPack}}, 
  url          = {http://hackage.haskell.org/packages/archive/Binpack/latest/doc/html/Data-BinPack.html}
}
